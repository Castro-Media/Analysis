{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Data Source to Catalog\n",
    "This notebook guides you through adding a new data source to `catalog.csv`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ensure required packages\n",
    "import importlib, subprocess, sys\n",
    "\n",
    "def _ensure(pkg_name, import_name=None):\n",
    "    try:\n",
    "        importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "    finally:\n",
    "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
    "\n",
    "for pkg in ('pandas','requests','feedparser'):\n",
    "    _ensure(pkg)\n",
    "print('Dependencies ready.')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import csv, re\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd, requests, feedparser\n",
    "\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "catalog_path = BASE_DIR / 'catalog.csv'\n",
    "cat = pd.read_csv(catalog_path)\n",
    "\n",
    "KEY_PHRASES = ['business','world news','economy','politics']\n",
    "\n",
    "def detect_filetype(url, resp):\n",
    "    u = url.lower()\n",
    "    if u.endswith('.rss') or u.endswith('.xml'):\n",
    "        return 'rss'\n",
    "    if u.endswith('.json'):\n",
    "        return 'json'\n",
    "    if u.endswith('.csv'):\n",
    "        return 'csv'\n",
    "    ct = resp.headers.get('content-type', '').lower()\n",
    "    if 'json' in ct:\n",
    "        return 'json'\n",
    "    if 'csv' in ct or 'text/plain' in ct:\n",
    "        return 'csv'\n",
    "    if 'xml' in ct or 'rss' in ct:\n",
    "        return 'rss'\n",
    "    return None\n",
    "\n",
    "def root_domain(url):\n",
    "    host = urlparse(url).netloc.split('@')[-1]  # drop creds if any\n",
    "    parts = host.split('.')\n",
    "    if len(parts) > 2:\n",
    "        return '.'.join(parts[-2:])\n",
    "    return host\n",
    "\n",
    "def guess_source(url):\n",
    "    rd = root_domain(url)\n",
    "    mask = cat['link'].astype(str).str.contains(rd) | cat['url'].astype(str).str.contains(rd)\n",
    "    if mask.any():\n",
    "        return cat.loc[mask, 'source'].iloc[0]\n",
    "    return rd.split('.')[0]\n",
    "\n",
    "def guess_category(title, url):\n",
    "    tokens = set(re.findall(r'[A-Za-z]+', (title or '') + ' ' + root_domain(url)))\n",
    "    scores = {}\n",
    "    for token in tokens:\n",
    "        mask = cat['description'].str.contains(token, case=False, na=False) | cat['category'].str.contains(token, case=False, na=False)\n",
    "        for c in cat.loc[mask, 'category']:\n",
    "            scores[c] = scores.get(c, 0) + 1\n",
    "    if scores:\n",
    "        return max(scores, key=scores.get)\n",
    "    return ''\n",
    "\n",
    "def guess_link(url):\n",
    "    parsed = urlparse(url)\n",
    "    return f'{parsed.scheme}://{root_domain(url)}'\n",
    "\n",
    "def guess_folder(category, source):\n",
    "    if category and source:\n",
    "        return f'{category}-{source}'\n",
    "    return ''\n",
    "\n",
    "while True:\n",
    "    url = input('URL (blank to exit): ').strip()\n",
    "    if not url:\n",
    "        break\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print('Error fetching:', e)\n",
    "        continue\n",
    "    ftype = detect_filetype(url, resp)\n",
    "    if not ftype:\n",
    "        print('Could not determine file type.')\n",
    "        continue\n",
    "    title = ''\n",
    "    if ftype == 'rss':\n",
    "        feed = feedparser.parse(resp.content)\n",
    "        title = feed.feed.get('title', '')\n",
    "        text = ' '.join(' '.join(filter(None, [e.get('title', ''), e.get('summary', '')])) for e in feed.entries).lower()\n",
    "        if any(p in text for p in KEY_PHRASES):\n",
    "            print('Key phrase found in feed.')\n",
    "    print('Detected type:', ftype)\n",
    "    if title:\n",
    "        print('Feed title:', title)\n",
    "    source_guess = guess_source(url)\n",
    "    category_guess = guess_category(title, url)\n",
    "    link_guess = guess_link(url)\n",
    "    folder_guess = guess_folder(category_guess or 'unknown', source_guess or 'unknown')\n",
    "    row = {\n",
    "        'category': input(f'Category [{category_guess}]: ').strip() or category_guess,\n",
    "        'source': input(f'Source [{source_guess}]: ').strip() or source_guess,\n",
    "        'filetype': ftype,\n",
    "        'folder': input(f'Folder name [{folder_guess}]: ').strip() or folder_guess,\n",
    "        'url': url,\n",
    "        'api_key': '',\n",
    "        'cadence': input('Cadence (e.g., hourly) [hourly]: ').strip() or 'hourly',\n",
    "        'last_fetched': '',\n",
    "        'description': input(f'Description [{title}]: ').strip() or title,\n",
    "        'link': input(f'Link [{link_guess}]: ').strip() or link_guess\n",
    "    }\n",
    "    print('Proposed row:', row)\n",
    "    if input('Add to catalog? [y/N] ').lower().startswith('y'):\n",
    "        with open(catalog_path, 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=cat.columns)\n",
    "            writer.writerow(row)\n",
    "        cat = pd.read_csv(catalog_path)\n",
    "        print('Row added.')\n",
    "    else:\n",
    "        print('Skipped.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}