{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Script\n",
    "This notebook orchestrates data downloads and analysis refreshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n",
      "\n",
      "Skipping GDPC1 – up to date\n",
      "Skipping A939RX0Q048SBEA – up to date\n",
      "Skipping M2REAL – up to date\n",
      "Skipping UNRATE – up to date\n",
      "Skipping CLVMNACSCAB1GQDE – up to date\n",
      "Skipping GFDEBTN – up to date\n",
      "Skipping GFDEGDQ188S – up to date\n",
      "Skipping TDSP – up to date\n",
      "Fetching news-us-politics-wapo … ✗ failed: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=30)\n",
      "Fetching news-us-wapo … ✓ success\n",
      "Fetching news-world-wapo … ✓ success\n",
      "Fetching news-business-wapo … ✓ success\n",
      "Fetching latimes-business … ✓ success\n",
      "Fetching latimes-us … ✓ success\n",
      "Fetching latimes-us-politics … ✓ success\n",
      "Skipping news-world-chi-tribune – up to date\n",
      "Skipping news-business-chi-tribune – up to date\n",
      "Skipping news-us-politics-chi-tribune – up to date\n",
      "\n",
      "Updated: news-us-wapo, news-world-wapo, news-business-wapo, latimes-business, latimes-us, latimes-us-politics\n"
     ]
    }
   ],
   "source": [
    "# ========== Bootstrap: ensure required Python packages are present ==========\n",
    "import importlib, subprocess, sys\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: str | None = None):\n",
    "    \"\"\"\n",
    "    Import `import_name` (defaults to `pkg_name`); if that fails, pip‑install.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"Package '{pkg_name}' not found — installing …\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "    finally:\n",
    "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
    "\n",
    "# --- Required third‑party libraries ------------------------------------------\n",
    "_ensure(\"pandas\")\n",
    "_ensure(\"requests\")\n",
    "_ensure(\"feedparser\")\n",
    "_ensure(\"textblob\")\n",
    "_ensure(\"jupyter\")\n",
    "print(\"All dependencies ready.\\n\")\n",
    "\n",
    "# --- Standard imports --------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json\n",
    "import pandas as pd, requests, urllib.parse\n",
    "\n",
    "# --- Helper: replace [date %Y-%m-%d] tokens -----------------------------------\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        fmt = m.group(1).strip()\n",
    "        return dt.date.today().strftime(fmt)\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "# --- Helper: append API key if specified -----------------------------------\n",
    "def add_apikey(url: str, env_var: str | None) -> str:\n",
    "    if env_var and str(env_var).lower() != \"nan\":\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f'{url}{sep}api_key={urllib.parse.quote_plus(key)}'\n",
    "        else:\n",
    "            print(f\"Warning: environment variable '{env_var}' not set.\")\n",
    "    return url\n",
    "\n",
    "# --- Cadence map (word → minimum days between fetches) ------------------------\n",
    "CADENCE_DAYS = {\n",
    "    \"daily\": 1,\n",
    "    \"weekly\": 7,\n",
    "    \"monthly\": 30,\n",
    "    \"quarterly\": 90,\n",
    "}\n",
    "\n",
    "# --- Resolve base directory so notebook works from repo root or data folder ---\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "\n",
    "# --- Load catalog -------------------------------------------------------------\n",
    "catalog_path = BASE_DIR / 'catalog.csv'\n",
    "cat = pd.read_csv(catalog_path)\n",
    "cat['filetype'] = cat['filetype'].astype(str).str.strip().str.lstrip('.')\n",
    "\n",
    "today = dt.date.today()\n",
    "updated_rows = []                # remember which rows we refresh\n",
    "\n",
    "for idx, row in cat.iterrows():\n",
    "    folder = BASE_DIR / str(row['category']) / str(row['source']) / str(row['folder'])\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = str(row['filetype']).strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    latest_fp = folder / f'latest.{output_ext}'\n",
    "    dated_fp = folder / f'{today:%Y-%m-%d}.{output_ext}'\n",
    "    if dated_fp.exists():\n",
    "        cat.at[idx, 'last_fetched'] = today.isoformat()\n",
    "        continue\n",
    "    last_fetched = (\n",
    "        pd.to_datetime(row[\"last_fetched\"]).date()\n",
    "        if pd.notna(row[\"last_fetched\"]) else None\n",
    "    )\n",
    "\n",
    "    # ---- Determine if an update is due --------------------------------------\n",
    "    cadence = str(row[\"cadence\"]).lower().strip()\n",
    "    min_age = CADENCE_DAYS.get(cadence, 30)        # default 30 days\n",
    "    needs_update = (\n",
    "        (not latest_fp.exists()) or\n",
    "        (not last_fetched) or\n",
    "        (today - last_fetched).days >= min_age\n",
    "    )\n",
    "\n",
    "    if not needs_update:\n",
    "        print(f\"Skipping {row['folder']} – up to date\")\n",
    "        continue\n",
    "\n",
    "    # ---- Build the request URL ---------------------------------------------\n",
    "    url = substitute_date_tokens(str(row[\"url\"]))\n",
    "    url = add_apikey(url, str(row.get('api_key') or '').strip() or None)\n",
    "\n",
    "    print(f\"Fetching {row['folder']} …\", end=\" \")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        r.raise_for_status()\n",
    "        if filetype.lower() in ('rss', 'xml'):\n",
    "            feed = feedparser.parse(r.content)\n",
    "            entries = []\n",
    "            for e in feed.entries:\n",
    "                text = ' '.join(filter(None, [e.get('title'), e.get('summary')]))\n",
    "                polarity = textblob.TextBlob(text).sentiment.polarity\n",
    "                entries.append({'title': e.get('title'), 'link': e.get('link'),\n",
    "                               'published': e.get('published'),\n",
    "                               'sentiment': polarity})\n",
    "            content_bytes = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "        else:\n",
    "            content_bytes = r.content\n",
    "        if filetype.lower() == 'json':\n",
    "            try:\n",
    "                data_json = r.json()\n",
    "            except Exception:\n",
    "                data_json = None\n",
    "            if isinstance(data_json, dict) and data_json.get('error_message'):\n",
    "                raise ValueError(data_json['error_message'])\n",
    "        # ---- Save snapshot and latest --------------------------------------\n",
    "        if latest_fp.exists() and latest_fp.read_bytes() == content_bytes:\n",
    "            cat.at[idx, 'last_fetched'] = today.isoformat()\n",
    "            print('no change')\n",
    "            continue\n",
    "        dated_fp.write_bytes(content_bytes)\n",
    "        shutil.copyfile(dated_fp, latest_fp)\n",
    "\n",
    "        # ---- Mark success in catalog ---------------------------------------\n",
    "        cat.at[idx, \"last_fetched\"] = today.isoformat()\n",
    "        updated_rows.append(row[\"folder\"])\n",
    "        print(\"✓ success\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ failed: {e}\")\n",
    "\n",
    "# --- Persist catalog if anything changed -------------------------------------\n",
    "if updated_rows:\n",
    "    cat.to_csv(catalog_path, index=False)\n",
    "    print(\"\\nUpdated:\", \", \".join(updated_rows))\n",
    "else:\n",
    "    print(\"Everything up to date.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index files generated for GDPC1, A939RX0Q048SBEA, M2REAL, UNRATE, CLVMNACSCAB1GQDE, GFDEBTN, GFDEGDQ188S, TDSP, news-us-nyt, news-world-nyt, news-africa-nyt, news-europe-nyt, news-asia-nyt, news-americas-nyt, news-middle-east-nyt, news-business-nyt, news-economy-nyt, news-us-politics-nyt, news-world-wsj, news-us-wsj, news-business-wsj, news-markets-wsj, news-economy-wsj, news-us-politics-wsj, news-us-politics-wapo, news-us-wapo, news-world-wapo, news-business-wapo, latimes-business, latimes-us, latimes-us-politics, news-world-chi-tribune, news-business-chi-tribune, news-us-politics-chi-tribune\n"
     ]
    }
   ],
   "source": [
    "# This cell updates the markdown index files for all the data sources\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "with open(BASE_DIR / 'catalog.csv', newline='') as f:\n",
    "    cat = list(csv.DictReader(f))\n",
    "\n",
    "for row in cat:\n",
    "    folder = BASE_DIR / row['category'] / row['source'] / row['folder']\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = row['filetype'].strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    desc = row['description'].strip()\n",
    "    source = row['source'].strip()\n",
    "    date = row.get('last_fetched', '').strip()\n",
    "\n",
    "    pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}\\.' + re.escape(output_ext) + r'$')\n",
    "    dated_files = sorted(p.name for p in folder.iterdir() if pattern.match(p.name))\n",
    "\n",
    "    lines = [\n",
    "        '---',\n",
    "        'layout: default',\n",
    "        f'title: {source} - {desc}',\n",
    "        f'date: {date}',\n",
    "        '---',\n",
    "        '',\n",
    "        f'## {source} - {desc}',\n",
    "        '',\n",
    "        '<div id=\"data-chart\"></div>',\n",
    "        '<div id=\"data-table\"></div>',\n",
    "    ]\n",
    "\n",
    "    if row['source'] == 'fred' and filetype == 'json':\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  ShowChart($('#data-chart'));\",\n",
    "            \"  SourceTabler($('#data-table'));\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "    else:\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  document.getElementById('data-table').textContent = 'This source isn\\'t supported for tables yet.';\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "\n",
    "    lines += [\n",
    "        '',\n",
    "        '## File Versions:',\n",
    "    ]\n",
    "    links = [f'[Latest version](./latest.{output_ext})'] + [f'[{fname}](./{fname})' for fname in dated_files]\n",
    "    for i, link in enumerate(links, 1):\n",
    "        lines.append(f'{i}. {link}')\n",
    "    (folder / 'index.md').write_text(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print('Index files generated for', ', '.join(r['folder'] for r in cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"notebooks\": [\n",
      "    [\n",
      "      \"analysis\\\\headlines\\\\update_headlines.ipynb\",\n",
      "      \"2025-06-07 17:04:09\"\n",
      "    ],\n",
      "    [\n",
      "      \"analysis\\\\news-topics\\\\analyze_headlines.ipynb\",\n",
      "      \"2025-06-05 13:21:02\"\n",
      "    ]\n",
      "  ],\n",
      "  \"latest_files\": [\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\A939RX0Q048SBEA\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:03\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\CLVMNACSCAB1GQDE\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:04\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\GDPC1\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:03\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\GFDEBTN\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:04\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\GFDEGDQ188S\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:05\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\M2REAL\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:03\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\TDSP\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:05\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\UNRATE\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:04\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-africa\\\\nyt\\\\news-africa-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:48\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-americas\\\\nyt\\\\news-americas-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:49\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-asia\\\\nyt\\\\news-asia-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:48\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\latimes\\\\latimes-business\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:32\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\nyt\\\\news-business-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:49\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\tribune\\\\news-business-chi-tribune\\\\latest.json\",\n",
      "      \"2025-06-04 21:25:09\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\wapo\\\\news-business-wapo\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:32\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\wsj\\\\news-business-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\wsj\\\\news-markets-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-economy\\\\nyt\\\\news-economy-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:49\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-economy\\\\wsj\\\\news-economy-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-europe\\\\nyt\\\\news-europe-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:48\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-middle-east\\\\nyt\\\\news-middle-east-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:49\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us\\\\latimes\\\\latimes-us\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:32\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us\\\\nyt\\\\news-us-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 17:06:49\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us\\\\wapo\\\\news-us-wapo\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:15\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us\\\\wsj\\\\news-us-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us-politics\\\\latimes\\\\latimes-us-politics\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:32\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us-politics\\\\nyt\\\\news-us-politics-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us-politics\\\\tribune\\\\news-us-politics-chi-tribune\\\\latest.json\",\n",
      "      \"2025-06-04 21:25:09\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us-politics\\\\wsj\\\\news-us-politics-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:51\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-world\\\\nyt\\\\news-world-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:48\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-world\\\\tribune\\\\news-world-chi-tribune\\\\latest.json\",\n",
      "      \"2025-06-04 21:25:08\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-world\\\\wapo\\\\news-world-wapo\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:23\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-world\\\\wsj\\\\news-world-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"analysis\\\\headlines\\\\latest.csv\",\n",
      "      \"2025-06-05 13:20:55\"\n",
      "    ]\n",
      "  ],\n",
      "  \"dependencies\": [\n",
      "    [\n",
      "      \"analysis\\\\headlines\\\\update_headlines.ipynb\",\n",
      "      \"2025-06-07 17:04:09\",\n",
      "      [\n",
      "        [\n",
      "          \"data\\\\news-us\\\\nyt\\\\news-us-nyt\\\\latest.json\",\n",
      "          \"2025-06-07 17:06:49\"\n",
      "        ],\n",
      "        [\n",
      "          \"data\\\\news-us\\\\wsj\\\\news-us-wsj\\\\latest.json\",\n",
      "          \"2025-06-07 16:01:50\"\n",
      "        ],\n",
      "        [\n",
      "          \"analysis\\\\headlines\\\\latest.csv\",\n",
      "          \"2025-06-05 13:20:55\"\n",
      "        ]\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"analysis\\\\news-topics\\\\analyze_headlines.ipynb\",\n",
      "      \"2025-06-05 13:21:02\",\n",
      "      [\n",
      "        [\n",
      "          \"analysis\\\\headlines\\\\latest.csv\",\n",
      "          \"2025-06-05 13:20:55\"\n",
      "        ]\n",
      "      ]\n",
      "    ]\n",
      "  ],\n",
      "  \"only_data_notebooks\": [],\n",
      "  \"other_notebooks\": [\n",
      "    [\n",
      "      \"analysis\\\\headlines\\\\update_headlines.ipynb\",\n",
      "      \"2025-06-07 17:04:09\"\n",
      "    ],\n",
      "    [\n",
      "      \"analysis\\\\news-topics\\\\analyze_headlines.ipynb\",\n",
      "      \"2025-06-05 13:21:02\"\n",
      "    ]\n",
      "  ],\n",
      "  \"outdated_notebooks\": [\n",
      "    \"analysis\\\\headlines\\\\update_headlines.ipynb\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4743"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependency report\n",
    "from pathlib import Path\n",
    "import json, re, time\n",
    "\n",
    "repo_dir = Path.cwd()\n",
    "if not (repo_dir / 'analysis').is_dir():\n",
    "    repo_dir = repo_dir.parent\n",
    "analysis_dir = repo_dir / 'analysis'\n",
    "data_dir = repo_dir / 'data'\n",
    "\n",
    "pattern = re.compile(r'[A-Za-z0-9_/.-]*latest\\.(?:csv|json|xml|rss)')\n",
    "\n",
    "def mtime_str(p: Path) -> str:\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(p.stat().st_mtime))\n",
    "\n",
    "# --- List notebooks ---\n",
    "ipynb_paths = sorted(analysis_dir.rglob('*.ipynb'))\n",
    "notebooks = [[str(p.relative_to(repo_dir)), mtime_str(p)] for p in ipynb_paths]\n",
    "\n",
    "# --- List latest files ---\n",
    "latest_files = sorted(data_dir.rglob('latest.*')) + sorted(analysis_dir.rglob('latest.*'))\n",
    "latest = [[str(p.relative_to(repo_dir)), mtime_str(p)] for p in latest_files]\n",
    "\n",
    "# --- Map notebook dependencies ---\n",
    "dep_map = {}\n",
    "for nb in ipynb_paths:\n",
    "    text = nb.read_text()\n",
    "    matches = sorted(set(pattern.findall(text)))\n",
    "    deps = []\n",
    "    for m in matches:\n",
    "        dep = (nb.parent / m).resolve()\n",
    "        if not dep.exists():\n",
    "            dep = (repo_dir / m.lstrip('./')).resolve()\n",
    "        if dep.exists():\n",
    "            deps.append([str(dep.relative_to(repo_dir)), mtime_str(dep)])\n",
    "    dep_map[str(nb.relative_to(repo_dir))] = {'modified': mtime_str(nb), 'deps': deps}\n",
    "\n",
    "dep_list = [[nb, info['modified'], info['deps']] for nb, info in dep_map.items()]\n",
    "\n",
    "only_data = []\n",
    "other = []\n",
    "for nb, info in dep_map.items():\n",
    "    if info['deps'] and all(d[0].startswith('data/') for d in info['deps']):\n",
    "        only_data.append([nb, info['modified']])\n",
    "    else:\n",
    "        other.append([nb, info['modified']])\n",
    "\n",
    "# --- Determine outdated notebooks ---\n",
    "outdated = []\n",
    "for nb, info in dep_map.items():\n",
    "    if info['deps']:\n",
    "        nb_mtime = (repo_dir / nb).stat().st_mtime\n",
    "        newest_dep = max((repo_dir / d[0]).stat().st_mtime for d in info['deps'])\n",
    "        if newest_dep > nb_mtime:\n",
    "            outdated.append(nb)\n",
    "\n",
    "report = {\n",
    "    'notebooks': notebooks,\n",
    "    'latest_files': latest,\n",
    "    'dependencies': dep_list,\n",
    "    'only_data_notebooks': only_data,\n",
    "    'other_notebooks': other,\n",
    "    'outdated_notebooks': outdated,\n",
    "}\n",
    "json_txt = json.dumps(report, indent=2)\n",
    "print(json_txt)\n",
    "(repo_dir / 'dependencies.json').write_text(json_txt + '\\n')\n",
    "(repo_dir / f\"dependencies-{time.strftime('%Y-%m-%d')}.json\").write_text(json_txt + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis\\headlines\\update_headlines.ipynb … ✗ failed: Command '['c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python.exe', '-m', 'jupyter', 'nbconvert', '--to', 'notebook', '--inplace', '--execute', 'c:\\\\Users\\\\CJ\\\\Documents\\\\GitHub\\\\Analysis\\\\analysis\\\\headlines\\\\update_headlines.ipynb']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "# Execute outdated notebooks\n",
    "import subprocess, sys\n",
    "\n",
    "def outdated(nb, deps):\n",
    "    nb_mtime = (repo_dir / nb).stat().st_mtime\n",
    "    for dep_path, _ in deps:\n",
    "        dep_mtime = (repo_dir / dep_path).stat().st_mtime\n",
    "        if dep_mtime > nb_mtime:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def execute(nb):\n",
    "    path = repo_dir / nb\n",
    "    print(f'Running {nb} …', end=' ')\n",
    "    try:\n",
    "        subprocess.check_call([\n",
    "            sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "            '--to', 'notebook', '--inplace', '--execute', str(path)\n",
    "        ])\n",
    "        print('✓ success')\n",
    "    except Exception as e:\n",
    "        print(f'✗ failed: {e}')\n",
    "\n",
    "for nb, _ in only_data:\n",
    "    if outdated(nb, dep_map[nb]['deps']):\n",
    "        execute(nb)\n",
    "\n",
    "for nb, _ in other:\n",
    "    if outdated(nb, dep_map[nb]['deps']):\n",
    "        execute(nb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
