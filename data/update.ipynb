{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Script\n",
    "This notebook orchestrates data downloads and analysis refreshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n",
      "\n"
     ]
    },
    {
     "ename": "DateParseError",
     "evalue": "Unknown datetime string format, unable to parse: Business News from the New York Times, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDateParseError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     74\u001b[39m     cat.at[idx, \u001b[33m'\u001b[39m\u001b[33mlast_fetched\u001b[39m\u001b[33m'\u001b[39m] = today.isoformat()\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     76\u001b[39m last_fetched = (\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlast_fetched\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.date()\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.notna(row[\u001b[33m\"\u001b[39m\u001b[33mlast_fetched\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     79\u001b[39m )\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# ---- Determine if an update is due --------------------------------------\u001b[39;00m\n\u001b[32m     82\u001b[39m cadence = \u001b[38;5;28mstr\u001b[39m(row[\u001b[33m\"\u001b[39m\u001b[33mcadence\u001b[39m\u001b[33m\"\u001b[39m]).lower().strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1101\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1099\u001b[39m         result = convert_listlike(argc, \u001b[38;5;28mformat\u001b[39m)\n\u001b[32m   1100\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m     result = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np.bool_):\n\u001b[32m   1103\u001b[39m         result = \u001b[38;5;28mbool\u001b[39m(result)  \u001b[38;5;66;03m# TODO: avoid this kludge.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:435\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m result, tz_parsed = \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[32m    447\u001b[39m     out_unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2398\u001b[39m, in \u001b[36mobjects_to_datetime64\u001b[39m\u001b[34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[39m\n\u001b[32m   2395\u001b[39m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[32m   2396\u001b[39m data = np.asarray(data, dtype=np.object_)\n\u001b[32m-> \u001b[39m\u001b[32m2398\u001b[39m result, tz_parsed = \u001b[43mtslib\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2400\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2403\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[43m=\u001b[49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2408\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m   2409\u001b[39m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[32m   2410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mtslib.pyx:414\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mtslib.pyx:596\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mtslib.pyx:553\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mconversion.pyx:641\u001b[39m, in \u001b[36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsing.pyx:336\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsing.pyx:666\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDateParseError\u001b[39m: Unknown datetime string format, unable to parse: Business News from the New York Times, at position 0"
     ]
    }
   ],
   "source": [
    "# ========== Bootstrap: ensure required Python packages are present ==========\n",
    "import importlib, subprocess, sys\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: str | None = None):\n",
    "    \"\"\"\n",
    "    Import `import_name` (defaults to `pkg_name`); if that fails, pip‑install.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"Package '{pkg_name}' not found — installing …\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "    finally:\n",
    "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
    "\n",
    "# --- Required third‑party libraries ------------------------------------------\n",
    "_ensure(\"pandas\")\n",
    "_ensure(\"requests\")\n",
    "_ensure(\"feedparser\")\n",
    "_ensure(\"textblob\")\n",
    "print(\"All dependencies ready.\\n\")\n",
    "\n",
    "# --- Standard imports --------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json\n",
    "import pandas as pd, requests, urllib.parse\n",
    "\n",
    "# --- Helper: replace [date %Y-%m-%d] tokens -----------------------------------\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        fmt = m.group(1).strip()\n",
    "        return dt.date.today().strftime(fmt)\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "# --- Helper: append API key if specified -----------------------------------\n",
    "def add_apikey(url: str, env_var: str | None) -> str:\n",
    "    if env_var:\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f'{url}{sep}api_key={urllib.parse.quote_plus(key)}'\n",
    "        else:\n",
    "            print(f\"Warning: environment variable '{env_var}' not set.\")\n",
    "    return url\n",
    "\n",
    "# --- Cadence map (word → minimum days between fetches) ------------------------\n",
    "CADENCE_DAYS = {\n",
    "    \"daily\": 1,\n",
    "    \"weekly\": 7,\n",
    "    \"monthly\": 30,\n",
    "    \"quarterly\": 90,\n",
    "}\n",
    "\n",
    "# --- Resolve base directory so notebook works from repo root or data folder ---\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "\n",
    "# --- Load catalog -------------------------------------------------------------\n",
    "catalog_path = BASE_DIR / 'catalog.csv'\n",
    "cat = pd.read_csv(catalog_path)\n",
    "cat['filetype'] = cat['filetype'].astype(str).str.strip().str.lstrip('.')\n",
    "\n",
    "today = dt.date.today()\n",
    "updated_rows = []                # remember which rows we refresh\n",
    "\n",
    "for idx, row in cat.iterrows():\n",
    "    folder = BASE_DIR / str(row['category']) / str(row['source']) / str(row['folder'])\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = str(row['filetype']).strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    latest_fp = folder / f'latest.{output_ext}'\n",
    "    dated_fp = folder / f'{today:%Y-%m-%d}.{output_ext}'\n",
    "    if dated_fp.exists():\n",
    "        cat.at[idx, 'last_fetched'] = today.isoformat()\n",
    "        continue\n",
    "    last_fetched = (\n",
    "        pd.to_datetime(row[\"last_fetched\"]).date()\n",
    "        if pd.notna(row[\"last_fetched\"]) else None\n",
    "    )\n",
    "\n",
    "    # ---- Determine if an update is due --------------------------------------\n",
    "    cadence = str(row[\"cadence\"]).lower().strip()\n",
    "    min_age = CADENCE_DAYS.get(cadence, 30)        # default 30 days\n",
    "    needs_update = (\n",
    "        (not latest_fp.exists()) or\n",
    "        (not last_fetched) or\n",
    "        (today - last_fetched).days >= min_age\n",
    "    )\n",
    "\n",
    "    if not needs_update:\n",
    "        print(f\"Skipping {row['folder']} – up to date\")\n",
    "        continue\n",
    "\n",
    "    # ---- Build the request URL ---------------------------------------------\n",
    "    url = substitute_date_tokens(str(row[\"url\"]))\n",
    "    url = add_apikey(url, str(row.get('api_key') or '').strip() or None)\n",
    "\n",
    "    print(f\"Fetching {row['folder']} …\", end=\" \")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        if filetype.lower() in ('rss', 'xml'):\n",
    "            feed = feedparser.parse(r.content)\n",
    "            entries = []\n",
    "            for e in feed.entries:\n",
    "                text = ' '.join(filter(None, [e.get('title'), e.get('summary')]))\n",
    "                polarity = textblob.TextBlob(text).sentiment.polarity\n",
    "                entries.append({'title': e.get('title'), 'link': e.get('link'),\n",
    "                               'published': e.get('published'),\n",
    "                               'sentiment': polarity})\n",
    "            content_bytes = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "        else:\n",
    "            content_bytes = r.content\n",
    "        if filetype.lower() == 'json':\n",
    "            try:\n",
    "                data_json = r.json()\n",
    "            except Exception:\n",
    "                data_json = None\n",
    "            if isinstance(data_json, dict) and data_json.get('error_message'):\n",
    "                raise ValueError(data_json['error_message'])\n",
    "        # ---- Save snapshot and latest --------------------------------------\n",
    "        if latest_fp.exists() and latest_fp.read_bytes() == content_bytes:\n",
    "            cat.at[idx, 'last_fetched'] = today.isoformat()\n",
    "            print('no change')\n",
    "            continue\n",
    "        dated_fp.write_bytes(content_bytes)\n",
    "        shutil.copyfile(dated_fp, latest_fp)\n",
    "\n",
    "        # ---- Mark success in catalog ---------------------------------------\n",
    "        cat.at[idx, \"last_fetched\"] = today.isoformat()\n",
    "        updated_rows.append(row[\"folder\"])\n",
    "        print(\"✓ success\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ failed: {e}\")\n",
    "\n",
    "# --- Persist catalog if anything changed -------------------------------------\n",
    "if updated_rows:\n",
    "    cat.to_csv(catalog_path, index=False)\n",
    "    print(\"\\nUpdated:\", \", \".join(updated_rows))\n",
    "else:\n",
    "    print(\"Everything up to date.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\CJ\\\\Documents\\\\GitHub\\\\Analysis\\\\data\\\\news-economy\\\\nyt\\\\news-economy-nyt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m date = row.get(\u001b[33m'\u001b[39m\u001b[33mlast_fetched\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).strip()\n\u001b[32m     18\u001b[39m pattern = re.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\\\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m + re.escape(output_ext) + \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m$\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m dated_files = \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m lines = [\n\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m---\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlayout: default\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m<div id=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata-table\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m></div>\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m ]\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mfred\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m filetype == \u001b[33m'\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     16\u001b[39m date = row.get(\u001b[33m'\u001b[39m\u001b[33mlast_fetched\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).strip()\n\u001b[32m     18\u001b[39m pattern = re.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\\\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m + re.escape(output_ext) + \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m$\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m dated_files = \u001b[38;5;28msorted\u001b[39m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m lines = [\n\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m---\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlayout: default\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m<div id=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata-table\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m></div>\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m ]\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mfred\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m filetype == \u001b[33m'\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:1057\u001b[39m, in \u001b[36mPath.iterdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1052\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Yield path objects of the directory contents.\u001b[39;00m\n\u001b[32m   1053\u001b[39m \n\u001b[32m   1054\u001b[39m \u001b[33;03m    The children are yielded in arbitrary order, and the\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03m    special entries '.' and '..' are not included.\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m   1058\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_child_relpath(name)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\CJ\\\\Documents\\\\GitHub\\\\Analysis\\\\data\\\\news-economy\\\\nyt\\\\news-economy-nyt'"
     ]
    }
   ],
   "source": [
    "# This cell updates the markdown index files for all the data sources\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "with open(BASE_DIR / 'catalog.csv', newline='') as f:\n",
    "    cat = list(csv.DictReader(f))\n",
    "\n",
    "for row in cat:\n",
    "    folder = BASE_DIR / row['category'] / row['source'] / row['folder']\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = row['filetype'].strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    desc = row['description'].strip()\n",
    "    source = row['source'].strip()\n",
    "    date = row.get('last_fetched', '').strip()\n",
    "\n",
    "    pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}\\.' + re.escape(output_ext) + r'$')\n",
    "    dated_files = sorted(p.name for p in folder.iterdir() if pattern.match(p.name))\n",
    "\n",
    "    lines = [\n",
    "        '---',\n",
    "        'layout: default',\n",
    "        f'title: {source} - {desc}',\n",
    "        f'date: {date}',\n",
    "        '---',\n",
    "        '',\n",
    "        f'## {source} - {desc}',\n",
    "        '',\n",
    "        '<div id=\"data-chart\"></div>',\n",
    "        '<div id=\"data-table\"></div>',\n",
    "    ]\n",
    "\n",
    "    if row['source'] == 'fred' and filetype == 'json':\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  ShowChart($('#data-chart'));\",\n",
    "            \"  SourceTabler($('#data-table'));\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "    else:\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  document.getElementById('data-table').textContent = 'This source isn\\'t supported for tables yet.';\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "\n",
    "    lines += [\n",
    "        '',\n",
    "        '## File Versions:',\n",
    "    ]\n",
    "    links = [f'[Latest version](./latest.{output_ext})'] + [f'[{fname}](./{fname})' for fname in dated_files]\n",
    "    for i, link in enumerate(links, 1):\n",
    "        lines.append(f'{i}. {link}')\n",
    "    (folder / 'index.md').write_text(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print('Index files generated for', ', '.join(r['folder'] for r in cat))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
