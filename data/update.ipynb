{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c898f38d",
   "metadata": {},
   "source": [
    "# Update Script\n",
    "This notebook orchestrates data downloads and analysis refreshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d400efd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:53:31.821640Z",
     "iopub.status.busy": "2025-07-16T05:53:31.821435Z",
     "iopub.status.idle": "2025-07-16T05:54:59.753764Z",
     "shell.execute_reply": "2025-07-16T05:54:59.753214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'pandas' not found - installing ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m163.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/16.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m202.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [tzdata]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.3.1 pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'feedparser' not found - installing ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=0f83c084b6367b5aa5ee2bd6ad5d75860d610014cf98a884e643bc172ca057c2\n",
      "  Stored in directory: /home/runner/.cache/pip/wheels/3d/4d/ef/37cdccc18d6fd7e0dd7817dcdf9146d4d6789c32a227a28134\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [feedparser]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'textblob' not found - installing ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk>=3.9->textblob)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk>=3.9->textblob)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex>=2021.8.3 (from nltk>=3.9->textblob)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk>=3.9->textblob)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/624.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/796.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk, textblob\n",
      "\u001b[?25l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [joblib]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [joblib]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [textblob]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 textblob-0.19.0 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n",
      "\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching GDPC1 (https://api.stlouisfed.org/fred/series/observations?series_id=GDPC1&file_type=json&observation_end=[date %Y-%m-%d])… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=GDPC1&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching A939RX0Q048SBEA (https://api.stlouisfed.org/fred/series/observations?series_id=A939RX0Q048SBEA&file_type=json&observation_end=[date %Y-%m-%d])… ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=A939RX0Q048SBEA&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching M2REAL (https://api.stlouisfed.org/fred/series/observations?series_id=M2REAL&file_type=json&observation_end=[date %Y-%m-%d])… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=M2REAL&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching UNRATE (https://api.stlouisfed.org/fred/series/observations?series_id=UNRATE&file_type=json&observation_end=[date %Y-%m-%d])… ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=UNRATE&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching CLVMNACSCAB1GQDE (https://api.stlouisfed.org/fred/series/observations?series_id=CLVMNACSCAB1GQDE&file_type=json&observation_end=[date %Y-%m-%d])… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=CLVMNACSCAB1GQDE&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching GFDEBTN (https://api.stlouisfed.org/fred/series/observations?series_id=GFDEBTN&file_type=json&observation_end=[date %Y-%m-%d])… ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=GFDEBTN&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching GFDEGDQ188S (https://api.stlouisfed.org/fred/series/observations?series_id=GFDEGDQ188S&file_type=json&observation_end=[date %Y-%m-%d])… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=GFDEGDQ188S&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching TDSP (https://api.stlouisfed.org/fred/series/observations?series_id=TDSP&file_type=json&observation_end=[date %Y-%m-%d])… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=TDSP&file_type=json&observation_end=2025-07-16\n",
      "Fetching news-us-nyt (https://rss.nytimes.com/services/xml/rss/nyt/US.xml)… ✓ success\n",
      "Fetching news-world-nyt (https://rss.nytimes.com/services/xml/rss/nyt/World.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-africa-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Africa.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-europe-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Europe.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-asia-nyt (https://rss.nytimes.com/services/xml/rss/nyt/AsiaPacific.xml)… no change\n",
      "Fetching news-americas-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Americas.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-middle-east-nyt (https://rss.nytimes.com/services/xml/rss/nyt/MiddleEast.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-business-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Business.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-economy-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Economy.xml)… no change\n",
      "Fetching news-us-politics-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Politics.xml)… ✓ success\n",
      "Fetching news-world-wsj (https://feeds.content.dowjones.io/public/rss/RSSWorldNews)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-wsj (https://feeds.content.dowjones.io/public/rss/RSSUSNews)… no change\n",
      "Fetching news-business-wsj (https://feeds.content.dowjones.io/public/rss/WSJcomUSBusiness)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-markets-wsj (https://feeds.content.dowjones.io/public/rss/RSSMarketsMain)… ✓ success\n",
      "Fetching news-economy-wsj (https://feeds.content.dowjones.io/public/rss/socialeconomyfeed)… no change\n",
      "Fetching news-us-politics-wsj (https://feeds.content.dowjones.io/public/rss/socialpoliticsfeed)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-politics-wapo (https://www.washingtonpost.com/arcio/rss/category/politics/)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=30)\n",
      "Fetching news-us-wapo (http://feeds.washingtonpost.com/rss/national)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 502 Server Error: Bad Gateway for url: https://feeds.washingtonpost.com/rss/national\n",
      "Fetching news-world-wapo (https://feeds.washingtonpost.com/rss/world)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 502 Server Error: Bad Gateway for url: https://feeds.washingtonpost.com/rss/world\n",
      "Fetching news-business-wapo (http://feeds.washingtonpost.com/rss/business)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching latimes-business (https://www.latimes.com/business/rss2.0.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching latimes-us (https://www.latimes.com/local/rss2.0.xml)… ✓ success\n",
      "Fetching latimes-us-politics (https://www.latimes.com/politics/rss2.0.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-world-chi-tribune (https://www.chicagotribune.com/arc/outboundfeeds/rss/section/nation-world/&sort=display_date:desc)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-business-chi-tribune (https://www.chicagotribune.com/arc/outboundfeeds/rss/section/business/&sort=display_date:desc)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-politics-chi-tribune (https://www.chicagotribune.com/arc/outboundfeeds/rss/section/politics/&sort=display_date:desc/)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-business-startribune (https://www.startribune.com/rss?sf=1&s=%2F)… ✓ success\n",
      "Fetching news-us-politics-startribune (https://www.startribune.com/politics/index.rss2)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 404 Client Error: Not Found for url: https://www.startribune.com/politics/index.rss2\n",
      "Fetching news-us-nypost (https://nypost.com/us-news/feed/)… ✓ success\n",
      "Fetching news-world-nypost (https://nypost.com/world-news/feed/)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-politics-nypost (https://nypost.com/politics/feed/)… no change\n",
      "Fetching news-business-nypost (https://nypost.com/business/feed/)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-world-toi (http://timesofindia.indiatimes.com/rssfeeds/296589292.cms)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-business-toi (http://timesofindia.indiatimes.com/rssfeeds/1898055.cms)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-us-toi (https://timesofindia.indiatimes.com/rssfeeds_us/72258322.cms)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-middle-east-toi (http://timesofindia.indiatimes.com/rssfeeds/1898272.cms)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-europe-toi (http://timesofindia.indiatimes.com/rssfeeds/1898274.cms)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-world-cbc (https://www.cbc.ca/webfeed/rss/rss-world)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-politics-cbc (https://www.cbc.ca/webfeed/rss/rss-politics)… no change\n",
      "Fetching news-africa-bbc (http://feeds.bbci.co.uk/news/world/africa/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-asia-bbc (http://feeds.bbci.co.uk/news/world/asia/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-europe-bbc (http://feeds.bbci.co.uk/news/world/europe/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-latin-america-bbc (http://feeds.bbci.co.uk/news/world/latin_america/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-middle-east-bbc (http://feeds.bbci.co.uk/news/world/middle_east/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-bbc (http://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-world-bbc (https://feeds.bbci.co.uk/news/world/rss.xml)… ✓ success\n",
      "Fetching news-business-bbc (https://feeds.bbci.co.uk/news/business/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-politics-bbc (https://feeds.bbci.co.uk/news/politics/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-top-dw (https://rss.dw.com/rdf/rss-en-all)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-europe-dw (https://rss.dw.com/rdf/rss-en-eu)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-world-dw (https://rss.dw.com/rdf/rss-en-world)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-business-dw (https://rss.dw.com/rdf/rss-en-bus)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-asia-dw (https://rss.dw.com/rdf/rss-en-asia)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching zip-demo-ca (nan)… ✗ failed: Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n",
      "\n",
      "Updated: news-us-nyt, news-world-nyt, news-europe-nyt, news-us-politics-nyt, news-markets-wsj, latimes-business, latimes-us, latimes-us-politics, news-us-business-startribune, news-us-nypost, news-world-toi, news-business-toi, news-middle-east-toi, news-asia-bbc, news-latin-america-bbc, news-world-bbc, news-business-bbc, news-top-dw, news-europe-dw\n"
     ]
    }
   ],
   "source": [
    "# ========== Bootstrap: ensure required Python packages are present ==========\n",
    "import importlib, subprocess, sys\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: Optional[str] = None, required: bool = True):\n",
    "    \"\"\"Import a module, installing it if necessary. If installation fails and\n",
    "    the package is required, the exception is raised. Optional packages may\n",
    "    remain unavailable.\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"Package '{pkg_name}' not found - installing ...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install {pkg_name}: {e}\")\n",
    "            if required:\n",
    "                raise\n",
    "    try:\n",
    "        mod = importlib.import_module(import_name or pkg_name)\n",
    "        globals()[import_name or pkg_name] = mod\n",
    "        return mod\n",
    "    except ModuleNotFoundError:\n",
    "        if required:\n",
    "            raise\n",
    "        print(f\"Package '{pkg_name}' is unavailable.\")\n",
    "        globals()[import_name or pkg_name] = None\n",
    "        return None\n",
    "# --- Required third-party libraries ------------------------------------------\n",
    "_ensure(\"pandas\")\n",
    "_ensure(\"requests\")\n",
    "_ensure(\"feedparser\")\n",
    "_ensure(\"textblob\")\n",
    "_ensure(\"jupyter\", required=False)\n",
    "_ensure(\"nbconvert\", required=False)\n",
    "print(\"All dependencies ready.\\n\")\n",
    "\n",
    "# --- Standard imports --------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json, feedparser, textblob\n",
    "import pandas as pd, requests, urllib.parse\n",
    "\n",
    "# --- Helper: replace [date %Y-%m-%d] tokens -----------------------------------\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        fmt = m.group(1).strip()\n",
    "        return dt.date.today().strftime(fmt)\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "# --- Helper: append API key if specified -----------------------------------\n",
    "def add_apikey(url: str, env_var: Optional[str]) -> str:\n",
    "    if env_var and str(env_var).lower() != \"nan\":\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f'{url}{sep}api_key={urllib.parse.quote_plus(key)}'\n",
    "        else:\n",
    "            print(f\"Warning: environment variable '{env_var}' not set.\")\n",
    "    return url\n",
    "\n",
    "# --- Cadence map (word → minimum seconds between fetches) ------------------------\n",
    "CADENCE_SECONDS = {\n",
    "    \"hourly\": 3600,\n",
    "    \"daily\": 86400,\n",
    "    \"weekly\": 604800,\n",
    "    \"monthly\": 2592000,\n",
    "    \"quarterly\": 7776000,\n",
    "}\n",
    "\n",
    "# --- Resolve base directory so notebook works from repo root or data folder ---\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "\n",
    "# --- Load catalog -------------------------------------------------------------\n",
    "catalog_path = BASE_DIR / 'catalog.csv'\n",
    "cat = pd.read_csv(catalog_path)\n",
    "cat['filetype'] = cat['filetype'].astype(str).str.strip().str.lstrip('.')\n",
    "\n",
    "now = dt.datetime.now()\n",
    "today = now.date()\n",
    "updated_rows = []                # remember which rows we refresh\n",
    "\n",
    "for idx, row in cat.iterrows():\n",
    "    folder = BASE_DIR / str(row['category']) / str(row['source']) / str(row['folder'])\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    cadence = str(row['cadence']).lower().strip()\n",
    "    filetype = str(row['filetype']).strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    latest_fp = folder / f'latest.{output_ext}'\n",
    "    url = str(row.get('url', '')).strip()\n",
    "    if not url or url.lower() in ('n/a', 'na', 'none'):\n",
    "        print(f\"Skipping {row['folder']} (static)\")\n",
    "        continue\n",
    "    dated_fp = folder / f\"{now:%Y-%m-%d-%H}.{output_ext}\" if cadence == \"hourly\" else folder / f\"{today:%Y-%m-%d}.{output_ext}\"\n",
    "    if dated_fp.exists():\n",
    "        if (not latest_fp.exists()) or latest_fp.read_bytes() != dated_fp.read_bytes():\n",
    "            shutil.copyfile(dated_fp, latest_fp)\n",
    "        cat.at[idx, 'last_fetched'] = now.isoformat(timespec='minutes')\n",
    "        continue\n",
    "    last_fetched = (\n",
    "        pd.to_datetime(row[\"last_fetched\"])\n",
    "        if pd.notna(row[\"last_fetched\"]) else None\n",
    "    )\n",
    "\n",
    "    # ---- Determine if an update is due --------------------------------------\n",
    "    cadence = str(row[\"cadence\"]).lower().strip()\n",
    "    min_age = CADENCE_SECONDS.get(cadence, 30*86400)        # default 30 days\n",
    "    needs_update = (\n",
    "        (not latest_fp.exists()) or\n",
    "        (not last_fetched) or\n",
    "        (now - last_fetched).total_seconds() >= min_age\n",
    "    )\n",
    "\n",
    "    #if not needs_update:\n",
    "        #print(f\"Skipping {row['folder']} - up to date\")\n",
    "        #continue\n",
    "\n",
    "    # ---- Build the request URL ---------------------------------------------\n",
    "    url = substitute_date_tokens(str(row[\"url\"]))\n",
    "    url = add_apikey(url, str(row.get('api_key') or '').strip() or None)\n",
    "\n",
    "    print(f\"Fetching {row['folder']} ({row['url']})…\", end=\" \")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        r.raise_for_status()\n",
    "        if filetype.lower() in ('rss', 'xml'):\n",
    "            feed = feedparser.parse(r.content)\n",
    "            entries = []\n",
    "            for e in feed.entries:\n",
    "                text = ' '.join(filter(None, [e.get('title'), e.get('summary')]))\n",
    "                polarity = textblob.TextBlob(text).sentiment.polarity\n",
    "                entries.append({'title': e.get('title'), 'link': e.get('link'),\n",
    "                               'published': e.get('published'),\n",
    "                               'sentiment': polarity})\n",
    "            content_bytes = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "        else:\n",
    "            content_bytes = r.content\n",
    "        if filetype.lower() == 'json':\n",
    "            try:\n",
    "                data_json = r.json()\n",
    "            except Exception:\n",
    "                data_json = None\n",
    "            if isinstance(data_json, dict) and data_json.get('error_message'):\n",
    "                raise ValueError(data_json['error_message'])\n",
    "        # ---- Save snapshot and latest --------------------------------------\n",
    "        if latest_fp.exists() and latest_fp.read_bytes() == content_bytes:\n",
    "            cat.at[idx, 'last_fetched'] = now.isoformat(timespec='minutes')\n",
    "            print('no change')\n",
    "            continue\n",
    "        dated_fp.write_bytes(content_bytes)\n",
    "        shutil.copyfile(dated_fp, latest_fp)\n",
    "\n",
    "        # ---- Mark success in catalog ---------------------------------------\n",
    "        cat.at[idx, \"last_fetched\"] = now.isoformat(timespec='minutes')\n",
    "        updated_rows.append(row[\"folder\"])\n",
    "        print(\"✓ success\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ failed: {e}\")\n",
    "\n",
    "# --- Persist catalog if anything changed -------------------------------------\n",
    "if updated_rows:\n",
    "    cat.to_csv(catalog_path, index=False)\n",
    "    print(\"\\nUpdated:\", \", \".join(updated_rows))\n",
    "else:\n",
    "    print(\"Everything up to date.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c044a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:54:59.755766Z",
     "iopub.status.busy": "2025-07-16T05:54:59.755386Z",
     "iopub.status.idle": "2025-07-16T05:54:59.804939Z",
     "shell.execute_reply": "2025-07-16T05:54:59.804372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index files generated for GDPC1, A939RX0Q048SBEA, M2REAL, UNRATE, CLVMNACSCAB1GQDE, GFDEBTN, GFDEGDQ188S, TDSP, news-us-nyt, news-world-nyt, news-africa-nyt, news-europe-nyt, news-asia-nyt, news-americas-nyt, news-middle-east-nyt, news-business-nyt, news-economy-nyt, news-us-politics-nyt, news-world-wsj, news-us-wsj, news-business-wsj, news-markets-wsj, news-economy-wsj, news-us-politics-wsj, news-us-politics-wapo, news-us-wapo, news-world-wapo, news-business-wapo, latimes-business, latimes-us, latimes-us-politics, news-world-chi-tribune, news-business-chi-tribune, news-us-politics-chi-tribune, news-us-business-startribune, news-us-politics-startribune, news-us-nypost, news-world-nypost, news-us-politics-nypost, news-business-nypost, news-world-toi, news-business-toi, news-us-toi, news-middle-east-toi, news-europe-toi, news-world-cbc, news-politics-cbc, news-africa-bbc, news-asia-bbc, news-europe-bbc, news-latin-america-bbc, news-middle-east-bbc, news-us-bbc, news-world-bbc, news-business-bbc, news-politics-bbc, news-top-dw, news-europe-dw, news-world-dw, news-business-dw, news-asia-dw, zip-demo-ca\n"
     ]
    }
   ],
   "source": [
    "# This cell updates the markdown index files for all the data sources\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "with open(BASE_DIR / 'catalog.csv', newline='') as f:\n",
    "    cat = list(csv.DictReader(f))\n",
    "\n",
    "for row in cat:\n",
    "    folder = BASE_DIR / row['category'] / row['source'] / row['folder']\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = row['filetype'].strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    desc = row['description'].strip()\n",
    "    source = row['source'].strip()\n",
    "    date = row.get('last_fetched', '').strip()\n",
    "\n",
    "    pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}(?:-\\d{2})?\\.\" + re.escape(output_ext) + r\"$\")\n",
    "    dated_files = sorted(p.name for p in folder.iterdir() if pattern.match(p.name))\n",
    "\n",
    "    lines = [\n",
    "        '---',\n",
    "        'layout: default',\n",
    "        f'title: {source} - {desc}',\n",
    "        f'date: {date}',\n",
    "        '---',\n",
    "        '',\n",
    "        f'## {source} - {desc}',\n",
    "        '',\n",
    "        '<div id=\"data-chart\"></div>',\n",
    "        '<div id=\"data-table\"></div>',\n",
    "    ]\n",
    "\n",
    "    if row['source'] == 'fred' and filetype == 'json':\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  ShowChart($('#data-chart'));\",\n",
    "            \"  SourceTabler($('#data-table'));\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "    else:\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  document.getElementById('data-table').textContent = 'This source isn\\'t supported for tables yet.';\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "\n",
    "    lines += [\n",
    "        '',\n",
    "        '## File Versions:',\n",
    "    ]\n",
    "    links = [f'[Latest version](./latest.{output_ext})'] + [f'[{fname}](./{fname})' for fname in dated_files]\n",
    "    for i, link in enumerate(links, 1):\n",
    "        lines.append(f'{i}. {link}')\n",
    "    (folder / 'index.md').write_text(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print('Index files generated for', ', '.join(r['folder'] for r in cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25e4e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:54:59.807086Z",
     "iopub.status.busy": "2025-07-16T05:54:59.806658Z",
     "iopub.status.idle": "2025-07-16T05:55:07.001072Z",
     "shell.execute_reply": "2025-07-16T05:55:07.000513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb …\n",
      "  Command: /opt/hostedtoolcache/Python/3.13.5/x64/bin/python -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug /home/runner/work/Analysis/Analysis/analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Searching ['/home/runner/.jupyter', '/home/runner/.local/etc/jupyter', '/opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'update_headlines-checkpoint'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['/opt/hostedtoolcache/Python/3.13.5/x64/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmpcu4u9bfj.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:53647\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:49827\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:49827\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:43539\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:43539\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:52383\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:52383\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:44045\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:53647\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:53647\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "print('No external dependencies required.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\nprint('No external dependencies required.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'No external dependencies required.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "# Some of the dependencies, to trigger the dependency checker:\n",
      "# data/news-us/nyt/news-us-nyt/latest.json\n",
      "# data/news-us/wsj/news-us-wsj/latest.json\n",
      "\n",
      "\n",
      "from pathlib import Path\n",
      "import csv\n",
      "import json\n",
      "import xml.etree.ElementTree as ET\n",
      "from datetime import datetime, timezone, timedelta\n",
      "from email.utils import parsedate_to_datetime\n",
      "import shutil\n",
      "\n",
      "BASE_DIR = Path.cwd()\n",
      "REPO_DIR = BASE_DIR\n",
      "while not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):\n",
      "    if REPO_DIR.parent == REPO_DIR:\n",
      "        raise FileNotFoundError('Repository root not found')\n",
      "    REPO_DIR = REPO_DIR.parent\n",
      "DATA_DIR = REPO_DIR / 'data'\n",
      "HEADLINES_DIR = REPO_DIR / 'analysis/headlines'\n",
      "HEADLINES_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "def parse_pubdate(date_str):\n",
      "    try:\n",
      "        dt = parsedate_to_datetime(date_str) if date_str else None\n",
      "        if dt is None:\n",
      "            return None\n",
      "        if dt.tzinfo is None:\n",
      "            dt = dt.replace(tzinfo=timezone.utc)\n",
      "        return dt.astimezone(timezone.utc)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def format_pubdate(dt):\n",
      "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''\n",
      "\n",
      "def parse_feed(path: Path):\n",
      "    entries = []\n",
      "    if path.suffix == '.json':\n",
      "        with open(path, 'r', encoding='utf-8') as f:\n",
      "            data = json.load(f)\n",
      "        for item in data.get('entries', []):\n",
      "            title = item.get('title')\n",
      "            link = item.get('link')\n",
      "            pub = parse_pubdate(item.get('published'))\n",
      "            if title and link:\n",
      "                entries.append((pub, title.strip(), link.strip()))\n",
      "    else:\n",
      "        try:\n",
      "            tree = ET.parse(path)\n",
      "            root = tree.getroot()\n",
      "        except ET.ParseError:\n",
      "            return entries\n",
      "        for item in root.iter():\n",
      "            if item.tag.lower().endswith(('item', 'entry')):\n",
      "                title = None\n",
      "                link = None\n",
      "                pub = None\n",
      "                for child in item:\n",
      "                    tag = child.tag.lower()\n",
      "                    if tag.endswith('title'):\n",
      "                        title = (child.text or '').strip()\n",
      "                    if tag.endswith('link'):\n",
      "                        link = (child.text or '').strip() or child.attrib.get('href')\n",
      "                    if tag.endswith(('pubdate', 'published', 'updated')):\n",
      "                        pub = parse_pubdate((child.text or '').strip())\n",
      "                if title and link:\n",
      "                    entries.append((pub, title, link))\n",
      "    return entries\n",
      "\n",
      "def collect_headlines():\n",
      "    all_entries = []\n",
      "    for source in DATA_DIR.iterdir():\n",
      "        if source.is_dir() and source.name.startswith('news'):\n",
      "            candidates = [p for p in source.rglob('latest.*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                candidates = [p for p in source.rglob('*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                continue\n",
      "            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
      "            source_name = latest_file.relative_to(DATA_DIR).parts[1]\n",
      "            for pub, title, link in parse_feed(latest_file):\n",
      "                all_entries.append((pub, title, link, source_name))\n",
      "    return all_entries\n",
      "\n",
      "def _date_key(date_str):\n",
      "    try:\n",
      "        return parsedate_to_datetime(date_str) if date_str else datetime.min\n",
      "    except Exception:\n",
      "        return datetime.min\n",
      "\n",
      "def update_headlines():\n",
      "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\n",
      "    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\n",
      "    if hourly_file.exists():\n",
      "        print(f\"{hourly_file.name} already exists. Skipping update.\")\n",
      "        return\n",
      "    entries = collect_headlines()\n",
      "    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
      "    deduped = []\n",
      "    seen_titles = set()\n",
      "    seen_links = set()\n",
      "    for pub, title, link, src in entries:\n",
      "        t_key = title.lower()\n",
      "        l_key = link.lower()\n",
      "        if t_key in seen_titles or l_key in seen_links:\n",
      "            continue\n",
      "        deduped.append((pub, src, title, link))\n",
      "        seen_titles.add(t_key)\n",
      "        seen_links.add(l_key)\n",
      "    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\n",
      "    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\n",
      "    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        writer.writerow(['pubdate', 'source', 'title', 'link'])\n",
      "        for pub, src, title, link in deduped:\n",
      "            writer.writerow([format_pubdate(pub), src, title, link])\n",
      "    latest_file = HEADLINES_DIR / \"latest.csv\"\n",
      "    shutil.copy(hourly_file, latest_file)\n",
      "    print(f\"Wrote {hourly_file} and updated latest.csv\")\n",
      "    # Get the most recent headline\n",
      "\n",
      "\n",
      "update_headlines()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': '# Some of the dependencies, to trigger the dependency checker:\\n# data/news-us/nyt/news-us-nyt/latest.json\\n# data/news-us/wsj/news-us-wsj/latest.json\\n\\n\\nfrom pathlib import Path\\nimport csv\\nimport json\\nimport xml.etree.ElementTree as ET\\nfrom datetime import datetime, timezone, timedelta\\nfrom email.utils import parsedate_to_datetime\\nimport shutil\\n\\nBASE_DIR = Path.cwd()\\nREPO_DIR = BASE_DIR\\nwhile not ((REPO_DIR / \\'data\\').exists() and (REPO_DIR / \\'analysis\\').exists()):\\n    if REPO_DIR.parent == REPO_DIR:\\n        raise FileNotFoundError(\\'Repository root not found\\')\\n    REPO_DIR = REPO_DIR.parent\\nDATA_DIR = REPO_DIR / \\'data\\'\\nHEADLINES_DIR = REPO_DIR / \\'analysis/headlines\\'\\nHEADLINES_DIR.mkdir(parents=True, exist_ok=True)\\n\\ndef parse_pubdate(date_str):\\n    try:\\n        dt = parsedate_to_datetime(date_str) if date_str else None\\n        if dt is None:\\n            return None\\n        if dt.tzinfo is None:\\n            dt = dt.replace(tzinfo=timezone.utc)\\n        return dt.astimezone(timezone.utc)\\n    except Exception:\\n        return None\\n\\ndef format_pubdate(dt):\\n    return dt.strftime(\\'%Y-%m-%d-%H-%M-%S +0000\\') if dt else \\'\\'\\n\\ndef parse_feed(path: Path):\\n    entries = []\\n    if path.suffix == \\'.json\\':\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            data = json.load(f)\\n        for item in data.get(\\'entries\\', []):\\n            title = item.get(\\'title\\')\\n            link = item.get(\\'link\\')\\n            pub = parse_pubdate(item.get(\\'published\\'))\\n            if title and link:\\n                entries.append((pub, title.strip(), link.strip()))\\n    else:\\n        try:\\n            tree = ET.parse(path)\\n            root = tree.getroot()\\n        except ET.ParseError:\\n            return entries\\n        for item in root.iter():\\n            if item.tag.lower().endswith((\\'item\\', \\'entry\\')):\\n                title = None\\n                link = None\\n                pub = None\\n                for child in item:\\n                    tag = child.tag.lower()\\n                    if tag.endswith(\\'title\\'):\\n                        title = (child.text or \\'\\').strip()\\n                    if tag.endswith(\\'link\\'):\\n                        link = (child.text or \\'\\').strip() or child.attrib.get(\\'href\\')\\n                    if tag.endswith((\\'pubdate\\', \\'published\\', \\'updated\\')):\\n                        pub = parse_pubdate((child.text or \\'\\').strip())\\n                if title and link:\\n                    entries.append((pub, title, link))\\n    return entries\\n\\ndef collect_headlines():\\n    all_entries = []\\n    for source in DATA_DIR.iterdir():\\n        if source.is_dir() and source.name.startswith(\\'news\\'):\\n            candidates = [p for p in source.rglob(\\'latest.*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                candidates = [p for p in source.rglob(\\'*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                continue\\n            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\\n            source_name = latest_file.relative_to(DATA_DIR).parts[1]\\n            for pub, title, link in parse_feed(latest_file):\\n                all_entries.append((pub, title, link, source_name))\\n    return all_entries\\n\\ndef _date_key(date_str):\\n    try:\\n        return parsedate_to_datetime(date_str) if date_str else datetime.min\\n    except Exception:\\n        return datetime.min\\n\\ndef update_headlines():\\n    timestamp = datetime.utcnow().strftime(\\'%Y-%m-%d-%H-00\\')\\n    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\\n    if hourly_file.exists():\\n        print(f\"{hourly_file.name} already exists. Skipping update.\")\\n        return\\n    entries = collect_headlines()\\n    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\\n    deduped = []\\n    seen_titles = set()\\n    seen_links = set()\\n    for pub, title, link, src in entries:\\n        t_key = title.lower()\\n        l_key = link.lower()\\n        if t_key in seen_titles or l_key in seen_links:\\n            continue\\n        deduped.append((pub, src, title, link))\\n        seen_titles.add(t_key)\\n        seen_links.add(l_key)\\n    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\\n    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\\n    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n        writer = csv.writer(f)\\n        writer.writerow([\\'pubdate\\', \\'source\\', \\'title\\', \\'link\\'])\\n        for pub, src, title, link in deduped:\\n            writer.writerow([format_pubdate(pub), src, title, link])\\n    latest_file = HEADLINES_DIR / \"latest.csv\"\\n    shutil.copy(hourly_file, latest_file)\\n    print(f\"Wrote {hourly_file} and updated latest.csv\")\\n    # Get the most recent headline\\n\\n\\nupdate_headlines()\\n\\n\\n\\n\\n\\n', 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'Wrote /home/runner/work/Analysis/Analysis/analysis/headlines/2025-07-16-05-00.csv and updated latest.csv\\n'}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"/tmp/ipykernel_2283/2800259878.py:94: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\\n\"}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 3\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7f51ab1f41a0>\n",
      "[NbConvertApp] Writing 8844 bytes to /home/runner/work/Analysis/Analysis/analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Running analysis/headlines/update_headlines.ipynb …\n",
      "  Command: /opt/hostedtoolcache/Python/3.13.5/x64/bin/python -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Searching ['/home/runner/.jupyter', '/home/runner/.local/etc/jupyter', '/opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'update_headlines'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['/opt/hostedtoolcache/Python/3.13.5/x64/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmpumyh525b.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55883\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:48713\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:48713\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:32943\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:32943\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:51451\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:51451\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:52525\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:55883\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55883\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "print('No external dependencies required.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\nprint('No external dependencies required.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'No external dependencies required.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "# Some of the dependencies, to trigger the dependency checker:\n",
      "# data/news-us/nyt/news-us-nyt/latest.json\n",
      "# data/news-us/wsj/news-us-wsj/latest.json\n",
      "\n",
      "\n",
      "from pathlib import Path\n",
      "import csv\n",
      "import json\n",
      "import xml.etree.ElementTree as ET\n",
      "from datetime import datetime, timezone, timedelta\n",
      "from email.utils import parsedate_to_datetime\n",
      "import shutil\n",
      "\n",
      "BASE_DIR = Path.cwd()\n",
      "REPO_DIR = BASE_DIR\n",
      "while not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):\n",
      "    if REPO_DIR.parent == REPO_DIR:\n",
      "        raise FileNotFoundError('Repository root not found')\n",
      "    REPO_DIR = REPO_DIR.parent\n",
      "DATA_DIR = REPO_DIR / 'data'\n",
      "HEADLINES_DIR = REPO_DIR / 'analysis/headlines'\n",
      "HEADLINES_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "def parse_pubdate(date_str):\n",
      "    try:\n",
      "        dt = parsedate_to_datetime(date_str) if date_str else None\n",
      "        if dt is None:\n",
      "            return None\n",
      "        if dt.tzinfo is None:\n",
      "            dt = dt.replace(tzinfo=timezone.utc)\n",
      "        return dt.astimezone(timezone.utc)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def format_pubdate(dt):\n",
      "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''\n",
      "\n",
      "def parse_feed(path: Path):\n",
      "    entries = []\n",
      "    if path.suffix == '.json':\n",
      "        with open(path, 'r', encoding='utf-8') as f:\n",
      "            data = json.load(f)\n",
      "        for item in data.get('entries', []):\n",
      "            title = item.get('title')\n",
      "            link = item.get('link')\n",
      "            pub = parse_pubdate(item.get('published'))\n",
      "            if title and link:\n",
      "                entries.append((pub, title.strip(), link.strip()))\n",
      "    else:\n",
      "        try:\n",
      "            tree = ET.parse(path)\n",
      "            root = tree.getroot()\n",
      "        except ET.ParseError:\n",
      "            return entries\n",
      "        for item in root.iter():\n",
      "            if item.tag.lower().endswith(('item', 'entry')):\n",
      "                title = None\n",
      "                link = None\n",
      "                pub = None\n",
      "                for child in item:\n",
      "                    tag = child.tag.lower()\n",
      "                    if tag.endswith('title'):\n",
      "                        title = (child.text or '').strip()\n",
      "                    if tag.endswith('link'):\n",
      "                        link = (child.text or '').strip() or child.attrib.get('href')\n",
      "                    if tag.endswith(('pubdate', 'published', 'updated')):\n",
      "                        pub = parse_pubdate((child.text or '').strip())\n",
      "                if title and link:\n",
      "                    entries.append((pub, title, link))\n",
      "    return entries\n",
      "\n",
      "def collect_headlines():\n",
      "    all_entries = []\n",
      "    feed_info = {}\n",
      "    for source in DATA_DIR.iterdir():\n",
      "        if source.is_dir() and source.name.startswith('news'):\n",
      "            candidates = [p for p in source.rglob('latest.*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                candidates = [p for p in source.rglob('*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                continue\n",
      "            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
      "            source_name = latest_file.relative_to(DATA_DIR).parts[1]\n",
      "            feed_entries = parse_feed(latest_file)\n",
      "            if feed_entries:\n",
      "                recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)\n",
      "                feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}\n",
      "            for pub, title, link in feed_entries:\n",
      "                all_entries.append((pub, title, link, source_name))\n",
      "    return all_entries, feed_info\n",
      "\n",
      "def _date_key(date_str):\n",
      "    try:\n",
      "        return parsedate_to_datetime(date_str) if date_str else datetime.min\n",
      "    except Exception:\n",
      "        return datetime.min\n",
      "\n",
      "def update_headlines():\n",
      "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\n",
      "    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\n",
      "    if hourly_file.exists():\n",
      "        print(f\"{hourly_file.name} already exists. Skipping update.\")\n",
      "        return\n",
      "    entries, feed_info = collect_headlines()\n",
      "    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
      "    deduped = []\n",
      "    seen_titles = set()\n",
      "    seen_links = set()\n",
      "    for pub, title, link, src in entries:\n",
      "        t_key = title.lower()\n",
      "        l_key = link.lower()\n",
      "        if t_key in seen_titles or l_key in seen_links:\n",
      "            continue\n",
      "        deduped.append((pub, src, title, link))\n",
      "        seen_titles.add(t_key)\n",
      "        seen_links.add(l_key)\n",
      "    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\n",
      "    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\n",
      "    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        writer.writerow(['pubdate', 'source', 'title', 'link'])\n",
      "        for pub, src, title, link in deduped:\n",
      "            writer.writerow([format_pubdate(pub), src, title, link])\n",
      "    latest_file = HEADLINES_DIR / \"latest.csv\"\n",
      "    shutil.copy(hourly_file, latest_file)\n",
      "    print(f\"Wrote {hourly_file} and updated latest.csv\")\n",
      "    print()\n",
      "    print('Feed summary:')\n",
      "    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")\n",
      "    for src, info in sorted(feed_info.items()):\n",
      "        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")\n",
      "    # Get the most recent headline\n",
      "\n",
      "\n",
      "update_headlines()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': '# Some of the dependencies, to trigger the dependency checker:\\n# data/news-us/nyt/news-us-nyt/latest.json\\n# data/news-us/wsj/news-us-wsj/latest.json\\n\\n\\nfrom pathlib import Path\\nimport csv\\nimport json\\nimport xml.etree.ElementTree as ET\\nfrom datetime import datetime, timezone, timedelta\\nfrom email.utils import parsedate_to_datetime\\nimport shutil\\n\\nBASE_DIR = Path.cwd()\\nREPO_DIR = BASE_DIR\\nwhile not ((REPO_DIR / \\'data\\').exists() and (REPO_DIR / \\'analysis\\').exists()):\\n    if REPO_DIR.parent == REPO_DIR:\\n        raise FileNotFoundError(\\'Repository root not found\\')\\n    REPO_DIR = REPO_DIR.parent\\nDATA_DIR = REPO_DIR / \\'data\\'\\nHEADLINES_DIR = REPO_DIR / \\'analysis/headlines\\'\\nHEADLINES_DIR.mkdir(parents=True, exist_ok=True)\\n\\ndef parse_pubdate(date_str):\\n    try:\\n        dt = parsedate_to_datetime(date_str) if date_str else None\\n        if dt is None:\\n            return None\\n        if dt.tzinfo is None:\\n            dt = dt.replace(tzinfo=timezone.utc)\\n        return dt.astimezone(timezone.utc)\\n    except Exception:\\n        return None\\n\\ndef format_pubdate(dt):\\n    return dt.strftime(\\'%Y-%m-%d-%H-%M-%S +0000\\') if dt else \\'\\'\\n\\ndef parse_feed(path: Path):\\n    entries = []\\n    if path.suffix == \\'.json\\':\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            data = json.load(f)\\n        for item in data.get(\\'entries\\', []):\\n            title = item.get(\\'title\\')\\n            link = item.get(\\'link\\')\\n            pub = parse_pubdate(item.get(\\'published\\'))\\n            if title and link:\\n                entries.append((pub, title.strip(), link.strip()))\\n    else:\\n        try:\\n            tree = ET.parse(path)\\n            root = tree.getroot()\\n        except ET.ParseError:\\n            return entries\\n        for item in root.iter():\\n            if item.tag.lower().endswith((\\'item\\', \\'entry\\')):\\n                title = None\\n                link = None\\n                pub = None\\n                for child in item:\\n                    tag = child.tag.lower()\\n                    if tag.endswith(\\'title\\'):\\n                        title = (child.text or \\'\\').strip()\\n                    if tag.endswith(\\'link\\'):\\n                        link = (child.text or \\'\\').strip() or child.attrib.get(\\'href\\')\\n                    if tag.endswith((\\'pubdate\\', \\'published\\', \\'updated\\')):\\n                        pub = parse_pubdate((child.text or \\'\\').strip())\\n                if title and link:\\n                    entries.append((pub, title, link))\\n    return entries\\n\\ndef collect_headlines():\\n    all_entries = []\\n    feed_info = {}\\n    for source in DATA_DIR.iterdir():\\n        if source.is_dir() and source.name.startswith(\\'news\\'):\\n            candidates = [p for p in source.rglob(\\'latest.*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                candidates = [p for p in source.rglob(\\'*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                continue\\n            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\\n            source_name = latest_file.relative_to(DATA_DIR).parts[1]\\n            feed_entries = parse_feed(latest_file)\\n            if feed_entries:\\n                recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)\\n                feed_info[source_name] = {\\'count\\': len(feed_entries), \\'recent\\': recent}\\n            for pub, title, link in feed_entries:\\n                all_entries.append((pub, title, link, source_name))\\n    return all_entries, feed_info\\n\\ndef _date_key(date_str):\\n    try:\\n        return parsedate_to_datetime(date_str) if date_str else datetime.min\\n    except Exception:\\n        return datetime.min\\n\\ndef update_headlines():\\n    timestamp = datetime.utcnow().strftime(\\'%Y-%m-%d-%H-00\\')\\n    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\\n    if hourly_file.exists():\\n        print(f\"{hourly_file.name} already exists. Skipping update.\")\\n        return\\n    entries, feed_info = collect_headlines()\\n    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\\n    deduped = []\\n    seen_titles = set()\\n    seen_links = set()\\n    for pub, title, link, src in entries:\\n        t_key = title.lower()\\n        l_key = link.lower()\\n        if t_key in seen_titles or l_key in seen_links:\\n            continue\\n        deduped.append((pub, src, title, link))\\n        seen_titles.add(t_key)\\n        seen_links.add(l_key)\\n    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\\n    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\\n    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n        writer = csv.writer(f)\\n        writer.writerow([\\'pubdate\\', \\'source\\', \\'title\\', \\'link\\'])\\n        for pub, src, title, link in deduped:\\n            writer.writerow([format_pubdate(pub), src, title, link])\\n    latest_file = HEADLINES_DIR / \"latest.csv\"\\n    shutil.copy(hourly_file, latest_file)\\n    print(f\"Wrote {hourly_file} and updated latest.csv\")\\n    print()\\n    print(\\'Feed summary:\\')\\n    print(f\"{\\'source\\':<20} {\\'count\\':>5}  {\\'most recent\\'}\")\\n    for src, info in sorted(feed_info.items()):\\n        print(f\"{src:<20} {info[\\'count\\']:5}  {format_pubdate(info[\\'recent\\'])}\")\\n    # Get the most recent headline\\n\\n\\nupdate_headlines()\\n\\n\\n\\n\\n\\n', 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': '2025-07-16-05-00.csv already exists. Skipping update.\\n'}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"/tmp/ipykernel_2304/1894882967.py:99: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\\n\"}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7efe40fc8050>\n",
      "[NbConvertApp] Writing 9209 bytes to /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Running analysis/news-topics/analyze_headlines.ipynb …\n",
      "  Command: /opt/hostedtoolcache/Python/3.13.5/x64/bin/python -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Searching ['/home/runner/.jupyter', '/home/runner/.local/etc/jupyter', '/opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'analyze_headlines'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['/opt/hostedtoolcache/Python/3.13.5/x64/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmpp_jf5q_u.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:52567\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:48919\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:48919\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:55101\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55101\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:46775\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:46775\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:50559\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:52567\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:52567\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "_ensure('pandas')\n",
      "print('All dependencies ready.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\n_ensure('pandas')\\nprint('All dependencies ready.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'All dependencies ready.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "import pandas as pd\n",
      "latest = pd.read_csv('../headlines/latest.csv')\n",
      "latest.head()\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import pandas as pd\\nlatest = pd.read_csv('../headlines/latest.csv')\\nlatest.head()\", 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': \"                     pubdate  source  \\\\\\n0  2025-07-16-05-41-46 +0000  nypost   \\n1  2025-07-16-05-37-50 +0000     bbc   \\n2  2025-07-16-05-22-29 +0000  nypost   \\n3  2025-07-16-04-56-20 +0000     bbc   \\n4  2025-07-16-04-46-03 +0000     bbc   \\n\\n                                               title  \\\\\\n0  Burger King worker, mom of 3 Nykia Hamilton fo...   \\n1  Driver held for hit-and-run death of world's '...   \\n2  Polite black bear stuns New Hampshire homeowne...   \\n3  Man who murdered British backpacker Peter Falc...   \\n4  S Korea arrests teacher and parent for exam pa...   \\n\\n                                                link  \\n0  https://nypost.com/2025/07/16/us-news/burger-k...  \\n1     https://www.bbc.com/news/articles/c2d0yep9d37o  \\n2  https://nypost.com/2025/07/16/us-news/polite-b...  \\n3     https://www.bbc.com/news/articles/cjrlg4x8e78o  \\n4     https://www.bbc.com/news/articles/cq6m17y8g25o  \", 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>2025-07-16-05-41-46 +0000</td>\\n      <td>nypost</td>\\n      <td>Burger King worker, mom of 3 Nykia Hamilton fo...</td>\\n      <td>https://nypost.com/2025/07/16/us-news/burger-k...</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2025-07-16-05-37-50 +0000</td>\\n      <td>bbc</td>\\n      <td>Driver held for hit-and-run death of world\\'s \\'...</td>\\n      <td>https://www.bbc.com/news/articles/c2d0yep9d37o</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2025-07-16-05-22-29 +0000</td>\\n      <td>nypost</td>\\n      <td>Polite black bear stuns New Hampshire homeowne...</td>\\n      <td>https://nypost.com/2025/07/16/us-news/polite-b...</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2025-07-16-04-56-20 +0000</td>\\n      <td>bbc</td>\\n      <td>Man who murdered British backpacker Peter Falc...</td>\\n      <td>https://www.bbc.com/news/articles/cjrlg4x8e78o</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>2025-07-16-04-46-03 +0000</td>\\n      <td>bbc</td>\\n      <td>S Korea arrests teacher and parent for exam pa...</td>\\n      <td>https://www.bbc.com/news/articles/cq6m17y8g25o</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 3\n",
      "[NbConvertApp] Executing cell:\n",
      "import re\n",
      "from collections import Counter\n",
      "from datetime import datetime\n",
      "\n",
      "with open('exclude.txt') as f:\n",
      "    stop_words = set(w.strip() for w in f if w.strip())\n",
      "words = re.findall(r'[A-Za-z]+', ' '.join(latest['title']).lower())\n",
      "filtered = [w for w in words if w not in stop_words and len(w) > 1]\n",
      "counts = Counter(filtered)\n",
      "score_df = (\n",
      "    pd.DataFrame(counts.items(), columns=['word','score'])\n",
      "    .sort_values('score', ascending=False)\n",
      ")\n",
      "score_df[['score','word']].to_csv('scores.csv', index=False)\n",
      "timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\n",
      "score_df[['score','word']].to_csv(f'scores-{timestamp}.csv', index=False)\n",
      "score_df.head()\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import re\\nfrom collections import Counter\\nfrom datetime import datetime\\n\\nwith open('exclude.txt') as f:\\n    stop_words = set(w.strip() for w in f if w.strip())\\nwords = re.findall(r'[A-Za-z]+', ' '.join(latest['title']).lower())\\nfiltered = [w for w in words if w not in stop_words and len(w) > 1]\\ncounts = Counter(filtered)\\nscore_df = (\\n    pd.DataFrame(counts.items(), columns=['word','score'])\\n    .sort_values('score', ascending=False)\\n)\\nscore_df[['score','word']].to_csv('scores.csv', index=False)\\ntimestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\\nscore_df[['score','word']].to_csv(f'scores-{timestamp}.csv', index=False)\\nscore_df.head()\\n\", 'execution_count': 3}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"/tmp/ipykernel_2323/2741963311.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\\n\"}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '          word  score\\n132      trump     14\\n191         up      5\\n41        dies      4\\n53   president      4\\n26         new      4', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>word</th>\\n      <th>score</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>132</th>\\n      <td>trump</td>\\n      <td>14</td>\\n    </tr>\\n    <tr>\\n      <th>191</th>\\n      <td>up</td>\\n      <td>5</td>\\n    </tr>\\n    <tr>\\n      <th>41</th>\\n      <td>dies</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>53</th>\\n      <td>president</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>26</th>\\n      <td>new</td>\\n      <td>4</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 3}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 5\n",
      "[NbConvertApp] Executing cell:\n",
      "word_scores = dict(score_df[['word','score']].values)\n",
      "latest['score'] = latest['title'].apply(\n",
      "    lambda t: sum(\n",
      "        word_scores.get(w.lower(), 0)\n",
      "        for w in re.findall(r'[A-Za-z]+', t)\n",
      "        if len(w) > 1\n",
      "    )\n",
      ")\n",
      "ranked = latest.sort_values('score', ascending=False)\n",
      "ranked[['score','pubdate','source','title','link']].to_csv('rank.csv', index=False)\n",
      "ranked[['score','pubdate','source','title','link']].to_csv(f'rank-{timestamp}.csv', index=False)\n",
      "ranked.head()\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"word_scores = dict(score_df[['word','score']].values)\\nlatest['score'] = latest['title'].apply(\\n    lambda t: sum(\\n        word_scores.get(w.lower(), 0)\\n        for w in re.findall(r'[A-Za-z]+', t)\\n        if len(w) > 1\\n    )\\n)\\nranked = latest.sort_values('score', ascending=False)\\nranked[['score','pubdate','source','title','link']].to_csv('rank.csv', index=False)\\nranked[['score','pubdate','source','title','link']].to_csv(f'rank-{timestamp}.csv', index=False)\\nranked.head()\\n\", 'execution_count': 4}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '                      pubdate   source  \\\\\\n24  2025-07-16-00-00-00 +0000      wsj   \\n43  2025-07-15-19-03-00 +0000      wsj   \\n20  2025-07-16-01-00-00 +0000      wsj   \\n18  2025-07-16-01-27-41 +0000   nypost   \\n35  2025-07-15-22-36-03 +0000  latimes   \\n\\n                                                title  \\\\\\n24  President Trump wants tariffs, the higher the ...   \\n43  JPMorgan CEO Jamie Dimon sounded Wall Street’s...   \\n20  The impact of President Trump’s whirlwind six ...   \\n18  Joe Rogan rips DOJ’s handling of Epstein files...   \\n35  Trump accuses Schiff of mortgage fraud, which ...   \\n\\n                                                 link  score  \\n24  https://www.wsj.com/economy/trade/forget-taco-...     34  \\n43  https://www.wsj.com/economy/central-banking/di...     33  \\n20  https://www.wsj.com/economy/trump-effect-start...     31  \\n18  https://nypost.com/2025/07/15/us-news/joe-roga...     30  \\n35  https://www.latimes.com/politics/story/2025-07...     28  ', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n      <th>score</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>24</th>\\n      <td>2025-07-16-00-00-00 +0000</td>\\n      <td>wsj</td>\\n      <td>President Trump wants tariffs, the higher the ...</td>\\n      <td>https://www.wsj.com/economy/trade/forget-taco-...</td>\\n      <td>34</td>\\n    </tr>\\n    <tr>\\n      <th>43</th>\\n      <td>2025-07-15-19-03-00 +0000</td>\\n      <td>wsj</td>\\n      <td>JPMorgan CEO Jamie Dimon sounded Wall Street’s...</td>\\n      <td>https://www.wsj.com/economy/central-banking/di...</td>\\n      <td>33</td>\\n    </tr>\\n    <tr>\\n      <th>20</th>\\n      <td>2025-07-16-01-00-00 +0000</td>\\n      <td>wsj</td>\\n      <td>The impact of President Trump’s whirlwind six ...</td>\\n      <td>https://www.wsj.com/economy/trump-effect-start...</td>\\n      <td>31</td>\\n    </tr>\\n    <tr>\\n      <th>18</th>\\n      <td>2025-07-16-01-27-41 +0000</td>\\n      <td>nypost</td>\\n      <td>Joe Rogan rips DOJ’s handling of Epstein files...</td>\\n      <td>https://nypost.com/2025/07/15/us-news/joe-roga...</td>\\n      <td>30</td>\\n    </tr>\\n    <tr>\\n      <th>35</th>\\n      <td>2025-07-15-22-36-03 +0000</td>\\n      <td>latimes</td>\\n      <td>Trump accuses Schiff of mortgage fraud, which ...</td>\\n      <td>https://www.latimes.com/politics/story/2025-07...</td>\\n      <td>28</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 4}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 7\n",
      "[NbConvertApp] Executing cell:\n",
      "top_rows = []\n",
      "working = word_scores.copy()\n",
      "remaining = latest.copy()\n",
      "for _ in range(10):\n",
      "    ranked_loop = remaining.assign(score=remaining['title'].apply(\n",
      "        lambda t: sum(working.get(w.lower(), 0)\n",
      "                      for w in re.findall(r'[A-Za-z]+', t)\n",
      "                      if len(w) > 1)\n",
      "    )).sort_values('score', ascending=False)\n",
      "    if ranked_loop.empty:\n",
      "        break\n",
      "    top_story = ranked_loop.iloc[0]\n",
      "    top_rows.append(top_story[['score','pubdate','source','title','link']])\n",
      "    words = set(re.findall(r'[A-Za-z]+', top_story['title'].lower()))\n",
      "    for w in words:\n",
      "        working.pop(w, None)\n",
      "    remaining = remaining.drop(top_story.name)\n",
      "top_df = pd.DataFrame(top_rows)\n",
      "top_df.to_csv('top.csv', index=False)\n",
      "top_df.to_csv(f'top-{timestamp}.csv', index=False)\n",
      "top_df\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"top_rows = []\\nworking = word_scores.copy()\\nremaining = latest.copy()\\nfor _ in range(10):\\n    ranked_loop = remaining.assign(score=remaining['title'].apply(\\n        lambda t: sum(working.get(w.lower(), 0)\\n                      for w in re.findall(r'[A-Za-z]+', t)\\n                      if len(w) > 1)\\n    )).sort_values('score', ascending=False)\\n    if ranked_loop.empty:\\n        break\\n    top_story = ranked_loop.iloc[0]\\n    top_rows.append(top_story[['score','pubdate','source','title','link']])\\n    words = set(re.findall(r'[A-Za-z]+', top_story['title'].lower()))\\n    for w in words:\\n        working.pop(w, None)\\n    remaining = remaining.drop(top_story.name)\\ntop_df = pd.DataFrame(top_rows)\\ntop_df.to_csv('top.csv', index=False)\\ntop_df.to_csv(f'top-{timestamp}.csv', index=False)\\ntop_df\\n\", 'execution_count': 5}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': \"    score                    pubdate  source  \\\\\\n24     34  2025-07-16-00-00-00 +0000     wsj   \\n40     23  2025-07-15-20-41-00 +0000     wsj   \\n12     23  2025-07-16-02-33-54 +0000  nypost   \\n2      20  2025-07-16-05-22-29 +0000  nypost   \\n1      19  2025-07-16-05-37-50 +0000     bbc   \\n43     19  2025-07-15-19-03-00 +0000     wsj   \\n15     19  2025-07-16-01-56-15 +0000  nypost   \\n8      17  2025-07-16-03-49-27 +0000  nypost   \\n25     16  2025-07-15-23-50-44 +0000  nypost   \\n0      14  2025-07-16-05-41-46 +0000  nypost   \\n\\n                                                title  \\\\\\n24  President Trump wants tariffs, the higher the ...   \\n40  Inflation picked up in June, a potential sign ...   \\n12  Beloved teacher dies in 100-foot fall just 2 m...   \\n2   Polite black bear stuns New Hampshire homeowne...   \\n1   Driver held for hit-and-run death of world's '...   \\n43  JPMorgan CEO Jamie Dimon sounded Wall Street’s...   \\n15  NYC Council rejects land use change for Bally’...   \\n8   Vance casts tie-breaking votes to move forward...   \\n25  Alleged burglar arrested in the shooting death...   \\n0   Burger King worker, mom of 3 Nykia Hamilton fo...   \\n\\n                                                 link  \\n24  https://www.wsj.com/economy/trade/forget-taco-...  \\n40  https://www.wsj.com/economy/inflation-hit-2-7-...  \\n12  https://nypost.com/2025/07/15/us-news/beloved-...  \\n2   https://nypost.com/2025/07/16/us-news/polite-b...  \\n1      https://www.bbc.com/news/articles/c2d0yep9d37o  \\n43  https://www.wsj.com/economy/central-banking/di...  \\n15  https://nypost.com/2025/07/15/us-news/nyc-coun...  \\n8   https://nypost.com/2025/07/15/us-news/vance-ca...  \\n25  https://nypost.com/2025/07/15/us-news/alleged-...  \\n0   https://nypost.com/2025/07/16/us-news/burger-k...  \", 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>score</th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>24</th>\\n      <td>34</td>\\n      <td>2025-07-16-00-00-00 +0000</td>\\n      <td>wsj</td>\\n      <td>President Trump wants tariffs, the higher the ...</td>\\n      <td>https://www.wsj.com/economy/trade/forget-taco-...</td>\\n    </tr>\\n    <tr>\\n      <th>40</th>\\n      <td>23</td>\\n      <td>2025-07-15-20-41-00 +0000</td>\\n      <td>wsj</td>\\n      <td>Inflation picked up in June, a potential sign ...</td>\\n      <td>https://www.wsj.com/economy/inflation-hit-2-7-...</td>\\n    </tr>\\n    <tr>\\n      <th>12</th>\\n      <td>23</td>\\n      <td>2025-07-16-02-33-54 +0000</td>\\n      <td>nypost</td>\\n      <td>Beloved teacher dies in 100-foot fall just 2 m...</td>\\n      <td>https://nypost.com/2025/07/15/us-news/beloved-...</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>20</td>\\n      <td>2025-07-16-05-22-29 +0000</td>\\n      <td>nypost</td>\\n      <td>Polite black bear stuns New Hampshire homeowne...</td>\\n      <td>https://nypost.com/2025/07/16/us-news/polite-b...</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>19</td>\\n      <td>2025-07-16-05-37-50 +0000</td>\\n      <td>bbc</td>\\n      <td>Driver held for hit-and-run death of world\\'s \\'...</td>\\n      <td>https://www.bbc.com/news/articles/c2d0yep9d37o</td>\\n    </tr>\\n    <tr>\\n      <th>43</th>\\n      <td>19</td>\\n      <td>2025-07-15-19-03-00 +0000</td>\\n      <td>wsj</td>\\n      <td>JPMorgan CEO Jamie Dimon sounded Wall Street’s...</td>\\n      <td>https://www.wsj.com/economy/central-banking/di...</td>\\n    </tr>\\n    <tr>\\n      <th>15</th>\\n      <td>19</td>\\n      <td>2025-07-16-01-56-15 +0000</td>\\n      <td>nypost</td>\\n      <td>NYC Council rejects land use change for Bally’...</td>\\n      <td>https://nypost.com/2025/07/15/us-news/nyc-coun...</td>\\n    </tr>\\n    <tr>\\n      <th>8</th>\\n      <td>17</td>\\n      <td>2025-07-16-03-49-27 +0000</td>\\n      <td>nypost</td>\\n      <td>Vance casts tie-breaking votes to move forward...</td>\\n      <td>https://nypost.com/2025/07/15/us-news/vance-ca...</td>\\n    </tr>\\n    <tr>\\n      <th>25</th>\\n      <td>16</td>\\n      <td>2025-07-15-23-50-44 +0000</td>\\n      <td>nypost</td>\\n      <td>Alleged burglar arrested in the shooting death...</td>\\n      <td>https://nypost.com/2025/07/15/us-news/alleged-...</td>\\n    </tr>\\n    <tr>\\n      <th>0</th>\\n      <td>14</td>\\n      <td>2025-07-16-05-41-46 +0000</td>\\n      <td>nypost</td>\\n      <td>Burger King worker, mom of 3 Nykia Hamilton fo...</td>\\n      <td>https://nypost.com/2025/07/16/us-news/burger-k...</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 5}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 9\n",
      "[NbConvertApp] Executing cell:\n",
      "import pandas as pd\n",
      "pd.read_csv('top.csv').to_json('top.json', orient='records', indent=2)\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import pandas as pd\\npd.read_csv('top.csv').to_json('top.json', orient='records', indent=2)\\n\", 'execution_count': 6}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 11\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7ff2bf04c050>\n",
      "[NbConvertApp] Writing 23615 bytes to /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Everything up to date.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update outdated notebooks until none remain\n",
    "import json, re, time, subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_dir = Path.cwd()\n",
    "if not (repo_dir / 'analysis').is_dir():\n",
    "    repo_dir = repo_dir.parent\n",
    "analysis_dir = repo_dir / 'analysis'\n",
    "data_dir = repo_dir / 'data'\n",
    "\n",
    "pattern = re.compile(r'[A-Za-z0-9_/.-]*latest\\.(?:csv|json|xml|rss)')\n",
    "\n",
    "def mtime_str(p: Path) -> str:\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(p.stat().st_mtime))\n",
    "\n",
    "def build_dep_map():\n",
    "    ipynb_paths = sorted(analysis_dir.rglob('*.ipynb'))\n",
    "    dep_map = {}\n",
    "    for nb in ipynb_paths:\n",
    "        text = nb.read_text()\n",
    "        matches = sorted(set(pattern.findall(text)))\n",
    "        deps = []\n",
    "        for m in matches:\n",
    "            dep = (nb.parent / m).resolve()\n",
    "            if not dep.exists():\n",
    "                dep = (repo_dir / m.lstrip('./')).resolve()\n",
    "            if dep.exists():\n",
    "                deps.append(dep)\n",
    "        dep_map[nb] = deps\n",
    "    return dep_map\n",
    "\n",
    "def outdated(nb, deps):\n",
    "    nb_mtime = nb.stat().st_mtime\n",
    "    return any(d.stat().st_mtime > nb_mtime for d in deps)\n",
    "\n",
    "\n",
    "def execute(nb: Path):\n",
    "    import shutil\n",
    "    rel = str(nb.relative_to(repo_dir))\n",
    "    if not shutil.which('jupyter'):\n",
    "        print(f'jupyter not available - skipping {rel}')\n",
    "        return\n",
    "    print(f'Running {rel} …')\n",
    "    cmd=[sys.executable,'-m','jupyter','nbconvert','--to','notebook','--inplace','--execute','--ExecutePreprocessor.timeout=600','--debug',str(nb)]\n",
    "    print('  Command:', ' '.join(cmd))\n",
    "    proc=subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    print(proc.stdout)\n",
    "    if proc.returncode==0:\n",
    "        print('  ✓ success')\n",
    "    else:\n",
    "        print(f'  ✗ failed with exit code {proc.returncode}')\n",
    "while True:\n",
    "    dep_map=build_dep_map()\n",
    "    outdated_nbs=[nb for nb,deps in dep_map.items() if deps and outdated(nb,deps)]\n",
    "    report={'outdated_notebooks':[str(nb.relative_to(repo_dir)) for nb in outdated_nbs]}\n",
    "    deps_path = repo_dir / 'dependencies.json'\n",
    "    deps_path.write_text(\n",
    "        json.dumps(report, indent=2) + \"\\n\"\n",
    "    )\n",
    "    if not outdated_nbs:\n",
    "        print('Everything up to date.')\n",
    "        break\n",
    "    for nb in outdated_nbs:\n",
    "        execute(nb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
