{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c898f38d",
   "metadata": {},
   "source": [
    "# Update Script\n",
    "This notebook orchestrates data downloads and analysis refreshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d400efd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T21:51:26.591801Z",
     "iopub.status.busy": "2025-07-16T21:51:26.591610Z",
     "iopub.status.idle": "2025-07-16T21:52:47.310554Z",
     "shell.execute_reply": "2025-07-16T21:52:47.310021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'pandas' not found - installing ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/12.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/16.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m145.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [tzdata]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.3.1 pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'feedparser' not found - installing ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=c7b60f04d5f110224faa16034767842b2abf7169fe6a0a22447f247cf7a89e3a\n",
      "  Stored in directory: /home/runner/.cache/pip/wheels/3d/4d/ef/37cdccc18d6fd7e0dd7817dcdf9146d4d6789c32a227a28134\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [feedparser]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'textblob' not found - installing ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk>=3.9->textblob)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk>=3.9->textblob)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex>=2021.8.3 (from nltk>=3.9->textblob)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk>=3.9->textblob)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/624.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/796.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m142.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tqdm, regex, joblib, click, nltk, textblob\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [joblib]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [joblib]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [textblob]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 textblob-0.19.0 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n",
      "\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching GDPC1 (https://api.stlouisfed.org/fred/series/observations?series_id=GDPC1&file_type=json&observation_end=[date %Y-%m-%d])… ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=GDPC1&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching A939RX0Q048SBEA (https://api.stlouisfed.org/fred/series/observations?series_id=A939RX0Q048SBEA&file_type=json&observation_end=[date %Y-%m-%d])… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=A939RX0Q048SBEA&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching M2REAL (https://api.stlouisfed.org/fred/series/observations?series_id=M2REAL&file_type=json&observation_end=[date %Y-%m-%d])… ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=M2REAL&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching UNRATE (https://api.stlouisfed.org/fred/series/observations?series_id=UNRATE&file_type=json&observation_end=[date %Y-%m-%d])… ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=UNRATE&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching CLVMNACSCAB1GQDE (https://api.stlouisfed.org/fred/series/observations?series_id=CLVMNACSCAB1GQDE&file_type=json&observation_end=[date %Y-%m-%d])… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=CLVMNACSCAB1GQDE&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching GFDEBTN (https://api.stlouisfed.org/fred/series/observations?series_id=GFDEBTN&file_type=json&observation_end=[date %Y-%m-%d])… ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=GFDEBTN&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching GFDEGDQ188S (https://api.stlouisfed.org/fred/series/observations?series_id=GFDEGDQ188S&file_type=json&observation_end=[date %Y-%m-%d])… ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=GFDEGDQ188S&file_type=json&observation_end=2025-07-16\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching TDSP (https://api.stlouisfed.org/fred/series/observations?series_id=TDSP&file_type=json&observation_end=[date %Y-%m-%d])… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=TDSP&file_type=json&observation_end=2025-07-16\n",
      "Fetching news-us-nyt (https://rss.nytimes.com/services/xml/rss/nyt/US.xml)… ✓ success\n",
      "Fetching news-world-nyt (https://rss.nytimes.com/services/xml/rss/nyt/World.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-africa-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Africa.xml)… ✓ success\n",
      "Fetching news-europe-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Europe.xml)… ✓ success\n",
      "Fetching news-asia-nyt (https://rss.nytimes.com/services/xml/rss/nyt/AsiaPacific.xml)… ✓ success\n",
      "Fetching news-americas-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Americas.xml)… no change\n",
      "Fetching news-middle-east-nyt (https://rss.nytimes.com/services/xml/rss/nyt/MiddleEast.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-business-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Business.xml)… ✓ success\n",
      "Fetching news-economy-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Economy.xml)… no change\n",
      "Fetching news-us-politics-nyt (https://rss.nytimes.com/services/xml/rss/nyt/Politics.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-world-wsj (https://feeds.content.dowjones.io/public/rss/RSSWorldNews)… no change\n",
      "Fetching news-us-wsj (https://feeds.content.dowjones.io/public/rss/RSSUSNews)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-business-wsj (https://feeds.content.dowjones.io/public/rss/WSJcomUSBusiness)… ✓ success\n",
      "Fetching news-markets-wsj (https://feeds.content.dowjones.io/public/rss/RSSMarketsMain)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-economy-wsj (https://feeds.content.dowjones.io/public/rss/socialeconomyfeed)… no change\n",
      "Fetching news-us-politics-wsj (https://feeds.content.dowjones.io/public/rss/socialpoliticsfeed)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-politics-wapo (https://www.washingtonpost.com/arcio/rss/category/politics/)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=30)\n",
      "Fetching news-us-wapo (http://feeds.washingtonpost.com/rss/national)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-world-wapo (https://feeds.washingtonpost.com/rss/world)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 502 Server Error: Bad Gateway for url: https://feeds.washingtonpost.com/rss/world\n",
      "Fetching news-business-wapo (http://feeds.washingtonpost.com/rss/business)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 502 Server Error: Bad Gateway for url: https://feeds.washingtonpost.com/rss/business\n",
      "Fetching latimes-business (https://www.latimes.com/business/rss2.0.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching latimes-us (https://www.latimes.com/local/rss2.0.xml)… ✓ success\n",
      "Fetching latimes-us-politics (https://www.latimes.com/politics/rss2.0.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-world-chi-tribune (https://www.chicagotribune.com/arc/outboundfeeds/rss/section/nation-world/&sort=display_date:desc)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 403 Client Error: Forbidden for url: https://www.chicagotribune.com/arc/outboundfeeds/rss/section/nation-world/&sort=display_date:desc\n",
      "Fetching news-business-chi-tribune (https://www.chicagotribune.com/arc/outboundfeeds/rss/section/business/&sort=display_date:desc)… ✗ failed: 403 Client Error: Forbidden for url: https://www.chicagotribune.com/arc/outboundfeeds/rss/section/business/&sort=display_date:desc\n",
      "Fetching news-us-politics-chi-tribune (https://www.chicagotribune.com/arc/outboundfeeds/rss/section/politics/&sort=display_date:desc/)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 403 Client Error: Forbidden for url: https://www.chicagotribune.com/arc/outboundfeeds/rss/section/politics/&sort=display_date:desc/\n",
      "Fetching news-us-business-startribune (https://www.startribune.com/rss?sf=1&s=%2F)… ✓ success\n",
      "Fetching news-us-politics-startribune (https://www.startribune.com/politics/index.rss2)… ✗ failed: 404 Client Error: Not Found for url: https://www.startribune.com/politics/index.rss2\n",
      "Fetching news-us-nypost (https://nypost.com/us-news/feed/)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-world-nypost (https://nypost.com/world-news/feed/)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-us-politics-nypost (https://nypost.com/politics/feed/)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-business-nypost (https://nypost.com/business/feed/)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-world-toi (http://timesofindia.indiatimes.com/rssfeeds/296589292.cms)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-business-toi (http://timesofindia.indiatimes.com/rssfeeds/1898055.cms)… no change\n",
      "Fetching news-us-toi (https://timesofindia.indiatimes.com/rssfeeds_us/72258322.cms)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-middle-east-toi (http://timesofindia.indiatimes.com/rssfeeds/1898272.cms)… no change\n",
      "Fetching news-europe-toi (http://timesofindia.indiatimes.com/rssfeeds/1898274.cms)… no change\n",
      "Fetching news-world-cbc (https://www.cbc.ca/webfeed/rss/rss-world)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-politics-cbc (https://www.cbc.ca/webfeed/rss/rss-politics)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-africa-bbc (http://feeds.bbci.co.uk/news/world/africa/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-asia-bbc (http://feeds.bbci.co.uk/news/world/asia/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-europe-bbc (http://feeds.bbci.co.uk/news/world/europe/rss.xml)… ✓ success\n",
      "Fetching news-latin-america-bbc (http://feeds.bbci.co.uk/news/world/latin_america/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-middle-east-bbc (http://feeds.bbci.co.uk/news/world/middle_east/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-us-bbc (http://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml)… ✓ success\n",
      "Fetching news-world-bbc (https://feeds.bbci.co.uk/news/world/rss.xml)… ✓ success\n",
      "Fetching news-business-bbc (https://feeds.bbci.co.uk/news/business/rss.xml)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-politics-bbc (https://feeds.bbci.co.uk/news/politics/rss.xml)… ✓ success\n",
      "Fetching news-top-dw (https://rss.dw.com/rdf/rss-en-all)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-europe-dw (https://rss.dw.com/rdf/rss-en-eu)… no change\n",
      "Fetching news-world-dw (https://rss.dw.com/rdf/rss-en-world)… no change\n",
      "Fetching news-business-dw (https://rss.dw.com/rdf/rss-en-bus)… "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-asia-dw (https://rss.dw.com/rdf/rss-en-asia)… no change\n",
      "Fetching zip-demo-ca (nan)… ✗ failed: Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n",
      "\n",
      "Updated: news-us-nyt, news-world-nyt, news-africa-nyt, news-europe-nyt, news-asia-nyt, news-middle-east-nyt, news-business-nyt, news-us-politics-nyt, news-us-wsj, news-business-wsj, news-markets-wsj, latimes-business, latimes-us, news-us-business-startribune, news-us-nypost, news-world-nypost, news-us-politics-nypost, news-politics-cbc, news-europe-bbc, news-middle-east-bbc, news-us-bbc, news-world-bbc, news-politics-bbc, news-top-dw\n"
     ]
    }
   ],
   "source": [
    "# ========== Bootstrap: ensure required Python packages are present ==========\n",
    "import importlib, subprocess, sys\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: Optional[str] = None, required: bool = True):\n",
    "    \"\"\"Import a module, installing it if necessary. If installation fails and\n",
    "    the package is required, the exception is raised. Optional packages may\n",
    "    remain unavailable.\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"Package '{pkg_name}' not found - installing ...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install {pkg_name}: {e}\")\n",
    "            if required:\n",
    "                raise\n",
    "    try:\n",
    "        mod = importlib.import_module(import_name or pkg_name)\n",
    "        globals()[import_name or pkg_name] = mod\n",
    "        return mod\n",
    "    except ModuleNotFoundError:\n",
    "        if required:\n",
    "            raise\n",
    "        print(f\"Package '{pkg_name}' is unavailable.\")\n",
    "        globals()[import_name or pkg_name] = None\n",
    "        return None\n",
    "# --- Required third-party libraries ------------------------------------------\n",
    "_ensure(\"pandas\")\n",
    "_ensure(\"requests\")\n",
    "_ensure(\"feedparser\")\n",
    "_ensure(\"textblob\")\n",
    "_ensure(\"jupyter\", required=False)\n",
    "_ensure(\"nbconvert\", required=False)\n",
    "print(\"All dependencies ready.\\n\")\n",
    "\n",
    "# --- Standard imports --------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json, feedparser, textblob\n",
    "import pandas as pd, requests, urllib.parse\n",
    "\n",
    "# --- Helper: replace [date %Y-%m-%d] tokens -----------------------------------\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        fmt = m.group(1).strip()\n",
    "        return dt.date.today().strftime(fmt)\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "# --- Helper: append API key if specified -----------------------------------\n",
    "def add_apikey(url: str, env_var: Optional[str]) -> str:\n",
    "    if env_var and str(env_var).lower() != \"nan\":\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f'{url}{sep}api_key={urllib.parse.quote_plus(key)}'\n",
    "        else:\n",
    "            print(f\"Warning: environment variable '{env_var}' not set.\")\n",
    "    return url\n",
    "\n",
    "# --- Cadence map (word → minimum seconds between fetches) ------------------------\n",
    "CADENCE_SECONDS = {\n",
    "    \"hourly\": 3600,\n",
    "    \"daily\": 86400,\n",
    "    \"weekly\": 604800,\n",
    "    \"monthly\": 2592000,\n",
    "    \"quarterly\": 7776000,\n",
    "}\n",
    "\n",
    "# --- Resolve base directory so notebook works from repo root or data folder ---\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "\n",
    "# --- Load catalog -------------------------------------------------------------\n",
    "catalog_path = BASE_DIR / 'catalog.csv'\n",
    "cat = pd.read_csv(catalog_path)\n",
    "cat['filetype'] = cat['filetype'].astype(str).str.strip().str.lstrip('.')\n",
    "\n",
    "now = dt.datetime.now()\n",
    "today = now.date()\n",
    "updated_rows = []                # remember which rows we refresh\n",
    "\n",
    "for idx, row in cat.iterrows():\n",
    "    folder = BASE_DIR / str(row['category']) / str(row['source']) / str(row['folder'])\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    cadence = str(row['cadence']).lower().strip()\n",
    "    filetype = str(row['filetype']).strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    latest_fp = folder / f'latest.{output_ext}'\n",
    "    url = str(row.get('url', '')).strip()\n",
    "    if not url or url.lower() in ('n/a', 'na', 'none'):\n",
    "        print(f\"Skipping {row['folder']} (static)\")\n",
    "        continue\n",
    "    dated_fp = folder / f\"{now:%Y-%m-%d-%H}.{output_ext}\" if cadence == \"hourly\" else folder / f\"{today:%Y-%m-%d}.{output_ext}\"\n",
    "    if dated_fp.exists():\n",
    "        if (not latest_fp.exists()) or latest_fp.read_bytes() != dated_fp.read_bytes():\n",
    "            shutil.copyfile(dated_fp, latest_fp)\n",
    "        cat.at[idx, 'last_fetched'] = now.isoformat(timespec='minutes')\n",
    "        continue\n",
    "    last_fetched = (\n",
    "        pd.to_datetime(row[\"last_fetched\"])\n",
    "        if pd.notna(row[\"last_fetched\"]) else None\n",
    "    )\n",
    "\n",
    "    # ---- Determine if an update is due --------------------------------------\n",
    "    cadence = str(row[\"cadence\"]).lower().strip()\n",
    "    min_age = CADENCE_SECONDS.get(cadence, 30*86400)        # default 30 days\n",
    "    needs_update = (\n",
    "        (not latest_fp.exists()) or\n",
    "        (not last_fetched) or\n",
    "        (now - last_fetched).total_seconds() >= min_age\n",
    "    )\n",
    "\n",
    "    #if not needs_update:\n",
    "        #print(f\"Skipping {row['folder']} - up to date\")\n",
    "        #continue\n",
    "\n",
    "    # ---- Build the request URL ---------------------------------------------\n",
    "    url = substitute_date_tokens(str(row[\"url\"]))\n",
    "    url = add_apikey(url, str(row.get('api_key') or '').strip() or None)\n",
    "\n",
    "    print(f\"Fetching {row['folder']} ({row['url']})…\", end=\" \")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        r.raise_for_status()\n",
    "        if filetype.lower() in ('rss', 'xml'):\n",
    "            feed = feedparser.parse(r.content)\n",
    "            entries = []\n",
    "            for e in feed.entries:\n",
    "                text = ' '.join(filter(None, [e.get('title'), e.get('summary')]))\n",
    "                polarity = textblob.TextBlob(text).sentiment.polarity\n",
    "                entries.append({'title': e.get('title'), 'link': e.get('link'),\n",
    "                               'published': e.get('published'),\n",
    "                               'sentiment': polarity})\n",
    "            content_bytes = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "        else:\n",
    "            content_bytes = r.content\n",
    "        if filetype.lower() == 'json':\n",
    "            try:\n",
    "                data_json = r.json()\n",
    "            except Exception:\n",
    "                data_json = None\n",
    "            if isinstance(data_json, dict) and data_json.get('error_message'):\n",
    "                raise ValueError(data_json['error_message'])\n",
    "        # ---- Save snapshot and latest --------------------------------------\n",
    "        if latest_fp.exists() and latest_fp.read_bytes() == content_bytes:\n",
    "            cat.at[idx, 'last_fetched'] = now.isoformat(timespec='minutes')\n",
    "            print('no change')\n",
    "            continue\n",
    "        dated_fp.write_bytes(content_bytes)\n",
    "        shutil.copyfile(dated_fp, latest_fp)\n",
    "\n",
    "        # ---- Mark success in catalog ---------------------------------------\n",
    "        cat.at[idx, \"last_fetched\"] = now.isoformat(timespec='minutes')\n",
    "        updated_rows.append(row[\"folder\"])\n",
    "        print(\"✓ success\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ failed: {e}\")\n",
    "\n",
    "# --- Persist catalog if anything changed -------------------------------------\n",
    "if updated_rows:\n",
    "    cat.to_csv(catalog_path, index=False)\n",
    "    print(\"\\nUpdated:\", \", \".join(updated_rows))\n",
    "else:\n",
    "    print(\"Everything up to date.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c044a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T21:52:47.312553Z",
     "iopub.status.busy": "2025-07-16T21:52:47.312207Z",
     "iopub.status.idle": "2025-07-16T21:52:47.333157Z",
     "shell.execute_reply": "2025-07-16T21:52:47.332608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index files generated for GDPC1, A939RX0Q048SBEA, M2REAL, UNRATE, CLVMNACSCAB1GQDE, GFDEBTN, GFDEGDQ188S, TDSP, news-us-nyt, news-world-nyt, news-africa-nyt, news-europe-nyt, news-asia-nyt, news-americas-nyt, news-middle-east-nyt, news-business-nyt, news-economy-nyt, news-us-politics-nyt, news-world-wsj, news-us-wsj, news-business-wsj, news-markets-wsj, news-economy-wsj, news-us-politics-wsj, news-us-politics-wapo, news-us-wapo, news-world-wapo, news-business-wapo, latimes-business, latimes-us, latimes-us-politics, news-world-chi-tribune, news-business-chi-tribune, news-us-politics-chi-tribune, news-us-business-startribune, news-us-politics-startribune, news-us-nypost, news-world-nypost, news-us-politics-nypost, news-business-nypost, news-world-toi, news-business-toi, news-us-toi, news-middle-east-toi, news-europe-toi, news-world-cbc, news-politics-cbc, news-africa-bbc, news-asia-bbc, news-europe-bbc, news-latin-america-bbc, news-middle-east-bbc, news-us-bbc, news-world-bbc, news-business-bbc, news-politics-bbc, news-top-dw, news-europe-dw, news-world-dw, news-business-dw, news-asia-dw, zip-demo-ca\n"
     ]
    }
   ],
   "source": [
    "# This cell updates the markdown index files for all the data sources\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "with open(BASE_DIR / 'catalog.csv', newline='') as f:\n",
    "    cat = list(csv.DictReader(f))\n",
    "\n",
    "for row in cat:\n",
    "    folder = BASE_DIR / row['category'] / row['source'] / row['folder']\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = row['filetype'].strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    desc = row['description'].strip()\n",
    "    source = row['source'].strip()\n",
    "    date = row.get('last_fetched', '').strip()\n",
    "\n",
    "    pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}(?:-\\d{2})?\\.\" + re.escape(output_ext) + r\"$\")\n",
    "    dated_files = sorted(p.name for p in folder.iterdir() if pattern.match(p.name))\n",
    "\n",
    "    lines = [\n",
    "        '---',\n",
    "        'layout: default',\n",
    "        f'title: {source} - {desc}',\n",
    "        f'date: {date}',\n",
    "        '---',\n",
    "        '',\n",
    "        f'## {source} - {desc}',\n",
    "        '',\n",
    "        '<div id=\"data-chart\"></div>',\n",
    "        '<div id=\"data-table\"></div>',\n",
    "    ]\n",
    "\n",
    "    if row['source'] == 'fred' and filetype == 'json':\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  ShowChart($('#data-chart'));\",\n",
    "            \"  SourceTabler($('#data-table'));\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "    else:\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  document.getElementById('data-table').textContent = 'This source isn\\'t supported for tables yet.';\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "\n",
    "    lines += [\n",
    "        '',\n",
    "        '## File Versions:',\n",
    "    ]\n",
    "    links = [f'[Latest version](./latest.{output_ext})'] + [f'[{fname}](./{fname})' for fname in dated_files]\n",
    "    for i, link in enumerate(links, 1):\n",
    "        lines.append(f'{i}. {link}')\n",
    "    (folder / 'index.md').write_text(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print('Index files generated for', ', '.join(r['folder'] for r in cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25e4e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T21:52:47.335039Z",
     "iopub.status.busy": "2025-07-16T21:52:47.334707Z",
     "iopub.status.idle": "2025-07-16T21:52:54.441620Z",
     "shell.execute_reply": "2025-07-16T21:52:54.441021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb …\n",
      "  Command: /opt/hostedtoolcache/Python/3.13.5/x64/bin/python -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug /home/runner/work/Analysis/Analysis/analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Searching ['/home/runner/.jupyter', '/home/runner/.local/etc/jupyter', '/opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'update_headlines-checkpoint'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['/opt/hostedtoolcache/Python/3.13.5/x64/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmpyjxp37rc.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:41169\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:53773\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:53773\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:42367\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:42367\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:33295\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:33295\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:37025\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:41169\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:41169\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "print('No external dependencies required.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\nprint('No external dependencies required.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'No external dependencies required.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "# Some of the dependencies, to trigger the dependency checker:\n",
      "# data/news-us/nyt/news-us-nyt/latest.json\n",
      "# data/news-us/wsj/news-us-wsj/latest.json\n",
      "\n",
      "\n",
      "from pathlib import Path\n",
      "import csv\n",
      "import json\n",
      "import xml.etree.ElementTree as ET\n",
      "from datetime import datetime, timezone, timedelta\n",
      "from email.utils import parsedate_to_datetime\n",
      "import shutil\n",
      "\n",
      "BASE_DIR = Path.cwd()\n",
      "REPO_DIR = BASE_DIR\n",
      "while not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):\n",
      "    if REPO_DIR.parent == REPO_DIR:\n",
      "        raise FileNotFoundError('Repository root not found')\n",
      "    REPO_DIR = REPO_DIR.parent\n",
      "DATA_DIR = REPO_DIR / 'data'\n",
      "HEADLINES_DIR = REPO_DIR / 'analysis/headlines'\n",
      "HEADLINES_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "def parse_pubdate(date_str):\n",
      "    try:\n",
      "        dt = parsedate_to_datetime(date_str) if date_str else None\n",
      "        if dt is None:\n",
      "            return None\n",
      "        if dt.tzinfo is None:\n",
      "            dt = dt.replace(tzinfo=timezone.utc)\n",
      "        return dt.astimezone(timezone.utc)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def format_pubdate(dt):\n",
      "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''\n",
      "\n",
      "def parse_feed(path: Path):\n",
      "    entries = []\n",
      "    if path.suffix == '.json':\n",
      "        with open(path, 'r', encoding='utf-8') as f:\n",
      "            data = json.load(f)\n",
      "        for item in data.get('entries', []):\n",
      "            title = item.get('title')\n",
      "            link = item.get('link')\n",
      "            pub = parse_pubdate(item.get('published'))\n",
      "            if title and link:\n",
      "                entries.append((pub, title.strip(), link.strip()))\n",
      "    else:\n",
      "        try:\n",
      "            tree = ET.parse(path)\n",
      "            root = tree.getroot()\n",
      "        except ET.ParseError:\n",
      "            return entries\n",
      "        for item in root.iter():\n",
      "            if item.tag.lower().endswith(('item', 'entry')):\n",
      "                title = None\n",
      "                link = None\n",
      "                pub = None\n",
      "                for child in item:\n",
      "                    tag = child.tag.lower()\n",
      "                    if tag.endswith('title'):\n",
      "                        title = (child.text or '').strip()\n",
      "                    if tag.endswith('link'):\n",
      "                        link = (child.text or '').strip() or child.attrib.get('href')\n",
      "                    if tag.endswith(('pubdate', 'published', 'updated')):\n",
      "                        pub = parse_pubdate((child.text or '').strip())\n",
      "                if title and link:\n",
      "                    entries.append((pub, title, link))\n",
      "    return entries\n",
      "\n",
      "def collect_headlines():\n",
      "    all_entries = []\n",
      "    for source in DATA_DIR.iterdir():\n",
      "        if source.is_dir() and source.name.startswith('news'):\n",
      "            candidates = [p for p in source.rglob('latest.*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                candidates = [p for p in source.rglob('*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                continue\n",
      "            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
      "            source_name = latest_file.relative_to(DATA_DIR).parts[1]\n",
      "            for pub, title, link in parse_feed(latest_file):\n",
      "                all_entries.append((pub, title, link, source_name))\n",
      "    return all_entries\n",
      "\n",
      "def _date_key(date_str):\n",
      "    try:\n",
      "        return parsedate_to_datetime(date_str) if date_str else datetime.min\n",
      "    except Exception:\n",
      "        return datetime.min\n",
      "\n",
      "def update_headlines():\n",
      "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\n",
      "    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\n",
      "    if hourly_file.exists():\n",
      "        print(f\"{hourly_file.name} already exists. Skipping update.\")\n",
      "        return\n",
      "    entries = collect_headlines()\n",
      "    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
      "    deduped = []\n",
      "    seen_titles = set()\n",
      "    seen_links = set()\n",
      "    for pub, title, link, src in entries:\n",
      "        t_key = title.lower()\n",
      "        l_key = link.lower()\n",
      "        if t_key in seen_titles or l_key in seen_links:\n",
      "            continue\n",
      "        deduped.append((pub, src, title, link))\n",
      "        seen_titles.add(t_key)\n",
      "        seen_links.add(l_key)\n",
      "    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\n",
      "    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\n",
      "    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        writer.writerow(['pubdate', 'source', 'title', 'link'])\n",
      "        for pub, src, title, link in deduped:\n",
      "            writer.writerow([format_pubdate(pub), src, title, link])\n",
      "    latest_file = HEADLINES_DIR / \"latest.csv\"\n",
      "    shutil.copy(hourly_file, latest_file)\n",
      "    print(f\"Wrote {hourly_file} and updated latest.csv\")\n",
      "    # Get the most recent headline\n",
      "\n",
      "\n",
      "update_headlines()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': '# Some of the dependencies, to trigger the dependency checker:\\n# data/news-us/nyt/news-us-nyt/latest.json\\n# data/news-us/wsj/news-us-wsj/latest.json\\n\\n\\nfrom pathlib import Path\\nimport csv\\nimport json\\nimport xml.etree.ElementTree as ET\\nfrom datetime import datetime, timezone, timedelta\\nfrom email.utils import parsedate_to_datetime\\nimport shutil\\n\\nBASE_DIR = Path.cwd()\\nREPO_DIR = BASE_DIR\\nwhile not ((REPO_DIR / \\'data\\').exists() and (REPO_DIR / \\'analysis\\').exists()):\\n    if REPO_DIR.parent == REPO_DIR:\\n        raise FileNotFoundError(\\'Repository root not found\\')\\n    REPO_DIR = REPO_DIR.parent\\nDATA_DIR = REPO_DIR / \\'data\\'\\nHEADLINES_DIR = REPO_DIR / \\'analysis/headlines\\'\\nHEADLINES_DIR.mkdir(parents=True, exist_ok=True)\\n\\ndef parse_pubdate(date_str):\\n    try:\\n        dt = parsedate_to_datetime(date_str) if date_str else None\\n        if dt is None:\\n            return None\\n        if dt.tzinfo is None:\\n            dt = dt.replace(tzinfo=timezone.utc)\\n        return dt.astimezone(timezone.utc)\\n    except Exception:\\n        return None\\n\\ndef format_pubdate(dt):\\n    return dt.strftime(\\'%Y-%m-%d-%H-%M-%S +0000\\') if dt else \\'\\'\\n\\ndef parse_feed(path: Path):\\n    entries = []\\n    if path.suffix == \\'.json\\':\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            data = json.load(f)\\n        for item in data.get(\\'entries\\', []):\\n            title = item.get(\\'title\\')\\n            link = item.get(\\'link\\')\\n            pub = parse_pubdate(item.get(\\'published\\'))\\n            if title and link:\\n                entries.append((pub, title.strip(), link.strip()))\\n    else:\\n        try:\\n            tree = ET.parse(path)\\n            root = tree.getroot()\\n        except ET.ParseError:\\n            return entries\\n        for item in root.iter():\\n            if item.tag.lower().endswith((\\'item\\', \\'entry\\')):\\n                title = None\\n                link = None\\n                pub = None\\n                for child in item:\\n                    tag = child.tag.lower()\\n                    if tag.endswith(\\'title\\'):\\n                        title = (child.text or \\'\\').strip()\\n                    if tag.endswith(\\'link\\'):\\n                        link = (child.text or \\'\\').strip() or child.attrib.get(\\'href\\')\\n                    if tag.endswith((\\'pubdate\\', \\'published\\', \\'updated\\')):\\n                        pub = parse_pubdate((child.text or \\'\\').strip())\\n                if title and link:\\n                    entries.append((pub, title, link))\\n    return entries\\n\\ndef collect_headlines():\\n    all_entries = []\\n    for source in DATA_DIR.iterdir():\\n        if source.is_dir() and source.name.startswith(\\'news\\'):\\n            candidates = [p for p in source.rglob(\\'latest.*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                candidates = [p for p in source.rglob(\\'*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                continue\\n            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\\n            source_name = latest_file.relative_to(DATA_DIR).parts[1]\\n            for pub, title, link in parse_feed(latest_file):\\n                all_entries.append((pub, title, link, source_name))\\n    return all_entries\\n\\ndef _date_key(date_str):\\n    try:\\n        return parsedate_to_datetime(date_str) if date_str else datetime.min\\n    except Exception:\\n        return datetime.min\\n\\ndef update_headlines():\\n    timestamp = datetime.utcnow().strftime(\\'%Y-%m-%d-%H-00\\')\\n    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\\n    if hourly_file.exists():\\n        print(f\"{hourly_file.name} already exists. Skipping update.\")\\n        return\\n    entries = collect_headlines()\\n    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\\n    deduped = []\\n    seen_titles = set()\\n    seen_links = set()\\n    for pub, title, link, src in entries:\\n        t_key = title.lower()\\n        l_key = link.lower()\\n        if t_key in seen_titles or l_key in seen_links:\\n            continue\\n        deduped.append((pub, src, title, link))\\n        seen_titles.add(t_key)\\n        seen_links.add(l_key)\\n    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\\n    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\\n    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n        writer = csv.writer(f)\\n        writer.writerow([\\'pubdate\\', \\'source\\', \\'title\\', \\'link\\'])\\n        for pub, src, title, link in deduped:\\n            writer.writerow([format_pubdate(pub), src, title, link])\\n    latest_file = HEADLINES_DIR / \"latest.csv\"\\n    shutil.copy(hourly_file, latest_file)\\n    print(f\"Wrote {hourly_file} and updated latest.csv\")\\n    # Get the most recent headline\\n\\n\\nupdate_headlines()\\n\\n\\n\\n\\n\\n', 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'Wrote /home/runner/work/Analysis/Analysis/analysis/headlines/2025-07-16-21-00.csv and updated latest.csv\\n'}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"/tmp/ipykernel_2268/2800259878.py:94: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\\n\"}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 3\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7fb30e8901a0>\n",
      "[NbConvertApp] Writing 8844 bytes to /home/runner/work/Analysis/Analysis/analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Running analysis/headlines/update_headlines.ipynb …\n",
      "  Command: /opt/hostedtoolcache/Python/3.13.5/x64/bin/python -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Searching ['/home/runner/.jupyter', '/home/runner/.local/etc/jupyter', '/opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'update_headlines'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['/opt/hostedtoolcache/Python/3.13.5/x64/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmpw6tsb6eo.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:43543\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:36141\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:36141\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:47433\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:47433\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:50169\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:50169\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:47275\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:43543\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:43543\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "print('No external dependencies required.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\nprint('No external dependencies required.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'No external dependencies required.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "# Some of the dependencies, to trigger the dependency checker:\n",
      "# data/news-us/nyt/news-us-nyt/latest.json\n",
      "# data/news-us/wsj/news-us-wsj/latest.json\n",
      "\n",
      "\n",
      "from pathlib import Path\n",
      "import csv\n",
      "import json\n",
      "import xml.etree.ElementTree as ET\n",
      "from datetime import datetime, timezone, timedelta\n",
      "from email.utils import parsedate_to_datetime\n",
      "import shutil\n",
      "\n",
      "BASE_DIR = Path.cwd()\n",
      "REPO_DIR = BASE_DIR\n",
      "while not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):\n",
      "    if REPO_DIR.parent == REPO_DIR:\n",
      "        raise FileNotFoundError('Repository root not found')\n",
      "    REPO_DIR = REPO_DIR.parent\n",
      "DATA_DIR = REPO_DIR / 'data'\n",
      "HEADLINES_DIR = REPO_DIR / 'analysis/headlines'\n",
      "HEADLINES_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "def parse_pubdate(date_str):\n",
      "    try:\n",
      "        dt = parsedate_to_datetime(date_str) if date_str else None\n",
      "        if dt is None:\n",
      "            return None\n",
      "        if dt.tzinfo is None:\n",
      "            dt = dt.replace(tzinfo=timezone.utc)\n",
      "        return dt.astimezone(timezone.utc)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def format_pubdate(dt):\n",
      "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''\n",
      "\n",
      "def parse_feed(path: Path):\n",
      "    entries = []\n",
      "    if path.suffix == '.json':\n",
      "        with open(path, 'r', encoding='utf-8') as f:\n",
      "            data = json.load(f)\n",
      "        for item in data.get('entries', []):\n",
      "            title = item.get('title')\n",
      "            link = item.get('link')\n",
      "            pub = parse_pubdate(item.get('published'))\n",
      "            if title and link:\n",
      "                entries.append((pub, title.strip(), link.strip()))\n",
      "    else:\n",
      "        try:\n",
      "            tree = ET.parse(path)\n",
      "            root = tree.getroot()\n",
      "        except ET.ParseError:\n",
      "            return entries\n",
      "        for item in root.iter():\n",
      "            if item.tag.lower().endswith(('item', 'entry')):\n",
      "                title = None\n",
      "                link = None\n",
      "                pub = None\n",
      "                for child in item:\n",
      "                    tag = child.tag.lower()\n",
      "                    if tag.endswith('title'):\n",
      "                        title = (child.text or '').strip()\n",
      "                    if tag.endswith('link'):\n",
      "                        link = (child.text or '').strip() or child.attrib.get('href')\n",
      "                    if tag.endswith(('pubdate', 'published', 'updated')):\n",
      "                        pub = parse_pubdate((child.text or '').strip())\n",
      "                if title and link:\n",
      "                    entries.append((pub, title, link))\n",
      "    return entries\n",
      "\n",
      "def collect_headlines():\n",
      "    all_entries = []\n",
      "    feed_info = {}\n",
      "    for source in DATA_DIR.iterdir():\n",
      "        if source.is_dir() and source.name.startswith('news'):\n",
      "            candidates = [p for p in source.rglob('latest.*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                candidates = [p for p in source.rglob('*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                continue\n",
      "            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
      "            source_name = latest_file.relative_to(DATA_DIR).parts[1]\n",
      "            feed_entries = parse_feed(latest_file)\n",
      "            if feed_entries:\n",
      "                recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)\n",
      "                feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}\n",
      "            for pub, title, link in feed_entries:\n",
      "                all_entries.append((pub, title, link, source_name))\n",
      "    return all_entries, feed_info\n",
      "\n",
      "def _date_key(date_str):\n",
      "    try:\n",
      "        return parsedate_to_datetime(date_str) if date_str else datetime.min\n",
      "    except Exception:\n",
      "        return datetime.min\n",
      "\n",
      "def update_headlines():\n",
      "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\n",
      "    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\n",
      "    if hourly_file.exists():\n",
      "        print(f\"{hourly_file.name} already exists. Skipping update.\")\n",
      "        return\n",
      "    entries, feed_info = collect_headlines()\n",
      "    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
      "    deduped = []\n",
      "    seen_titles = set()\n",
      "    seen_links = set()\n",
      "    for pub, title, link, src in entries:\n",
      "        t_key = title.lower()\n",
      "        l_key = link.lower()\n",
      "        if t_key in seen_titles or l_key in seen_links:\n",
      "            continue\n",
      "        deduped.append((pub, src, title, link))\n",
      "        seen_titles.add(t_key)\n",
      "        seen_links.add(l_key)\n",
      "    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\n",
      "    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\n",
      "    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        writer.writerow(['pubdate', 'source', 'title', 'link'])\n",
      "        for pub, src, title, link in deduped:\n",
      "            writer.writerow([format_pubdate(pub), src, title, link])\n",
      "    latest_file = HEADLINES_DIR / \"latest.csv\"\n",
      "    shutil.copy(hourly_file, latest_file)\n",
      "    print(f\"Wrote {hourly_file} and updated latest.csv\")\n",
      "    print()\n",
      "    print('Feed summary:')\n",
      "    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")\n",
      "    for src, info in sorted(feed_info.items()):\n",
      "        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")\n",
      "    # Get the most recent headline\n",
      "\n",
      "\n",
      "update_headlines()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': '# Some of the dependencies, to trigger the dependency checker:\\n# data/news-us/nyt/news-us-nyt/latest.json\\n# data/news-us/wsj/news-us-wsj/latest.json\\n\\n\\nfrom pathlib import Path\\nimport csv\\nimport json\\nimport xml.etree.ElementTree as ET\\nfrom datetime import datetime, timezone, timedelta\\nfrom email.utils import parsedate_to_datetime\\nimport shutil\\n\\nBASE_DIR = Path.cwd()\\nREPO_DIR = BASE_DIR\\nwhile not ((REPO_DIR / \\'data\\').exists() and (REPO_DIR / \\'analysis\\').exists()):\\n    if REPO_DIR.parent == REPO_DIR:\\n        raise FileNotFoundError(\\'Repository root not found\\')\\n    REPO_DIR = REPO_DIR.parent\\nDATA_DIR = REPO_DIR / \\'data\\'\\nHEADLINES_DIR = REPO_DIR / \\'analysis/headlines\\'\\nHEADLINES_DIR.mkdir(parents=True, exist_ok=True)\\n\\ndef parse_pubdate(date_str):\\n    try:\\n        dt = parsedate_to_datetime(date_str) if date_str else None\\n        if dt is None:\\n            return None\\n        if dt.tzinfo is None:\\n            dt = dt.replace(tzinfo=timezone.utc)\\n        return dt.astimezone(timezone.utc)\\n    except Exception:\\n        return None\\n\\ndef format_pubdate(dt):\\n    return dt.strftime(\\'%Y-%m-%d-%H-%M-%S +0000\\') if dt else \\'\\'\\n\\ndef parse_feed(path: Path):\\n    entries = []\\n    if path.suffix == \\'.json\\':\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            data = json.load(f)\\n        for item in data.get(\\'entries\\', []):\\n            title = item.get(\\'title\\')\\n            link = item.get(\\'link\\')\\n            pub = parse_pubdate(item.get(\\'published\\'))\\n            if title and link:\\n                entries.append((pub, title.strip(), link.strip()))\\n    else:\\n        try:\\n            tree = ET.parse(path)\\n            root = tree.getroot()\\n        except ET.ParseError:\\n            return entries\\n        for item in root.iter():\\n            if item.tag.lower().endswith((\\'item\\', \\'entry\\')):\\n                title = None\\n                link = None\\n                pub = None\\n                for child in item:\\n                    tag = child.tag.lower()\\n                    if tag.endswith(\\'title\\'):\\n                        title = (child.text or \\'\\').strip()\\n                    if tag.endswith(\\'link\\'):\\n                        link = (child.text or \\'\\').strip() or child.attrib.get(\\'href\\')\\n                    if tag.endswith((\\'pubdate\\', \\'published\\', \\'updated\\')):\\n                        pub = parse_pubdate((child.text or \\'\\').strip())\\n                if title and link:\\n                    entries.append((pub, title, link))\\n    return entries\\n\\ndef collect_headlines():\\n    all_entries = []\\n    feed_info = {}\\n    for source in DATA_DIR.iterdir():\\n        if source.is_dir() and source.name.startswith(\\'news\\'):\\n            candidates = [p for p in source.rglob(\\'latest.*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                candidates = [p for p in source.rglob(\\'*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                continue\\n            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\\n            source_name = latest_file.relative_to(DATA_DIR).parts[1]\\n            feed_entries = parse_feed(latest_file)\\n            if feed_entries:\\n                recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)\\n                feed_info[source_name] = {\\'count\\': len(feed_entries), \\'recent\\': recent}\\n            for pub, title, link in feed_entries:\\n                all_entries.append((pub, title, link, source_name))\\n    return all_entries, feed_info\\n\\ndef _date_key(date_str):\\n    try:\\n        return parsedate_to_datetime(date_str) if date_str else datetime.min\\n    except Exception:\\n        return datetime.min\\n\\ndef update_headlines():\\n    timestamp = datetime.utcnow().strftime(\\'%Y-%m-%d-%H-00\\')\\n    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\\n    if hourly_file.exists():\\n        print(f\"{hourly_file.name} already exists. Skipping update.\")\\n        return\\n    entries, feed_info = collect_headlines()\\n    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\\n    deduped = []\\n    seen_titles = set()\\n    seen_links = set()\\n    for pub, title, link, src in entries:\\n        t_key = title.lower()\\n        l_key = link.lower()\\n        if t_key in seen_titles or l_key in seen_links:\\n            continue\\n        deduped.append((pub, src, title, link))\\n        seen_titles.add(t_key)\\n        seen_links.add(l_key)\\n    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\\n    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\\n    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n        writer = csv.writer(f)\\n        writer.writerow([\\'pubdate\\', \\'source\\', \\'title\\', \\'link\\'])\\n        for pub, src, title, link in deduped:\\n            writer.writerow([format_pubdate(pub), src, title, link])\\n    latest_file = HEADLINES_DIR / \"latest.csv\"\\n    shutil.copy(hourly_file, latest_file)\\n    print(f\"Wrote {hourly_file} and updated latest.csv\")\\n    print()\\n    print(\\'Feed summary:\\')\\n    print(f\"{\\'source\\':<20} {\\'count\\':>5}  {\\'most recent\\'}\")\\n    for src, info in sorted(feed_info.items()):\\n        print(f\"{src:<20} {info[\\'count\\']:5}  {format_pubdate(info[\\'recent\\'])}\")\\n    # Get the most recent headline\\n\\n\\nupdate_headlines()\\n\\n\\n\\n\\n\\n', 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': '2025-07-16-21-00.csv already exists. Skipping update.\\n'}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"/tmp/ipykernel_2286/1894882967.py:99: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\\n\"}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7f60d6134050>\n",
      "[NbConvertApp] Writing 9209 bytes to /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Running analysis/news-topics/analyze_headlines.ipynb …\n",
      "  Command: /opt/hostedtoolcache/Python/3.13.5/x64/bin/python -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Searching ['/home/runner/.jupyter', '/home/runner/.local/etc/jupyter', '/opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'analyze_headlines'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['/opt/hostedtoolcache/Python/3.13.5/x64/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmp67j9t9me.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:34545\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:48221\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:48221\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:35069\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:35069\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:42805\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:42805\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:48427\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:34545\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:34545\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "_ensure('pandas')\n",
      "print('All dependencies ready.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\n_ensure('pandas')\\nprint('All dependencies ready.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'All dependencies ready.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "import pandas as pd\n",
      "latest = pd.read_csv('../headlines/latest.csv')\n",
      "latest.head()\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import pandas as pd\\nlatest = pd.read_csv('../headlines/latest.csv')\\nlatest.head()\", 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': \"                     pubdate       source  \\\\\\n0  2025-07-16-21-50-04 +0000  startribune   \\n1  2025-07-16-21-48-16 +0000          bbc   \\n2  2025-07-16-21-40-27 +0000          bbc   \\n3  2025-07-16-21-38-43 +0000          nyt   \\n4  2025-07-16-21-38-07 +0000          bbc   \\n\\n                                               title  \\\\\\n0  Trump slams his own supporters as 'weaklings' ...   \\n1  US says 'specific steps' agreed to end Syria v...   \\n2  Two dead and many injured in Russian strike on...   \\n3  Eswatini Says It Will Repatriate Migrants Depo...   \\n4       What we know so far about Afghan data breach   \\n\\n                                                link  \\n0  https://www.startribune.com/trump-slams-his-ow...  \\n1     https://www.bbc.com/news/articles/cp90l77187zo  \\n2     https://www.bbc.com/news/articles/c62d17ld032o  \\n3  https://www.nytimes.com/2025/07/16/world/afric...  \\n4     https://www.bbc.com/news/articles/c79qyl907lxo  \", 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>2025-07-16-21-50-04 +0000</td>\\n      <td>startribune</td>\\n      <td>Trump slams his own supporters as \\'weaklings\\' ...</td>\\n      <td>https://www.startribune.com/trump-slams-his-ow...</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2025-07-16-21-48-16 +0000</td>\\n      <td>bbc</td>\\n      <td>US says \\'specific steps\\' agreed to end Syria v...</td>\\n      <td>https://www.bbc.com/news/articles/cp90l77187zo</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2025-07-16-21-40-27 +0000</td>\\n      <td>bbc</td>\\n      <td>Two dead and many injured in Russian strike on...</td>\\n      <td>https://www.bbc.com/news/articles/c62d17ld032o</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2025-07-16-21-38-43 +0000</td>\\n      <td>nyt</td>\\n      <td>Eswatini Says It Will Repatriate Migrants Depo...</td>\\n      <td>https://www.nytimes.com/2025/07/16/world/afric...</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>2025-07-16-21-38-07 +0000</td>\\n      <td>bbc</td>\\n      <td>What we know so far about Afghan data breach</td>\\n      <td>https://www.bbc.com/news/articles/c79qyl907lxo</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 3\n",
      "[NbConvertApp] Executing cell:\n",
      "import re\n",
      "from collections import Counter\n",
      "from datetime import datetime\n",
      "\n",
      "with open('exclude.txt') as f:\n",
      "    stop_words = set(w.strip() for w in f if w.strip())\n",
      "words = re.findall(r'[A-Za-z]+', ' '.join(latest['title']).lower())\n",
      "filtered = [w for w in words if w not in stop_words and len(w) > 1]\n",
      "counts = Counter(filtered)\n",
      "score_df = (\n",
      "    pd.DataFrame(counts.items(), columns=['word','score'])\n",
      "    .sort_values('score', ascending=False)\n",
      ")\n",
      "score_df[['score','word']].to_csv('scores.csv', index=False)\n",
      "timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\n",
      "score_df[['score','word']].to_csv(f'scores-{timestamp}.csv', index=False)\n",
      "score_df.head()\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import re\\nfrom collections import Counter\\nfrom datetime import datetime\\n\\nwith open('exclude.txt') as f:\\n    stop_words = set(w.strip() for w in f if w.strip())\\nwords = re.findall(r'[A-Za-z]+', ' '.join(latest['title']).lower())\\nfiltered = [w for w in words if w not in stop_words and len(w) > 1]\\ncounts = Counter(filtered)\\nscore_df = (\\n    pd.DataFrame(counts.items(), columns=['word','score'])\\n    .sort_values('score', ascending=False)\\n)\\nscore_df[['score','word']].to_csv('scores.csv', index=False)\\ntimestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\\nscore_df[['score','word']].to_csv(f'scores-{timestamp}.csv', index=False)\\nscore_df.head()\\n\", 'execution_count': 3}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"/tmp/ipykernel_2305/2741963311.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\\n\"}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '        word  score\\n0      trump     12\\n206       pm      4\\n17   strikes      4\\n29      will      4\\n55    israel      4', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>word</th>\\n      <th>score</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>trump</td>\\n      <td>12</td>\\n    </tr>\\n    <tr>\\n      <th>206</th>\\n      <td>pm</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>17</th>\\n      <td>strikes</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>29</th>\\n      <td>will</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>55</th>\\n      <td>israel</td>\\n      <td>4</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 3}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 5\n",
      "[NbConvertApp] Executing cell:\n",
      "word_scores = dict(score_df[['word','score']].values)\n",
      "latest['score'] = latest['title'].apply(\n",
      "    lambda t: sum(\n",
      "        word_scores.get(w.lower(), 0)\n",
      "        for w in re.findall(r'[A-Za-z]+', t)\n",
      "        if len(w) > 1\n",
      "    )\n",
      ")\n",
      "ranked = latest.sort_values('score', ascending=False)\n",
      "ranked[['score','pubdate','source','title','link']].to_csv('rank.csv', index=False)\n",
      "ranked[['score','pubdate','source','title','link']].to_csv(f'rank-{timestamp}.csv', index=False)\n",
      "ranked.head()\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"word_scores = dict(score_df[['word','score']].values)\\nlatest['score'] = latest['title'].apply(\\n    lambda t: sum(\\n        word_scores.get(w.lower(), 0)\\n        for w in re.findall(r'[A-Za-z]+', t)\\n        if len(w) > 1\\n    )\\n)\\nranked = latest.sort_values('score', ascending=False)\\nranked[['score','pubdate','source','title','link']].to_csv('rank.csv', index=False)\\nranked[['score','pubdate','source','title','link']].to_csv(f'rank-{timestamp}.csv', index=False)\\nranked.head()\\n\", 'execution_count': 4}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': \"                      pubdate       source  \\\\\\n0   2025-07-16-21-50-04 +0000  startribune   \\n15  2025-07-16-19-56-53 +0000          nyt   \\n3   2025-07-16-21-38-43 +0000          nyt   \\n9   2025-07-16-20-53-24 +0000          bbc   \\n18  2025-07-16-19-15-59 +0000          bbc   \\n\\n                                                title  \\\\\\n0   Trump slams his own supporters as 'weaklings' ...   \\n15  Carney Moves to Reduce Canada’s Chinese Steel ...   \\n3   Eswatini Says It Will Repatriate Migrants Depo...   \\n9   Trump discussed firing Fed boss but 'highly un...   \\n18  Canada curbs steel imports to shield domestic ...   \\n\\n                                                 link  score  \\n0   https://www.startribune.com/trump-slams-his-ow...     27  \\n15  https://www.nytimes.com/2025/07/16/world/canad...     26  \\n3   https://www.nytimes.com/2025/07/16/world/afric...     25  \\n9      https://www.bbc.com/news/articles/c4geyrdprwjo     25  \\n18     https://www.bbc.com/news/articles/cy5wv7rzvk0o     25  \", 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n      <th>score</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>2025-07-16-21-50-04 +0000</td>\\n      <td>startribune</td>\\n      <td>Trump slams his own supporters as \\'weaklings\\' ...</td>\\n      <td>https://www.startribune.com/trump-slams-his-ow...</td>\\n      <td>27</td>\\n    </tr>\\n    <tr>\\n      <th>15</th>\\n      <td>2025-07-16-19-56-53 +0000</td>\\n      <td>nyt</td>\\n      <td>Carney Moves to Reduce Canada’s Chinese Steel ...</td>\\n      <td>https://www.nytimes.com/2025/07/16/world/canad...</td>\\n      <td>26</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2025-07-16-21-38-43 +0000</td>\\n      <td>nyt</td>\\n      <td>Eswatini Says It Will Repatriate Migrants Depo...</td>\\n      <td>https://www.nytimes.com/2025/07/16/world/afric...</td>\\n      <td>25</td>\\n    </tr>\\n    <tr>\\n      <th>9</th>\\n      <td>2025-07-16-20-53-24 +0000</td>\\n      <td>bbc</td>\\n      <td>Trump discussed firing Fed boss but \\'highly un...</td>\\n      <td>https://www.bbc.com/news/articles/c4geyrdprwjo</td>\\n      <td>25</td>\\n    </tr>\\n    <tr>\\n      <th>18</th>\\n      <td>2025-07-16-19-15-59 +0000</td>\\n      <td>bbc</td>\\n      <td>Canada curbs steel imports to shield domestic ...</td>\\n      <td>https://www.bbc.com/news/articles/cy5wv7rzvk0o</td>\\n      <td>25</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 4}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 7\n",
      "[NbConvertApp] Executing cell:\n",
      "top_rows = []\n",
      "working = word_scores.copy()\n",
      "remaining = latest.copy()\n",
      "for _ in range(10):\n",
      "    ranked_loop = remaining.assign(score=remaining['title'].apply(\n",
      "        lambda t: sum(working.get(w.lower(), 0)\n",
      "                      for w in re.findall(r'[A-Za-z]+', t)\n",
      "                      if len(w) > 1)\n",
      "    )).sort_values('score', ascending=False)\n",
      "    if ranked_loop.empty:\n",
      "        break\n",
      "    top_story = ranked_loop.iloc[0]\n",
      "    top_rows.append(top_story[['score','pubdate','source','title','link']])\n",
      "    words = set(re.findall(r'[A-Za-z]+', top_story['title'].lower()))\n",
      "    for w in words:\n",
      "        working.pop(w, None)\n",
      "    remaining = remaining.drop(top_story.name)\n",
      "top_df = pd.DataFrame(top_rows)\n",
      "top_df.to_csv('top.csv', index=False)\n",
      "top_df.to_csv(f'top-{timestamp}.csv', index=False)\n",
      "top_df\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"top_rows = []\\nworking = word_scores.copy()\\nremaining = latest.copy()\\nfor _ in range(10):\\n    ranked_loop = remaining.assign(score=remaining['title'].apply(\\n        lambda t: sum(working.get(w.lower(), 0)\\n                      for w in re.findall(r'[A-Za-z]+', t)\\n                      if len(w) > 1)\\n    )).sort_values('score', ascending=False)\\n    if ranked_loop.empty:\\n        break\\n    top_story = ranked_loop.iloc[0]\\n    top_rows.append(top_story[['score','pubdate','source','title','link']])\\n    words = set(re.findall(r'[A-Za-z]+', top_story['title'].lower()))\\n    for w in words:\\n        working.pop(w, None)\\n    remaining = remaining.drop(top_story.name)\\ntop_df = pd.DataFrame(top_rows)\\ntop_df.to_csv('top.csv', index=False)\\ntop_df.to_csv(f'top-{timestamp}.csv', index=False)\\ntop_df\\n\", 'execution_count': 5}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': \"    score                    pubdate       source  \\\\\\n0      27  2025-07-16-21-50-04 +0000  startribune   \\n1      19  2025-07-16-21-48-16 +0000          bbc   \\n59     17  2025-07-16-03-00-00 +0000        world   \\n51     16  2025-07-16-10-35-23 +0000          nyt   \\n15     14  2025-07-16-19-56-53 +0000          nyt   \\n29     14  2025-07-16-17-12-33 +0000  startribune   \\n28     14  2025-07-16-17-22-24 +0000          bbc   \\n19     13  2025-07-16-19-10-07 +0000  startribune   \\n49     13  2025-07-16-12-20-00 +0000        world   \\n24     12  2025-07-16-18-23-44 +0000  startribune   \\n\\n                                                title  \\\\\\n0   Trump slams his own supporters as 'weaklings' ...   \\n1   US says 'specific steps' agreed to end Syria v...   \\n59  Through Trial and Error, Iran Found Gaps in Is...   \\n51  South African AIDS Activist Pushes for H.I.V. ...   \\n15  Carney Moves to Reduce Canada’s Chinese Steel ...   \\n29  A $70 kosher pastrami sandwich? That’s what on...   \\n28  Serious questions for Tories over Afghan data ...   \\n19  Riders have been slow to hop on the new Gold L...   \\n49  Nvidia CEO Lavishes Praise on China in Beijing...   \\n24  Vance Boelter plans to plead not guilty in sho...   \\n\\n                                                 link  \\n0   https://www.startribune.com/trump-slams-his-ow...  \\n1      https://www.bbc.com/news/articles/cp90l77187zo  \\n59  https://www.wsj.com/world/middle-east/iran-isr...  \\n51  https://www.nytimes.com/2025/07/14/health/sout...  \\n15  https://www.nytimes.com/2025/07/16/world/canad...  \\n29  https://www.startribune.com/minneapolis-dfl-co...  \\n28     https://www.bbc.com/news/articles/c98w2e9leywo  \\n19  https://www.startribune.com/riders-have-been-s...  \\n49  https://www.wsj.com/world/asia/nvidia-ceo-lavi...  \\n24  https://www.startribune.com/vance-boelter-plan...  \", 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>score</th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>27</td>\\n      <td>2025-07-16-21-50-04 +0000</td>\\n      <td>startribune</td>\\n      <td>Trump slams his own supporters as \\'weaklings\\' ...</td>\\n      <td>https://www.startribune.com/trump-slams-his-ow...</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>19</td>\\n      <td>2025-07-16-21-48-16 +0000</td>\\n      <td>bbc</td>\\n      <td>US says \\'specific steps\\' agreed to end Syria v...</td>\\n      <td>https://www.bbc.com/news/articles/cp90l77187zo</td>\\n    </tr>\\n    <tr>\\n      <th>59</th>\\n      <td>17</td>\\n      <td>2025-07-16-03-00-00 +0000</td>\\n      <td>world</td>\\n      <td>Through Trial and Error, Iran Found Gaps in Is...</td>\\n      <td>https://www.wsj.com/world/middle-east/iran-isr...</td>\\n    </tr>\\n    <tr>\\n      <th>51</th>\\n      <td>16</td>\\n      <td>2025-07-16-10-35-23 +0000</td>\\n      <td>nyt</td>\\n      <td>South African AIDS Activist Pushes for H.I.V. ...</td>\\n      <td>https://www.nytimes.com/2025/07/14/health/sout...</td>\\n    </tr>\\n    <tr>\\n      <th>15</th>\\n      <td>14</td>\\n      <td>2025-07-16-19-56-53 +0000</td>\\n      <td>nyt</td>\\n      <td>Carney Moves to Reduce Canada’s Chinese Steel ...</td>\\n      <td>https://www.nytimes.com/2025/07/16/world/canad...</td>\\n    </tr>\\n    <tr>\\n      <th>29</th>\\n      <td>14</td>\\n      <td>2025-07-16-17-12-33 +0000</td>\\n      <td>startribune</td>\\n      <td>A $70 kosher pastrami sandwich? That’s what on...</td>\\n      <td>https://www.startribune.com/minneapolis-dfl-co...</td>\\n    </tr>\\n    <tr>\\n      <th>28</th>\\n      <td>14</td>\\n      <td>2025-07-16-17-22-24 +0000</td>\\n      <td>bbc</td>\\n      <td>Serious questions for Tories over Afghan data ...</td>\\n      <td>https://www.bbc.com/news/articles/c98w2e9leywo</td>\\n    </tr>\\n    <tr>\\n      <th>19</th>\\n      <td>13</td>\\n      <td>2025-07-16-19-10-07 +0000</td>\\n      <td>startribune</td>\\n      <td>Riders have been slow to hop on the new Gold L...</td>\\n      <td>https://www.startribune.com/riders-have-been-s...</td>\\n    </tr>\\n    <tr>\\n      <th>49</th>\\n      <td>13</td>\\n      <td>2025-07-16-12-20-00 +0000</td>\\n      <td>world</td>\\n      <td>Nvidia CEO Lavishes Praise on China in Beijing...</td>\\n      <td>https://www.wsj.com/world/asia/nvidia-ceo-lavi...</td>\\n    </tr>\\n    <tr>\\n      <th>24</th>\\n      <td>12</td>\\n      <td>2025-07-16-18-23-44 +0000</td>\\n      <td>startribune</td>\\n      <td>Vance Boelter plans to plead not guilty in sho...</td>\\n      <td>https://www.startribune.com/vance-boelter-plan...</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 5}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 9\n",
      "[NbConvertApp] Executing cell:\n",
      "import pandas as pd\n",
      "pd.read_csv('top.csv').to_json('top.json', orient='records', indent=2)\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import pandas as pd\\npd.read_csv('top.csv').to_json('top.json', orient='records', indent=2)\\n\", 'execution_count': 6}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 11\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7feaffb18050>\n",
      "[NbConvertApp] Writing 23717 bytes to /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Everything up to date.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update outdated notebooks until none remain\n",
    "import json, re, time, subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_dir = Path.cwd()\n",
    "if not (repo_dir / 'analysis').is_dir():\n",
    "    repo_dir = repo_dir.parent\n",
    "analysis_dir = repo_dir / 'analysis'\n",
    "data_dir = repo_dir / 'data'\n",
    "\n",
    "pattern = re.compile(r'[A-Za-z0-9_/.-]*latest\\.(?:csv|json|xml|rss)')\n",
    "\n",
    "def mtime_str(p: Path) -> str:\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(p.stat().st_mtime))\n",
    "\n",
    "def build_dep_map():\n",
    "    ipynb_paths = sorted(analysis_dir.rglob('*.ipynb'))\n",
    "    dep_map = {}\n",
    "    for nb in ipynb_paths:\n",
    "        text = nb.read_text()\n",
    "        matches = sorted(set(pattern.findall(text)))\n",
    "        deps = []\n",
    "        for m in matches:\n",
    "            dep = (nb.parent / m).resolve()\n",
    "            if not dep.exists():\n",
    "                dep = (repo_dir / m.lstrip('./')).resolve()\n",
    "            if dep.exists():\n",
    "                deps.append(dep)\n",
    "        dep_map[nb] = deps\n",
    "    return dep_map\n",
    "\n",
    "def outdated(nb, deps):\n",
    "    nb_mtime = nb.stat().st_mtime\n",
    "    return any(d.stat().st_mtime > nb_mtime for d in deps)\n",
    "\n",
    "\n",
    "def execute(nb: Path):\n",
    "    import shutil\n",
    "    rel = str(nb.relative_to(repo_dir))\n",
    "    if not shutil.which('jupyter'):\n",
    "        print(f'jupyter not available - skipping {rel}')\n",
    "        return\n",
    "    print(f'Running {rel} …')\n",
    "    cmd=[sys.executable,'-m','jupyter','nbconvert','--to','notebook','--inplace','--execute','--ExecutePreprocessor.timeout=600','--debug',str(nb)]\n",
    "    print('  Command:', ' '.join(cmd))\n",
    "    proc=subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    print(proc.stdout)\n",
    "    if proc.returncode==0:\n",
    "        print('  ✓ success')\n",
    "    else:\n",
    "        print(f'  ✗ failed with exit code {proc.returncode}')\n",
    "while True:\n",
    "    dep_map=build_dep_map()\n",
    "    outdated_nbs=[nb for nb,deps in dep_map.items() if deps and outdated(nb,deps)]\n",
    "    report={'outdated_notebooks':[str(nb.relative_to(repo_dir)) for nb in outdated_nbs]}\n",
    "    deps_path = repo_dir / 'dependencies.json'\n",
    "    deps_path.write_text(\n",
    "        json.dumps(report, indent=2) + \"\\n\"\n",
    "    )\n",
    "    if not outdated_nbs:\n",
    "        print('Everything up to date.')\n",
    "        break\n",
    "    for nb in outdated_nbs:\n",
    "        execute(nb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
