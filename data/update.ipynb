{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Script\n",
    "This notebook orchestrates data downloads and analysis refreshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'nbconvert' not found \u2014 installing \u2026\n",
      "All dependencies ready.\n",
      "\n",
      "Skipping GDPC1 \u2013 up to date\n",
      "Skipping A939RX0Q048SBEA \u2013 up to date\n",
      "Skipping M2REAL \u2013 up to date\n",
      "Skipping UNRATE \u2013 up to date\n",
      "Skipping CLVMNACSCAB1GQDE \u2013 up to date\n",
      "Skipping GFDEBTN \u2013 up to date\n",
      "Skipping GFDEGDQ188S \u2013 up to date\n",
      "Skipping TDSP \u2013 up to date\n",
      "Fetching news-us-politics-wapo \u2026 \u2717 failed: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=30)\n",
      "Skipping news-world-chi-tribune \u2013 up to date\n",
      "Skipping news-business-chi-tribune \u2013 up to date\n",
      "Skipping news-us-politics-chi-tribune \u2013 up to date\n",
      "Everything up to date.\n"
     ]
    }
   ],
   "source": [
    "# ========== Bootstrap: ensure required Python packages are present ==========\n",
    "import importlib, subprocess, sys\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: str | None = None):\n",
    "    \"\"\"\n",
    "    Import `import_name` (defaults to `pkg_name`); if that fails, pip\u2011install.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"Package '{pkg_name}' not found \u2014 installing \u2026\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "    finally:\n",
    "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
    "\n",
    "# --- Required third\u2011party libraries ------------------------------------------\n",
    "_ensure(\"pandas\")\n",
    "_ensure(\"requests\")\n",
    "_ensure(\"feedparser\")\n",
    "_ensure(\"textblob\")\n",
    "_ensure(\"jupyter\")\n",
    "_ensure(\"nbconvert\")\n",
    "print(\"All dependencies ready.\\n\")\n",
    "\n",
    "# --- Standard imports --------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json\n",
    "import pandas as pd, requests, urllib.parse\n",
    "\n",
    "# --- Helper: replace [date %Y-%m-%d] tokens -----------------------------------\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        fmt = m.group(1).strip()\n",
    "        return dt.date.today().strftime(fmt)\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "# --- Helper: append API key if specified -----------------------------------\n",
    "def add_apikey(url: str, env_var: str | None) -> str:\n",
    "    if env_var and str(env_var).lower() != \"nan\":\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f'{url}{sep}api_key={urllib.parse.quote_plus(key)}'\n",
    "        else:\n",
    "            print(f\"Warning: environment variable '{env_var}' not set.\")\n",
    "    return url\n",
    "\n",
    "# --- Cadence map (word \u2192 minimum seconds between fetches) ------------------------\n",
    "CADENCE_SECONDS = {\n",
    "    \"hourly\": 3600,\n",
    "    \"daily\": 86400,\n",
    "    \"weekly\": 604800,\n",
    "    \"monthly\": 2592000,\n",
    "    \"quarterly\": 7776000,\n",
    "}\n",
    "\n",
    "# --- Resolve base directory so notebook works from repo root or data folder ---\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "\n",
    "# --- Load catalog -------------------------------------------------------------\n",
    "catalog_path = BASE_DIR / 'catalog.csv'\n",
    "cat = pd.read_csv(catalog_path)\n",
    "cat['filetype'] = cat['filetype'].astype(str).str.strip().str.lstrip('.')\n",
    "\n",
    "now = dt.datetime.now()\n",
    "today = now.date()\n",
    "updated_rows = []                # remember which rows we refresh\n",
    "\n",
    "for idx, row in cat.iterrows():\n",
    "    folder = BASE_DIR / str(row['category']) / str(row['source']) / str(row['folder'])\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = str(row['filetype']).strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    latest_fp = folder / f'latest.{output_ext}'\n",
    "    dated_fp = folder / f\"{now:%Y-%m-%d-%H}.{output_ext}\" if cadence == \"hourly\" else folder / f\"{today:%Y-%m-%d}.{output_ext}\"\n",
    "    if dated_fp.exists():\n",
    "        cat.at[idx, 'last_fetched'] = now.isoformat(timespec='minutes')\n",
    "        continue\n",
    "    last_fetched = (\n",
    "        pd.to_datetime(row[\"last_fetched\"])\n",
    "        if pd.notna(row[\"last_fetched\"]) else None\n",
    "    )\n",
    "\n",
    "    # ---- Determine if an update is due --------------------------------------\n",
    "    cadence = str(row[\"cadence\"]).lower().strip()\n",
    "    min_age = CADENCE_SECONDS.get(cadence, 30*86400)        # default 30 days\n",
    "    needs_update = (\n",
    "        (not latest_fp.exists()) or\n",
    "        (not last_fetched) or\n",
    "        (now - last_fetched).total_seconds() >= min_age\n",
    "    )\n",
    "\n",
    "    if not needs_update:\n",
    "        print(f\"Skipping {row['folder']} \u2013 up to date\")\n",
    "        continue\n",
    "\n",
    "    # ---- Build the request URL ---------------------------------------------\n",
    "    url = substitute_date_tokens(str(row[\"url\"]))\n",
    "    url = add_apikey(url, str(row.get('api_key') or '').strip() or None)\n",
    "\n",
    "    print(f\"Fetching {row['folder']} \u2026\", end=\" \")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        r.raise_for_status()\n",
    "        if filetype.lower() in ('rss', 'xml'):\n",
    "            feed = feedparser.parse(r.content)\n",
    "            entries = []\n",
    "            for e in feed.entries:\n",
    "                text = ' '.join(filter(None, [e.get('title'), e.get('summary')]))\n",
    "                polarity = textblob.TextBlob(text).sentiment.polarity\n",
    "                entries.append({'title': e.get('title'), 'link': e.get('link'),\n",
    "                               'published': e.get('published'),\n",
    "                               'sentiment': polarity})\n",
    "            content_bytes = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "        else:\n",
    "            content_bytes = r.content\n",
    "        if filetype.lower() == 'json':\n",
    "            try:\n",
    "                data_json = r.json()\n",
    "            except Exception:\n",
    "                data_json = None\n",
    "            if isinstance(data_json, dict) and data_json.get('error_message'):\n",
    "                raise ValueError(data_json['error_message'])\n",
    "        # ---- Save snapshot and latest --------------------------------------\n",
    "        if latest_fp.exists() and latest_fp.read_bytes() == content_bytes:\n",
    "            cat.at[idx, 'last_fetched'] = now.isoformat(timespec='minutes')\n",
    "            print('no change')\n",
    "            continue\n",
    "        dated_fp.write_bytes(content_bytes)\n",
    "        shutil.copyfile(dated_fp, latest_fp)\n",
    "\n",
    "        # ---- Mark success in catalog ---------------------------------------\n",
    "        cat.at[idx, \"last_fetched\"] = now.isoformat(timespec='minutes')\n",
    "        updated_rows.append(row[\"folder\"])\n",
    "        print(\"\u2713 success\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u2717 failed: {e}\")\n",
    "\n",
    "# --- Persist catalog if anything changed -------------------------------------\n",
    "if updated_rows:\n",
    "    cat.to_csv(catalog_path, index=False)\n",
    "    print(\"\\nUpdated:\", \", \".join(updated_rows))\n",
    "else:\n",
    "    print(\"Everything up to date.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index files generated for GDPC1, A939RX0Q048SBEA, M2REAL, UNRATE, CLVMNACSCAB1GQDE, GFDEBTN, GFDEGDQ188S, TDSP, news-us-nyt, news-world-nyt, news-africa-nyt, news-europe-nyt, news-asia-nyt, news-americas-nyt, news-middle-east-nyt, news-business-nyt, news-economy-nyt, news-us-politics-nyt, news-world-wsj, news-us-wsj, news-business-wsj, news-markets-wsj, news-economy-wsj, news-us-politics-wsj, news-us-politics-wapo, news-us-wapo, news-world-wapo, news-business-wapo, latimes-business, latimes-us, latimes-us-politics, news-world-chi-tribune, news-business-chi-tribune, news-us-politics-chi-tribune\n"
     ]
    }
   ],
   "source": [
    "# This cell updates the markdown index files for all the data sources\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "with open(BASE_DIR / 'catalog.csv', newline='') as f:\n",
    "    cat = list(csv.DictReader(f))\n",
    "\n",
    "for row in cat:\n",
    "    folder = BASE_DIR / row['category'] / row['source'] / row['folder']\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = row['filetype'].strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    desc = row['description'].strip()\n",
    "    source = row['source'].strip()\n",
    "    date = row.get('last_fetched', '').strip()\n",
    "\n",
    "    pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}(?:-\\d{2})?\\.\" + re.escape(output_ext) + r\"$\")\n",
    "    dated_files = sorted(p.name for p in folder.iterdir() if pattern.match(p.name))\n",
    "\n",
    "    lines = [\n",
    "        '---',\n",
    "        'layout: default',\n",
    "        f'title: {source} - {desc}',\n",
    "        f'date: {date}',\n",
    "        '---',\n",
    "        '',\n",
    "        f'## {source} - {desc}',\n",
    "        '',\n",
    "        '<div id=\"data-chart\"></div>',\n",
    "        '<div id=\"data-table\"></div>',\n",
    "    ]\n",
    "\n",
    "    if row['source'] == 'fred' and filetype == 'json':\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  ShowChart($('#data-chart'));\",\n",
    "            \"  SourceTabler($('#data-table'));\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "    else:\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  document.getElementById('data-table').textContent = 'This source isn\\'t supported for tables yet.';\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "\n",
    "    lines += [\n",
    "        '',\n",
    "        '## File Versions:',\n",
    "    ]\n",
    "    links = [f'[Latest version](./latest.{output_ext})'] + [f'[{fname}](./{fname})' for fname in dated_files]\n",
    "    for i, link in enumerate(links, 1):\n",
    "        lines.append(f'{i}. {link}')\n",
    "    (folder / 'index.md').write_text(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print('Index files generated for', ', '.join(r['folder'] for r in cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"notebooks\": [\n",
      "    [\n",
      "      \"analysis\\\\headlines\\\\update_headlines.ipynb\",\n",
      "      \"2025-06-07 18:10:36\"\n",
      "    ],\n",
      "    [\n",
      "      \"analysis\\\\news-topics\\\\analyze_headlines.ipynb\",\n",
      "      \"2025-06-05 13:21:02\"\n",
      "    ]\n",
      "  ],\n",
      "  \"latest_files\": [\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\A939RX0Q048SBEA\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:03\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\CLVMNACSCAB1GQDE\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:04\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\GDPC1\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:03\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\GFDEBTN\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:04\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\GFDEGDQ188S\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:05\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\M2REAL\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:03\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\TDSP\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:05\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\economics\\\\fred\\\\UNRATE\\\\latest.json\",\n",
      "      \"2025-06-04 20:12:04\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-africa\\\\nyt\\\\news-africa-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:48\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-americas\\\\nyt\\\\news-americas-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:49\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-asia\\\\nyt\\\\news-asia-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:48\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\latimes\\\\latimes-business\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:32\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\nyt\\\\news-business-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:49\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\tribune\\\\news-business-chi-tribune\\\\latest.json\",\n",
      "      \"2025-06-04 21:25:09\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\wapo\\\\news-business-wapo\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:32\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\wsj\\\\news-business-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-business\\\\wsj\\\\news-markets-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-economy\\\\nyt\\\\news-economy-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:49\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-economy\\\\wsj\\\\news-economy-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-europe\\\\nyt\\\\news-europe-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:48\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-middle-east\\\\nyt\\\\news-middle-east-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:49\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us\\\\latimes\\\\latimes-us\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:32\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us\\\\nyt\\\\news-us-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 18:10:36\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us\\\\wapo\\\\news-us-wapo\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:15\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us\\\\wsj\\\\news-us-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us-politics\\\\latimes\\\\latimes-us-politics\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:32\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us-politics\\\\nyt\\\\news-us-politics-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us-politics\\\\tribune\\\\news-us-politics-chi-tribune\\\\latest.json\",\n",
      "      \"2025-06-04 21:25:09\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-us-politics\\\\wsj\\\\news-us-politics-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:51\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-world\\\\nyt\\\\news-world-nyt\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:48\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-world\\\\tribune\\\\news-world-chi-tribune\\\\latest.json\",\n",
      "      \"2025-06-04 21:25:08\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-world\\\\wapo\\\\news-world-wapo\\\\latest.json\",\n",
      "      \"2025-06-07 17:05:23\"\n",
      "    ],\n",
      "    [\n",
      "      \"data\\\\news-world\\\\wsj\\\\news-world-wsj\\\\latest.json\",\n",
      "      \"2025-06-07 16:01:50\"\n",
      "    ],\n",
      "    [\n",
      "      \"analysis\\\\headlines\\\\latest.csv\",\n",
      "      \"2025-06-07 17:13:34\"\n",
      "    ]\n",
      "  ],\n",
      "  \"dependencies\": [\n",
      "    [\n",
      "      \"analysis\\\\headlines\\\\update_headlines.ipynb\",\n",
      "      \"2025-06-07 18:10:36\",\n",
      "      [\n",
      "        [\n",
      "          \"data\\\\news-us\\\\nyt\\\\news-us-nyt\\\\latest.json\",\n",
      "          \"2025-06-07 18:10:36\"\n",
      "        ],\n",
      "        [\n",
      "          \"data\\\\news-us\\\\wsj\\\\news-us-wsj\\\\latest.json\",\n",
      "          \"2025-06-07 16:01:50\"\n",
      "        ],\n",
      "        [\n",
      "          \"analysis\\\\headlines\\\\latest.csv\",\n",
      "          \"2025-06-07 17:13:34\"\n",
      "        ]\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"analysis\\\\news-topics\\\\analyze_headlines.ipynb\",\n",
      "      \"2025-06-05 13:21:02\",\n",
      "      [\n",
      "        [\n",
      "          \"analysis\\\\headlines\\\\latest.csv\",\n",
      "          \"2025-06-07 17:13:34\"\n",
      "        ]\n",
      "      ]\n",
      "    ]\n",
      "  ],\n",
      "  \"only_data_notebooks\": [],\n",
      "  \"other_notebooks\": [\n",
      "    [\n",
      "      \"analysis\\\\headlines\\\\update_headlines.ipynb\",\n",
      "      \"2025-06-07 18:10:36\"\n",
      "    ],\n",
      "    [\n",
      "      \"analysis\\\\news-topics\\\\analyze_headlines.ipynb\",\n",
      "      \"2025-06-05 13:21:02\"\n",
      "    ]\n",
      "  ],\n",
      "  \"outdated_notebooks\": [\n",
      "    \"analysis\\\\headlines\\\\update_headlines.ipynb\",\n",
      "    \"analysis\\\\news-topics\\\\analyze_headlines.ipynb\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4797"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependency report\n",
    "from pathlib import Path\n",
    "import json, re, time\n",
    "\n",
    "repo_dir = Path.cwd()\n",
    "if not (repo_dir / 'analysis').is_dir():\n",
    "    repo_dir = repo_dir.parent\n",
    "analysis_dir = repo_dir / 'analysis'\n",
    "data_dir = repo_dir / 'data'\n",
    "\n",
    "pattern = re.compile(r'[A-Za-z0-9_/.-]*latest\\.(?:csv|json|xml|rss)')\n",
    "\n",
    "def mtime_str(p: Path) -> str:\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(p.stat().st_mtime))\n",
    "\n",
    "# --- List notebooks ---\n",
    "ipynb_paths = sorted(analysis_dir.rglob('*.ipynb'))\n",
    "notebooks = [[str(p.relative_to(repo_dir)), mtime_str(p)] for p in ipynb_paths]\n",
    "\n",
    "# --- List latest files ---\n",
    "latest_files = sorted(data_dir.rglob('latest.*')) + sorted(analysis_dir.rglob('latest.*'))\n",
    "latest = [[str(p.relative_to(repo_dir)), mtime_str(p)] for p in latest_files]\n",
    "\n",
    "# --- Map notebook dependencies ---\n",
    "dep_map = {}\n",
    "for nb in ipynb_paths:\n",
    "    text = nb.read_text()\n",
    "    matches = sorted(set(pattern.findall(text)))\n",
    "    deps = []\n",
    "    for m in matches:\n",
    "        dep = (nb.parent / m).resolve()\n",
    "        if not dep.exists():\n",
    "            dep = (repo_dir / m.lstrip('./')).resolve()\n",
    "        if dep.exists():\n",
    "            deps.append([str(dep.relative_to(repo_dir)), mtime_str(dep)])\n",
    "    dep_map[str(nb.relative_to(repo_dir))] = {'modified': mtime_str(nb), 'deps': deps}\n",
    "\n",
    "dep_list = [[nb, info['modified'], info['deps']] for nb, info in dep_map.items()]\n",
    "\n",
    "only_data = []\n",
    "other = []\n",
    "for nb, info in dep_map.items():\n",
    "    if info['deps'] and all(d[0].startswith('data/') for d in info['deps']):\n",
    "        only_data.append([nb, info['modified']])\n",
    "    else:\n",
    "        other.append([nb, info['modified']])\n",
    "\n",
    "# --- Determine outdated notebooks ---\n",
    "outdated = []\n",
    "for nb, info in dep_map.items():\n",
    "    if info['deps']:\n",
    "        nb_mtime = (repo_dir / nb).stat().st_mtime\n",
    "        newest_dep = max((repo_dir / d[0]).stat().st_mtime for d in info['deps'])\n",
    "        if newest_dep > nb_mtime:\n",
    "            outdated.append(nb)\n",
    "\n",
    "report = {\n",
    "    'notebooks': notebooks,\n",
    "    'latest_files': latest,\n",
    "    'dependencies': dep_list,\n",
    "    'only_data_notebooks': only_data,\n",
    "    'other_notebooks': other,\n",
    "    'outdated_notebooks': outdated,\n",
    "}\n",
    "json_txt = json.dumps(report, indent=2)\n",
    "print(json_txt)\n",
    "(repo_dir / 'dependencies.json').write_text(json_txt + '\\n')\n",
    "(repo_dir / f\"dependencies-{time.strftime('%Y-%m-%d')}.json\").write_text(json_txt + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis\\headlines\\update_headlines.ipynb \u2026\n",
      "  Command: c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\headlines\\update_headlines.ipynb\n",
      "[NbConvertApp] Searching ['C:\\\\Users\\\\CJ\\\\.jupyter', 'C:\\\\Users\\\\CJ\\\\AppData\\\\Roaming\\\\Python\\\\etc\\\\jupyter', 'c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\etc\\\\jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in C:\\Users\\CJ\\AppData\\Roaming\\Python\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in C:\\Users\\CJ\\.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in C:\\Users\\CJ\\AppData\\Roaming\\Python\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in C:\\Users\\CJ\\.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\headlines\\update_headlines.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'update_headlines'\n",
      "c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nbformat\\__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python.exe', '-m', 'ipykernel_launcher', '-f', 'C:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Temp\\\\tmpsryzabjl.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:62172\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:62169\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:62169\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:62168\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:62168\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:62170\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:62170\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:62171\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:62172\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:62172\n",
      "C:\\Users\\CJ\\AppData\\Roaming\\Python\\Python312\\site-packages\\zmq\\_future.py:718: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# Some of the dependencies, to trigger the dependency checker:\n",
      "# data/news-us/nyt/news-us-nyt/latest.json\n",
      "# data/news-us/wsj/news-us-wsj/latest.json\n",
      "\n",
      "\n",
      "from pathlib import Path\n",
      "import csv\n",
      "import json\n",
      "import xml.etree.ElementTree as ET\n",
      "from datetime import datetime, timezone, timedelta\n",
      "from email.utils import parsedate_to_datetime\n",
      "import shutil\n",
      "\n",
      "BASE_DIR = Path.cwd()\n",
      "REPO_DIR = BASE_DIR\n",
      "while not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):\n",
      "    if REPO_DIR.parent == REPO_DIR:\n",
      "        raise FileNotFoundError('Repository root not found')\n",
      "    REPO_DIR = REPO_DIR.parent\n",
      "DATA_DIR = REPO_DIR / 'data'\n",
      "HEADLINES_DIR = REPO_DIR / 'analysis/headlines'\n",
      "HEADLINES_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "def parse_pubdate(date_str):\n",
      "    try:\n",
      "        dt = parsedate_to_datetime(date_str) if date_str else None\n",
      "        if dt is None:\n",
      "            return None\n",
      "        if dt.tzinfo is None:\n",
      "            dt = dt.replace(tzinfo=timezone.utc)\n",
      "        return dt.astimezone(timezone.utc)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def format_pubdate(dt):\n",
      "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''\n",
      "\n",
      "def parse_feed(path: Path):\n",
      "    entries = []\n",
      "    if path.suffix == '.json':\n",
      "        with open(path, 'r', encoding='utf-8') as f:\n",
      "            data = json.load(f)\n",
      "        for item in data.get('entries', []):\n",
      "            title = item.get('title')\n",
      "            link = item.get('link')\n",
      "            pub = parse_pubdate(item.get('published'))\n",
      "            if title and link:\n",
      "                entries.append((pub, title.strip(), link.strip()))\n",
      "    else:\n",
      "        try:\n",
      "            tree = ET.parse(path)\n",
      "            root = tree.getroot()\n",
      "        except ET.ParseError:\n",
      "            return entries\n",
      "        for item in root.iter():\n",
      "            if item.tag.lower().endswith(('item', 'entry')):\n",
      "                title = None\n",
      "                link = None\n",
      "                pub = None\n",
      "                for child in item:\n",
      "                    tag = child.tag.lower()\n",
      "                    if tag.endswith('title'):\n",
      "                        title = (child.text or '').strip()\n",
      "                    if tag.endswith('link'):\n",
      "                        link = (child.text or '').strip() or child.attrib.get('href')\n",
      "                    if tag.endswith(('pubdate', 'published', 'updated')):\n",
      "                        pub = parse_pubdate((child.text or '').strip())\n",
      "                if title and link:\n",
      "                    entries.append((pub, title, link))\n",
      "    return entries\n",
      "\n",
      "def collect_headlines():\n",
      "    all_entries = []\n",
      "    for source in DATA_DIR.iterdir():\n",
      "        if source.is_dir() and source.name.startswith('news'):\n",
      "            candidates = [p for p in source.rglob('latest.*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                candidates = [p for p in source.rglob('*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                continue\n",
      "            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
      "            source_name = latest_file.relative_to(DATA_DIR).parts[1]\n",
      "            for pub, title, link in parse_feed(latest_file):\n",
      "                all_entries.append((pub, title, link, source_name))\n",
      "    return all_entries\n",
      "\n",
      "def _date_key(date_str):\n",
      "    try:\n",
      "        return parsedate_to_datetime(date_str) if date_str else datetime.min\n",
      "    except Exception:\n",
      "        return datetime.min\n",
      "\n",
      "def update_headlines():\n",
      "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\n",
      "    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\n",
      "    if hourly_file.exists():\n",
      "        print(f\"{hourly_file.name} already exists. Skipping update.\")\n",
      "        return\n",
      "    entries = collect_headlines()\n",
      "    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
      "    deduped = []\n",
      "    seen_titles = set()\n",
      "    seen_links = set()\n",
      "    for pub, title, link, src in entries:\n",
      "        t_key = title.lower()\n",
      "        l_key = link.lower()\n",
      "        if t_key in seen_titles or l_key in seen_links:\n",
      "            continue\n",
      "        deduped.append((pub, src, title, link))\n",
      "        seen_titles.add(t_key)\n",
      "        seen_links.add(l_key)\n",
      "    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\n",
      "    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\n",
      "    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        writer.writerow(['pubdate', 'source', 'title', 'link'])\n",
      "        for pub, src, title, link in deduped:\n",
      "            writer.writerow([format_pubdate(pub), src, title, link])\n",
      "    latest_file = HEADLINES_DIR / \"latest.csv\"\n",
      "    shutil.copy(hourly_file, latest_file)\n",
      "    print(f\"Wrote {hourly_file} and updated latest.csv\")\n",
      "    # Get the most recent headline\n",
      "\n",
      "\n",
      "update_headlines()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': '# Some of the dependencies, to trigger the dependency checker:\\n# data/news-us/nyt/news-us-nyt/latest.json\\n# data/news-us/wsj/news-us-wsj/latest.json\\n\\n\\nfrom pathlib import Path\\nimport csv\\nimport json\\nimport xml.etree.ElementTree as ET\\nfrom datetime import datetime, timezone, timedelta\\nfrom email.utils import parsedate_to_datetime\\nimport shutil\\n\\nBASE_DIR = Path.cwd()\\nREPO_DIR = BASE_DIR\\nwhile not ((REPO_DIR / \\'data\\').exists() and (REPO_DIR / \\'analysis\\').exists()):\\n    if REPO_DIR.parent == REPO_DIR:\\n        raise FileNotFoundError(\\'Repository root not found\\')\\n    REPO_DIR = REPO_DIR.parent\\nDATA_DIR = REPO_DIR / \\'data\\'\\nHEADLINES_DIR = REPO_DIR / \\'analysis/headlines\\'\\nHEADLINES_DIR.mkdir(parents=True, exist_ok=True)\\n\\ndef parse_pubdate(date_str):\\n    try:\\n        dt = parsedate_to_datetime(date_str) if date_str else None\\n        if dt is None:\\n            return None\\n        if dt.tzinfo is None:\\n            dt = dt.replace(tzinfo=timezone.utc)\\n        return dt.astimezone(timezone.utc)\\n    except Exception:\\n        return None\\n\\ndef format_pubdate(dt):\\n    return dt.strftime(\\'%Y-%m-%d-%H-%M-%S +0000\\') if dt else \\'\\'\\n\\ndef parse_feed(path: Path):\\n    entries = []\\n    if path.suffix == \\'.json\\':\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            data = json.load(f)\\n        for item in data.get(\\'entries\\', []):\\n            title = item.get(\\'title\\')\\n            link = item.get(\\'link\\')\\n            pub = parse_pubdate(item.get(\\'published\\'))\\n            if title and link:\\n                entries.append((pub, title.strip(), link.strip()))\\n    else:\\n        try:\\n            tree = ET.parse(path)\\n            root = tree.getroot()\\n        except ET.ParseError:\\n            return entries\\n        for item in root.iter():\\n            if item.tag.lower().endswith((\\'item\\', \\'entry\\')):\\n                title = None\\n                link = None\\n                pub = None\\n                for child in item:\\n                    tag = child.tag.lower()\\n                    if tag.endswith(\\'title\\'):\\n                        title = (child.text or \\'\\').strip()\\n                    if tag.endswith(\\'link\\'):\\n                        link = (child.text or \\'\\').strip() or child.attrib.get(\\'href\\')\\n                    if tag.endswith((\\'pubdate\\', \\'published\\', \\'updated\\')):\\n                        pub = parse_pubdate((child.text or \\'\\').strip())\\n                if title and link:\\n                    entries.append((pub, title, link))\\n    return entries\\n\\ndef collect_headlines():\\n    all_entries = []\\n    for source in DATA_DIR.iterdir():\\n        if source.is_dir() and source.name.startswith(\\'news\\'):\\n            candidates = [p for p in source.rglob(\\'latest.*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                candidates = [p for p in source.rglob(\\'*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                continue\\n            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\\n            source_name = latest_file.relative_to(DATA_DIR).parts[1]\\n            for pub, title, link in parse_feed(latest_file):\\n                all_entries.append((pub, title, link, source_name))\\n    return all_entries\\n\\ndef _date_key(date_str):\\n    try:\\n        return parsedate_to_datetime(date_str) if date_str else datetime.min\\n    except Exception:\\n        return datetime.min\\n\\ndef update_headlines():\\n    timestamp = datetime.utcnow().strftime(\\'%Y-%m-%d-%H-00\\')\\n    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\\n    if hourly_file.exists():\\n        print(f\"{hourly_file.name} already exists. Skipping update.\")\\n        return\\n    entries = collect_headlines()\\n    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\\n    deduped = []\\n    seen_titles = set()\\n    seen_links = set()\\n    for pub, title, link, src in entries:\\n        t_key = title.lower()\\n        l_key = link.lower()\\n        if t_key in seen_titles or l_key in seen_links:\\n            continue\\n        deduped.append((pub, src, title, link))\\n        seen_titles.add(t_key)\\n        seen_links.add(l_key)\\n    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\\n    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\\n    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n        writer = csv.writer(f)\\n        writer.writerow([\\'pubdate\\', \\'source\\', \\'title\\', \\'link\\'])\\n        for pub, src, title, link in deduped:\\n            writer.writerow([format_pubdate(pub), src, title, link])\\n    latest_file = HEADLINES_DIR / \"latest.csv\"\\n    shutil.copy(hourly_file, latest_file)\\n    print(f\"Wrote {hourly_file} and updated latest.csv\")\\n    # Get the most recent headline\\n\\n\\nupdate_headlines()\\n\\n\\n\\n\\n\\n', 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'Wrote c:\\\\Users\\\\CJ\\\\Documents\\\\GitHub\\\\Analysis\\\\analysis\\\\headlines\\\\2025-06-08-01-00.csv and updated latest.csv\\n'}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"C:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_41832\\\\2800259878.py:94: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\\n\"}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Kernel is taking too long to finish, terminating\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x00000278002A07D0>\n",
      "[NbConvertApp] Writing 7498 bytes to c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\headlines\\update_headlines.ipynb\n",
      "\n",
      "  \u2713 success\n",
      "Running analysis\\news-topics\\analyze_headlines.ipynb \u2026\n",
      "  Command: c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\news-topics\\analyze_headlines.ipynb\n",
      "[NbConvertApp] Searching ['C:\\\\Users\\\\CJ\\\\.jupyter', 'C:\\\\Users\\\\CJ\\\\AppData\\\\Roaming\\\\Python\\\\etc\\\\jupyter', 'c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\etc\\\\jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in C:\\Users\\CJ\\AppData\\Roaming\\Python\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in C:\\Users\\CJ\\.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in C:\\Users\\CJ\\AppData\\Roaming\\Python\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in C:\\Users\\CJ\\.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\news-topics\\analyze_headlines.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'analyze_headlines'\n",
      "c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nbformat\\__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python.exe', '-m', 'ipykernel_launcher', '-f', 'C:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Temp\\\\tmpu9k8fxwd.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:62289\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:62286\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:62286\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:62285\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:62285\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:62287\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:62287\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:62288\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:62289\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:62289\n",
      "C:\\Users\\CJ\\AppData\\Roaming\\Python\\Python312\\site-packages\\zmq\\_future.py:718: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "import pandas as pd\n",
      "latest = pd.read_csv('../headlines/latest.csv')\n",
      "latest.head()\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import pandas as pd\\nlatest = pd.read_csv('../headlines/latest.csv')\\nlatest.head()\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '                     pubdate source  \\\\\\n0  2025-06-07-22-31-19 +0000    nyt   \\n1  2025-06-07-22-15-19 +0000    nyt   \\n2  2025-06-07-22-00-28 +0000    nyt   \\n3  2025-06-07-21-53-39 +0000    nyt   \\n4  2025-06-07-21-06-45 +0000    nyt   \\n\\n                                               title  \\\\\\n0  Immigration Agents Conduct More Raids in LA, C...   \\n1  Trump Warns of Consequences for Musk if He Bac...   \\n2  WorldPride Parade Attendees Celebrate in D.C. ...   \\n3  Trump Targets Workplaces as Immigration Crackd...   \\n4  The world is celebrating Pride in Washington, ...   \\n\\n                                                link  \\n0  https://www.nytimes.com/2025/06/07/us/la-immig...  \\n1  https://www.nytimes.com/2025/06/07/us/politics...  \\n2  https://www.nytimes.com/2025/06/07/us/worldpri...  \\n3  https://www.nytimes.com/2025/06/07/us/trump-ta...  \\n4  https://www.nytimes.com/2025/06/07/us/the-worl...  ', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>2025-06-07-22-31-19 +0000</td>\\n      <td>nyt</td>\\n      <td>Immigration Agents Conduct More Raids in LA, C...</td>\\n      <td>https://www.nytimes.com/2025/06/07/us/la-immig...</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2025-06-07-22-15-19 +0000</td>\\n      <td>nyt</td>\\n      <td>Trump Warns of Consequences for Musk if He Bac...</td>\\n      <td>https://www.nytimes.com/2025/06/07/us/politics...</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2025-06-07-22-00-28 +0000</td>\\n      <td>nyt</td>\\n      <td>WorldPride Parade Attendees Celebrate in D.C. ...</td>\\n      <td>https://www.nytimes.com/2025/06/07/us/worldpri...</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2025-06-07-21-53-39 +0000</td>\\n      <td>nyt</td>\\n      <td>Trump Targets Workplaces as Immigration Crackd...</td>\\n      <td>https://www.nytimes.com/2025/06/07/us/trump-ta...</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>2025-06-07-21-06-45 +0000</td>\\n      <td>nyt</td>\\n      <td>The world is celebrating Pride in Washington, ...</td>\\n      <td>https://www.nytimes.com/2025/06/07/us/the-worl...</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 2\n",
      "[NbConvertApp] Executing cell:\n",
      "import re\n",
      "from collections import Counter\n",
      "from datetime import datetime\n",
      "\n",
      "with open('exclude.txt') as f:\n",
      "    stop_words = set(w.strip() for w in f if w.strip())\n",
      "words = re.findall(r'[A-Za-z]+', ' '.join(latest['title']).lower())\n",
      "filtered = [w for w in words if w not in stop_words and len(w) > 1]\n",
      "counts = Counter(filtered)\n",
      "score_df = (\n",
      "    pd.DataFrame(counts.items(), columns=['word','score'])\n",
      "    .sort_values('score', ascending=False)\n",
      ")\n",
      "score_df[['score','word']].to_csv('scores.csv', index=False)\n",
      "timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\n",
      "score_df[['score','word']].to_csv(f'scores-{timestamp}.csv', index=False)\n",
      "score_df.head()\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import re\\nfrom collections import Counter\\nfrom datetime import datetime\\n\\nwith open('exclude.txt') as f:\\n    stop_words = set(w.strip() for w in f if w.strip())\\nwords = re.findall(r'[A-Za-z]+', ' '.join(latest['title']).lower())\\nfiltered = [w for w in words if w not in stop_words and len(w) > 1]\\ncounts = Counter(filtered)\\nscore_df = (\\n    pd.DataFrame(counts.items(), columns=['word','score'])\\n    .sort_values('score', ascending=False)\\n)\\nscore_df[['score','word']].to_csv('scores.csv', index=False)\\ntimestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\\nscore_df[['score','word']].to_csv(f'scores-{timestamp}.csv', index=False)\\nscore_df.head()\\n\", 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"C:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_27196\\\\2741963311.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\\n\"}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '        word  score\\n7      trump     15\\n10      musk      4\\n111     dies      4\\n90       ban      4\\n155  russian      4', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>word</th>\\n      <th>score</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>7</th>\\n      <td>trump</td>\\n      <td>15</td>\\n    </tr>\\n    <tr>\\n      <th>10</th>\\n      <td>musk</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>111</th>\\n      <td>dies</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>90</th>\\n      <td>ban</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>155</th>\\n      <td>russian</td>\\n      <td>4</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 4\n",
      "[NbConvertApp] Executing cell:\n",
      "word_scores = dict(score_df[['word','score']].values)\n",
      "latest['score'] = latest['title'].apply(\n",
      "    lambda t: sum(\n",
      "        word_scores.get(w.lower(), 0)\n",
      "        for w in re.findall(r'[A-Za-z]+', t)\n",
      "        if len(w) > 1\n",
      "    )\n",
      ")\n",
      "ranked = latest.sort_values('score', ascending=False)\n",
      "ranked[['score','pubdate','source','title','link']].to_csv('rank.csv', index=False)\n",
      "ranked[['score','pubdate','source','title','link']].to_csv(f'rank-{timestamp}.csv', index=False)\n",
      "ranked.head()\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"word_scores = dict(score_df[['word','score']].values)\\nlatest['score'] = latest['title'].apply(\\n    lambda t: sum(\\n        word_scores.get(w.lower(), 0)\\n        for w in re.findall(r'[A-Za-z]+', t)\\n        if len(w) > 1\\n    )\\n)\\nranked = latest.sort_values('score', ascending=False)\\nranked[['score','pubdate','source','title','link']].to_csv('rank.csv', index=False)\\nranked[['score','pubdate','source','title','link']].to_csv(f'rank-{timestamp}.csv', index=False)\\nranked.head()\\n\", 'execution_count': 3}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '                      pubdate source  \\\\\\n37  2025-06-07-09-00-00 +0000    wsj   \\n1   2025-06-07-22-15-19 +0000    nyt   \\n26  2025-06-07-10-00-07 +0000   wapo   \\n13  2025-06-07-18-04-22 +0000    nyt   \\n10  2025-06-07-18-34-42 +0000    nyt   \\n\\n                                                title  \\\\\\n37  Elon Musk and House Republicans both promised ...   \\n1   Trump Warns of Consequences for Musk if He Bac...   \\n26  Trump travel ban targets nations mired in civi...   \\n13  Haitians Reel as Trump\u00e2\u20ac\u2122s Travel Ban Tears Fami...   \\n10  After His Trump Blowup, Musk May Be Out. But D...   \\n\\n                                                 link  score  \\n37  https://www.wsj.com/politics/policy/republican...     37  \\n1   https://www.nytimes.com/2025/06/07/us/politics...     32  \\n26  https://www.washingtonpost.com/world/2025/06/0...     30  \\n13  https://www.nytimes.com/2025/06/07/us/haiti-tr...     28  \\n10  https://www.nytimes.com/2025/06/07/us/politics...     28  ', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n      <th>score</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>37</th>\\n      <td>2025-06-07-09-00-00 +0000</td>\\n      <td>wsj</td>\\n      <td>Elon Musk and House Republicans both promised ...</td>\\n      <td>https://www.wsj.com/politics/policy/republican...</td>\\n      <td>37</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2025-06-07-22-15-19 +0000</td>\\n      <td>nyt</td>\\n      <td>Trump Warns of Consequences for Musk if He Bac...</td>\\n      <td>https://www.nytimes.com/2025/06/07/us/politics...</td>\\n      <td>32</td>\\n    </tr>\\n    <tr>\\n      <th>26</th>\\n      <td>2025-06-07-10-00-07 +0000</td>\\n      <td>wapo</td>\\n      <td>Trump travel ban targets nations mired in civi...</td>\\n      <td>https://www.washingtonpost.com/world/2025/06/0...</td>\\n      <td>30</td>\\n    </tr>\\n    <tr>\\n      <th>13</th>\\n      <td>2025-06-07-18-04-22 +0000</td>\\n      <td>nyt</td>\\n      <td>Haitians Reel as Trump\u00e2\u20ac\u2122s Travel Ban Tears Fami...</td>\\n      <td>https://www.nytimes.com/2025/06/07/us/haiti-tr...</td>\\n      <td>28</td>\\n    </tr>\\n    <tr>\\n      <th>10</th>\\n      <td>2025-06-07-18-34-42 +0000</td>\\n      <td>nyt</td>\\n      <td>After His Trump Blowup, Musk May Be Out. But D...</td>\\n      <td>https://www.nytimes.com/2025/06/07/us/politics...</td>\\n      <td>28</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 3}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 6\n",
      "[NbConvertApp] Executing cell:\n",
      "top_rows = []\n",
      "working = word_scores.copy()\n",
      "remaining = latest.copy()\n",
      "for _ in range(10):\n",
      "    ranked_loop = remaining.assign(score=remaining['title'].apply(\n",
      "        lambda t: sum(working.get(w.lower(), 0)\n",
      "                      for w in re.findall(r'[A-Za-z]+', t)\n",
      "                      if len(w) > 1)\n",
      "    )).sort_values('score', ascending=False)\n",
      "    if ranked_loop.empty:\n",
      "        break\n",
      "    top_story = ranked_loop.iloc[0]\n",
      "    top_rows.append(top_story[['score','pubdate','source','title','link']])\n",
      "    words = set(re.findall(r'[A-Za-z]+', top_story['title'].lower()))\n",
      "    for w in words:\n",
      "        working.pop(w, None)\n",
      "    remaining = remaining.drop(top_story.name)\n",
      "top_df = pd.DataFrame(top_rows)\n",
      "top_df.to_csv('top.csv', index=False)\n",
      "top_df.to_csv(f'top-{timestamp}.csv', index=False)\n",
      "top_df\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"top_rows = []\\nworking = word_scores.copy()\\nremaining = latest.copy()\\nfor _ in range(10):\\n    ranked_loop = remaining.assign(score=remaining['title'].apply(\\n        lambda t: sum(working.get(w.lower(), 0)\\n                      for w in re.findall(r'[A-Za-z]+', t)\\n                      if len(w) > 1)\\n    )).sort_values('score', ascending=False)\\n    if ranked_loop.empty:\\n        break\\n    top_story = ranked_loop.iloc[0]\\n    top_rows.append(top_story[['score','pubdate','source','title','link']])\\n    words = set(re.findall(r'[A-Za-z]+', top_story['title'].lower()))\\n    for w in words:\\n        working.pop(w, None)\\n    remaining = remaining.drop(top_story.name)\\ntop_df = pd.DataFrame(top_rows)\\ntop_df.to_csv('top.csv', index=False)\\ntop_df.to_csv(f'top-{timestamp}.csv', index=False)\\ntop_df\\n\", 'execution_count': 4}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '    score                    pubdate source  \\\\\\n37     37  2025-06-07-09-00-00 +0000    wsj   \\n26     30  2025-06-07-10-00-07 +0000   wapo   \\n20     16  2025-06-07-14-39-12 +0000    nyt   \\n23     16  2025-06-07-11-45-25 +0000    nyt   \\n18     15  2025-06-07-15-10-19 +0000    nyt   \\n46     14  2025-06-07-02-21-40 +0000   wapo   \\n22     12  2025-06-07-12-18-44 +0000    nyt   \\n21     11  2025-06-07-14-20-18 +0000    nyt   \\n14     10  2025-06-07-17-49-00 +0000    nyt   \\n17     10  2025-06-07-17-06-17 +0000    nyt   \\n\\n                                                title  \\\\\\n37  Elon Musk and House Republicans both promised ...   \\n26  Trump travel ban targets nations mired in civi...   \\n20  Kilmar Abrego Garcia Returned to U.S. From El ...   \\n23  Russian Spies Are Suspicious of China, Even as...   \\n18  Agents Use Military-Style Force Against Protes...   \\n46  Boston Consulting Group CEO apologizes for Isr...   \\n22  D.C. Police Officer Sentenced to Prison for Le...   \\n21  Paul Durcan, Irish Poet of Tortured and Tender...   \\n14  Trump\u00e2\u20ac\u2122s Proposed Cut Would Deal Serious Setbac...   \\n17  Israel Says It Made Record Weapons Sales Abroa...   \\n\\n                                                 link  \\n37  https://www.wsj.com/politics/policy/republican...  \\n26  https://www.washingtonpost.com/world/2025/06/0...  \\n20  https://www.nytimes.com/2025/06/06/us/politics...  \\n23  https://www.nytimes.com/2025/06/07/world/europ...  \\n18  https://www.nytimes.com/2025/06/06/us/los-ange...  \\n46  https://www.washingtonpost.com/national-securi...  \\n22  https://www.nytimes.com/2025/06/07/us/dc-polic...  \\n21  https://www.nytimes.com/2025/06/06/books/paul-...  \\n14  https://www.nytimes.com/2025/06/07/us/high-spe...  \\n17  https://www.nytimes.com/2025/06/04/world/middl...  ', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>score</th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>37</th>\\n      <td>37</td>\\n      <td>2025-06-07-09-00-00 +0000</td>\\n      <td>wsj</td>\\n      <td>Elon Musk and House Republicans both promised ...</td>\\n      <td>https://www.wsj.com/politics/policy/republican...</td>\\n    </tr>\\n    <tr>\\n      <th>26</th>\\n      <td>30</td>\\n      <td>2025-06-07-10-00-07 +0000</td>\\n      <td>wapo</td>\\n      <td>Trump travel ban targets nations mired in civi...</td>\\n      <td>https://www.washingtonpost.com/world/2025/06/0...</td>\\n    </tr>\\n    <tr>\\n      <th>20</th>\\n      <td>16</td>\\n      <td>2025-06-07-14-39-12 +0000</td>\\n      <td>nyt</td>\\n      <td>Kilmar Abrego Garcia Returned to U.S. From El ...</td>\\n      <td>https://www.nytimes.com/2025/06/06/us/politics...</td>\\n    </tr>\\n    <tr>\\n      <th>23</th>\\n      <td>16</td>\\n      <td>2025-06-07-11-45-25 +0000</td>\\n      <td>nyt</td>\\n      <td>Russian Spies Are Suspicious of China, Even as...</td>\\n      <td>https://www.nytimes.com/2025/06/07/world/europ...</td>\\n    </tr>\\n    <tr>\\n      <th>18</th>\\n      <td>15</td>\\n      <td>2025-06-07-15-10-19 +0000</td>\\n      <td>nyt</td>\\n      <td>Agents Use Military-Style Force Against Protes...</td>\\n      <td>https://www.nytimes.com/2025/06/06/us/los-ange...</td>\\n    </tr>\\n    <tr>\\n      <th>46</th>\\n      <td>14</td>\\n      <td>2025-06-07-02-21-40 +0000</td>\\n      <td>wapo</td>\\n      <td>Boston Consulting Group CEO apologizes for Isr...</td>\\n      <td>https://www.washingtonpost.com/national-securi...</td>\\n    </tr>\\n    <tr>\\n      <th>22</th>\\n      <td>12</td>\\n      <td>2025-06-07-12-18-44 +0000</td>\\n      <td>nyt</td>\\n      <td>D.C. Police Officer Sentenced to Prison for Le...</td>\\n      <td>https://www.nytimes.com/2025/06/07/us/dc-polic...</td>\\n    </tr>\\n    <tr>\\n      <th>21</th>\\n      <td>11</td>\\n      <td>2025-06-07-14-20-18 +0000</td>\\n      <td>nyt</td>\\n      <td>Paul Durcan, Irish Poet of Tortured and Tender...</td>\\n      <td>https://www.nytimes.com/2025/06/06/books/paul-...</td>\\n    </tr>\\n    <tr>\\n      <th>14</th>\\n      <td>10</td>\\n      <td>2025-06-07-17-49-00 +0000</td>\\n      <td>nyt</td>\\n      <td>Trump\u00e2\u20ac\u2122s Proposed Cut Would Deal Serious Setbac...</td>\\n      <td>https://www.nytimes.com/2025/06/07/us/high-spe...</td>\\n    </tr>\\n    <tr>\\n      <th>17</th>\\n      <td>10</td>\\n      <td>2025-06-07-17-06-17 +0000</td>\\n      <td>nyt</td>\\n      <td>Israel Says It Made Record Weapons Sales Abroa...</td>\\n      <td>https://www.nytimes.com/2025/06/04/world/middl...</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 4}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 8\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x000001F26AAD2150>\n",
      "[NbConvertApp] Writing 21633 bytes to c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\news-topics\\analyze_headlines.ipynb\n",
      "\n",
      "  \u2713 success\n"
     ]
    }
   ],
   "source": [
    "# Execute outdated notebooks\n",
    "import subprocess, sys\n",
    "\n",
    "def outdated(nb, deps):\n",
    "    nb_mtime = (repo_dir / nb).stat().st_mtime\n",
    "    for dep_path, _ in deps:\n",
    "        dep_mtime = (repo_dir / dep_path).stat().st_mtime\n",
    "        if dep_mtime > nb_mtime:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def execute(nb):\n",
    "    path = repo_dir / nb\n",
    "    print(f'Running {nb} \u2026')\n",
    "    if not path.exists():\n",
    "        print(f'  \u2717 path not found: {path}')\n",
    "        return\n",
    "    cmd = [\n",
    "        sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "        '--to', 'notebook', '--inplace', '--execute',\n",
    "        '--ExecutePreprocessor.timeout=600', '--debug', str(path)\n",
    "    ]\n",
    "    print('  Command:', ' '.join(cmd))\n",
    "    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    print(proc.stdout)\n",
    "    if proc.returncode == 0:\n",
    "        print('  \u2713 success')\n",
    "    else:\n",
    "        print(f'  \u2717 failed with exit code {proc.returncode}')\n",
    "\n",
    "for nb, _ in only_data:\n",
    "    if outdated(nb, dep_map[nb]['deps']):\n",
    "        execute(nb)\n",
    "\n",
    "for nb, _ in other:\n",
    "    if outdated(nb, dep_map[nb]['deps']):\n",
    "        execute(nb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
