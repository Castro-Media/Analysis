{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime, timezone\n",
    "from email.utils import parsedate_to_datetime\n",
    "import shutil\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "if (BASE_DIR / 'data').exists():\n",
    "    REPO_DIR = BASE_DIR\n",
    "else:\n",
    "    REPO_DIR = BASE_DIR.parent.parent\n",
    "DATA_DIR = REPO_DIR / 'data'\n",
    "HEADLINES_DIR = REPO_DIR / 'analysis/headlines'\n",
    "HEADLINES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def parse_pubdate(date_str):\n",
    "    try:\n",
    "        dt = parsedate_to_datetime(date_str) if date_str else None\n",
    "        if dt is None:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone.utc)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def format_pubdate(dt):\n",
    "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''\n",
    "\n",
    "def parse_feed(path: Path):\n",
    "    entries = []\n",
    "    if path.suffix == '.json':\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        for item in data.get('entries', []):\n",
    "            title = item.get('title')\n",
    "            link = item.get('link')\n",
    "            pub = parse_pubdate(item.get('published'))\n",
    "            if title and link:\n",
    "                entries.append((pub, title.strip(), link.strip()))\n",
    "    else:\n",
    "        try:\n",
    "            tree = ET.parse(path)\n",
    "            root = tree.getroot()\n",
    "        except ET.ParseError:\n",
    "            return entries\n",
    "        for item in root.iter():\n",
    "            if item.tag.lower().endswith(('item', 'entry')):\n",
    "                title = None\n",
    "                link = None\n",
    "                pub = None\n",
    "                for child in item:\n",
    "                    tag = child.tag.lower()\n",
    "                    if tag.endswith('title'):\n",
    "                        title = (child.text or '').strip()\n",
    "                    if tag.endswith('link'):\n",
    "                        link = (child.text or '').strip() or child.attrib.get('href')\n",
    "                    if tag.endswith(('pubdate', 'published', 'updated')):\n",
    "                        pub = parse_pubdate((child.text or '').strip())\n",
    "                if title and link:\n",
    "                    entries.append((pub, title, link))\n",
    "    return entries\n",
    "\n",
    "def collect_headlines():\n",
    "    all_entries = []\n",
    "    for source in DATA_DIR.iterdir():\n",
    "        if source.is_dir() and source.name.startswith('news'):\n",
    "            candidates = [p for p in source.rglob('latest.*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
    "            if not candidates:\n",
    "                candidates = [p for p in source.rglob('*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
    "            if not candidates:\n",
    "                continue\n",
    "            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
    "            source_name = latest_file.relative_to(DATA_DIR).parts[1]\n",
    "            for pub, title, link in parse_feed(latest_file):\n",
    "                all_entries.append((pub, title, link, source_name))\n",
    "    return all_entries\n",
    "\n",
    "def _date_key(date_str):\n",
    "    try:\n",
    "        return parsedate_to_datetime(date_str) if date_str else datetime.min\n",
    "    except Exception:\n",
    "        return datetime.min\n",
    "\n",
    "def update_headlines():\n",
    "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\n",
    "    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\n",
    "    if hourly_file.exists():\n",
    "        print(f\"{hourly_file.name} already exists. Skipping update.\")\n",
    "        return\n",
    "    entries = collect_headlines()\n",
    "    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
    "    deduped = []\n",
    "    seen_titles = set()\n",
    "    seen_links = set()\n",
    "    for pub, title, link, src in entries:\n",
    "        t_key = title.lower()\n",
    "        l_key = link.lower()\n",
    "        if t_key in seen_titles or l_key in seen_links:\n",
    "            continue\n",
    "        deduped.append((pub, src, title, link))\n",
    "        seen_titles.add(t_key)\n",
    "        seen_links.add(l_key)\n",
    "    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['pubdate', 'source', 'title', 'link'])\n",
    "        for pub, src, title, link in deduped:\n",
    "            writer.writerow([format_pubdate(pub), src, title, link])\n",
    "    latest_file = HEADLINES_DIR / \"latest.csv\"\n",
    "    shutil.copy(hourly_file, latest_file)\n",
    "    print(f\"Wrote {hourly_file} and updated latest.csv\")\n",
    "\n",
    "    update_headlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
