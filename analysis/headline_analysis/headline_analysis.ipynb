{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# Headline Analysis\n",
        "\n",
        "This notebook collects headlines from all news sources and builds summary files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from email.utils import parsedate_to_datetime\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "try:\n",
        "    import yaml\n",
        "except ModuleNotFoundError:\n",
        "    yaml = None\n",
        "\n",
        "try:\n",
        "    current = Path(__file__).resolve()\n",
        "except NameError:\n",
        "    current = Path.cwd()\n",
        "REPO_DIR = current\n",
        "while not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):\n",
        "    if REPO_DIR.parent == REPO_DIR:\n",
        "        raise FileNotFoundError('Repository root not found')\n",
        "    REPO_DIR = REPO_DIR.parent\n",
        "\n",
        "NEWS_DIR = REPO_DIR / 'data/news'\n",
        "PROJECT_DIR = REPO_DIR / 'analysis/headline_analysis'\n",
        "ARCHIVE_DIR = PROJECT_DIR / 'archive'\n",
        "PROJECT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ARCHIVE_DIR.mkdir(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "helpers",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "These utilities handle JSON and YAML files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "helpers-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_json(obj, path: Path) -> None:\n",
        "    \"\"\"Write an object as JSON.\"\"\"\n",
        "    text = json.dumps(obj, indent=2, ensure_ascii=False)\n",
        "    path.write_text(text + \"\\n\", encoding='utf-8')\n",
        "\n",
        "\n",
        "def archive_json(obj, archive_dir: Path, stem: str) -> None:\n",
        "    \"\"\"Save an archived JSON file.\"\"\"\n",
        "    archive_dir.mkdir(exist_ok=True)\n",
        "    tag = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
        "    save_json(obj, archive_dir / f'{stem}-{tag}.json')\n",
        "\n",
        "\n",
        "def load_front_matter(path: Path) -> dict:\n",
        "    \"\"\"Return YAML front matter for a markdown file.\"\"\"\n",
        "    text = path.read_text(encoding='utf-8')\n",
        "    match = re.search(r'^---\\n(.*?)\\n---', text, re.S)\n",
        "    if not match:\n",
        "        return {}\n",
        "    front = match.group(1)\n",
        "    if yaml:\n",
        "        return yaml.safe_load(front)\n",
        "    meta = {}\n",
        "    for line in front.splitlines():\n",
        "        if ':' in line:\n",
        "            key, val = line.split(':', 1)\n",
        "            meta[key.strip()] = val.strip()\n",
        "    return meta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "collect-sources-md",
      "metadata": {},
      "source": [
        "## Source Collection\n",
        "Find all news source directories and read their metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "collect-sources-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_sources(news_dir: Path, out_dir: Path) -> list:\n",
        "    \"\"\"Collect news sources.\"\"\"\n",
        "    sources = []\n",
        "    for index_path in news_dir.rglob('index.md'):\n",
        "        meta = load_front_matter(index_path)\n",
        "        title = meta.get('title')\n",
        "        category = meta.get('category')\n",
        "        src = meta.get('source')\n",
        "        if title and category and src:\n",
        "            sources.append({\n",
        "                'title': str(title),\n",
        "                'category': str(category),\n",
        "                'source': str(src),\n",
        "                'path': str(index_path.parent)\n",
        "            })\n",
        "    save_json(sources, out_dir / 'sources.json')\n",
        "    archive_json(sources, out_dir / 'archive', 'sources')\n",
        "    return sources"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "parsers-md",
      "metadata": {},
      "source": [
        "## Feed Parsers\n",
        "Functions that convert feed files into headline records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "parsers-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_pubdate(date_str: str) -> datetime | None:",
        "",
        "    \"\"\"Parse a date string into UTC.\"\"\"",
        "",
        "    try:",
        "",
        "        dt = parsedate_to_datetime(date_str) if date_str else None",
        "",
        "        if dt is None:",
        "",
        "            return None",
        "",
        "        if dt.tzinfo is None:",
        "",
        "            dt = dt.replace(tzinfo=timezone.utc)",
        "",
        "        return dt.astimezone(timezone.utc)",
        "",
        "    except Exception:",
        "",
        "        return None",
        "",
        "",
        "",
        "",
        "",
        "def format_pubdate(dt: datetime | None) -> str:",
        "",
        "    \"\"\"Format a datetime for output.\"\"\"",
        "",
        "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''",
        "",
        "",
        "",
        "",
        "",
        "def parse_json_feed(path: Path) -> list:",
        "    \"\"\"Parse a JSON feed file.\"\"\"",
        "    items = []",
        "    text = path.read_text(encoding='utf-8').strip()",
        "    if not text:",
        "        return items",
        "    try:",
        "        data = json.loads(text)",
        "    except json.JSONDecodeError:",
        "        return items",
        "    for item in data.get('entries', []):",
        "        title = item.get('title')",
        "        link = item.get('link')",
        "        pub = parse_pubdate(item.get('published'))",
        "        if title and link:",
        "            items.append(",
        "                {",
        "                    'title': title.strip(),",
        "                    'link': link.strip(),",
        "                    'pubdate': pub,",
        "                }",
        "            )",
        "    return items",
        "",
        "",
        "",
        "",
        "",
        "def parse_xml_feed(path: Path) -> list:",
        "",
        "    \"\"\"Parse an XML feed file.\"\"\"",
        "",
        "    items = []",
        "",
        "    try:",
        "",
        "        root = ET.parse(path).getroot()",
        "",
        "    except ET.ParseError:",
        "",
        "        return items",
        "",
        "    for item in root.iter():",
        "",
        "        if item.tag.lower().endswith(('item', 'entry')):",
        "",
        "            title = None",
        "",
        "            link = None",
        "",
        "            pub = None",
        "",
        "            for child in item:",
        "",
        "                tag = child.tag.lower()",
        "",
        "                if tag.endswith('title'):",
        "",
        "                    title = (child.text or '').strip()",
        "",
        "                if tag.endswith('link'):",
        "",
        "                    link = (child.text or '').strip() or child.attrib.get('href')",
        "",
        "                if tag.endswith(('pubdate', 'published', 'updated')):",
        "",
        "                    pub = parse_pubdate((child.text or '').strip())",
        "",
        "            if title and link:",
        "",
        "                items.append({'title': title, 'link': link, 'pubdate': pub})",
        "",
        "    return items",
        "",
        "",
        "",
        "",
        "",
        "def parse_feed(path: Path) -> list:",
        "",
        "    \"\"\"Parse a feed file of any supported type.\"\"\"",
        "",
        "    return parse_json_feed(path) if path.suffix == '.json' else parse_xml_feed(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "headlines-md",
      "metadata": {},
      "source": [
        "## Headline Collection\n",
        "Gather headlines from each source and filter by time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "headlines-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_headlines(sources: list, out_dir: Path) -> list:\n",
        "    \"\"\"Collect headlines from all sources.\"\"\"\n",
        "    headlines = []\n",
        "    for src in sources:\n",
        "        dir_path = Path(src['path'])\n",
        "        latest = None\n",
        "        for ext in ('.json', '.rss', '.xml'):\n",
        "            fp = dir_path / f'latest{ext}'\n",
        "            if fp.exists():\n",
        "                latest = fp\n",
        "                break\n",
        "        if not latest:\n",
        "            continue\n",
        "        for item in parse_feed(latest):\n",
        "            headlines.append({\n",
        "                'headline': item['title'],\n",
        "                'link': item['link'],\n",
        "                'source': src['source'],\n",
        "                'pubdate': item['pubdate']\n",
        "            })\n",
        "    min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
        "    headlines.sort(key=lambda r: r['pubdate'] or min_time, reverse=True)\n",
        "    serial = [{**h, 'pubdate': format_pubdate(h['pubdate'])} for h in headlines]\n",
        "    save_json(serial, out_dir / 'headlines.json')\n",
        "    archive_json(serial, out_dir / 'archive', 'headlines')\n",
        "    return headlines\n",
        "\n",
        "\n",
        "def filter_headlines(headlines: list, delta: timedelta, name: str, out_dir: Path) -> list:\n",
        "    \"\"\"Filter headlines newer than cutoff.\"\"\"\n",
        "    cutoff = datetime.now(timezone.utc) - delta\n",
        "    subset = [h for h in headlines if h['pubdate'] and h['pubdate'] >= cutoff]\n",
        "    serial = [{**h, 'pubdate': format_pubdate(h['pubdate'])} for h in subset]\n",
        "    save_json(serial, out_dir / f'{name}.json')\n",
        "    archive_json(serial, out_dir / 'archive', name)\n",
        "    return subset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary-md",
      "metadata": {},
      "source": [
        "## Summary Building\n",
        "Create a CSV with headline counts for each source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "summary-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def archive_csv(rows: list, archive_dir: Path, stem: str) -> None:\n",
        "    \"\"\"Archive a CSV file.\"\"\"\n",
        "    archive_dir.mkdir(exist_ok=True)\n",
        "    tag = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
        "    path = archive_dir / f'{stem}-{tag}.csv'\n",
        "    with open(path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['source', '1h', '24h', '7d'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "\n",
        "def build_summary(headlines: list, out_dir: Path) -> None:\n",
        "    \"\"\"Create summary counts for each source.\"\"\"\n",
        "    counts = {}\n",
        "    now = datetime.now(timezone.utc)\n",
        "    for h in headlines:\n",
        "        src = h['source']\n",
        "        dt = h['pubdate']\n",
        "        if not dt:\n",
        "            continue\n",
        "        counts.setdefault(src, [0, 0, 0])\n",
        "        if dt >= now - timedelta(hours=1):\n",
        "            counts[src][0] += 1\n",
        "        if dt >= now - timedelta(hours=24):\n",
        "            counts[src][1] += 1\n",
        "        if dt >= now - timedelta(days=7):\n",
        "            counts[src][2] += 1\n",
        "    rows = [{'source': s, '1h': c[0], '24h': c[1], '7d': c[2]} for s, c in counts.items()]\n",
        "    rows.sort(key=lambda r: (-r['1h'], -r['24h'], -r['7d'], r['source']))\n",
        "    out_path = out_dir / 'summary.csv'\n",
        "    with open(out_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['source', '1h', '24h', '7d'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "    archive_csv(rows, out_dir / 'archive', 'summary')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run-md",
      "metadata": {},
      "source": [
        "## Run Everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "run-code",
      "metadata": {},
      "outputs": [
        {
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     15\u001b[39m         headlines.append({\n\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     20\u001b[39m         })\n\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m items = []\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    353\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
            "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ],
      "source": [
        "def main() -> None:\n",
        "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
        "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
        "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
        "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
        "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
        "    build_summary(headlines, PROJECT_DIR)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
