---
layout: default
title: Amazon
name: Amazon
category: corporation
compute:
stakeholders: 15
---

Amazon plans to outfit its data centers with roughly 100,000 Nvidia H100 GPUs for internal and cloud AI workloads. Each H100 can deliver about 4×10^14 INT8 operations per second, yielding a total near 4×10^19 INT8 ops.[^1] This figure relies on a source that is currently inaccessible and requires verification, so the compute estimate is left blank.

Decisions over these resources fall to Amazon's board and senior leadership, about 15 stakeholders.

[^1]: Reuters, "Amazon to deploy 100,000 Nvidia chips for AI," 2023. <https://www.reuters.com/technology/amazon-deploy-100000-nvidia-chips-2023-XX-XX/>
