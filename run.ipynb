{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00052047",
   "metadata": {},
   "source": [
    "# Run Analysis\n",
    "\n",
    "Data sources are stored as arbitrarily deep directories in /data. A data source is a directory with an index file which contains front matter such as filetype, cadence, etc related to how to access the data and when/whether to update the data automatically.  \n",
    "\n",
    "Some data sources have a static cadence, meaning they wont be automatically updated by this notebook.  \n",
    "\n",
    "Others specify an update cadence, a time last updated, and a method of updating which may include things like an api key, etc.  \n",
    "\n",
    "An analysis, likewise will be an arbitrarily deep sirectory wihtin /analysis which contains an index file with front matter like title and dependencies.  \n",
    "\n",
    "Dependencies are paths to data sources or other analyses. Whenever an analysis was last modified before any one of its dependencies, the analysis is stale and needs to be run again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c687cb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T08:55:08.306451Z",
     "iopub.status.busy": "2025-10-23T08:55:08.306200Z",
     "iopub.status.idle": "2025-10-23T08:55:24.438758Z",
     "shell.execute_reply": "2025-10-23T08:55:24.438029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.4-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m211.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.4-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/16.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m251.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pytz, numpy, pandas\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pandas]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2K"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed numpy-2.3.4 pandas-2.3.3 pytz-2025.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=f9b61c229bbc197bf39aae7e5fc3d9096d37d9875f213899ee83f72ce725ba7f\n",
      "  Stored in directory: /home/runner/.cache/pip/wheels/3d/4d/ef/37cdccc18d6fd7e0dd7817dcdf9146d4d6789c32a227a28134\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [feedparser]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed feedparser-6.0.12 sgmllib3k-1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk>=3.9->textblob)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk>=3.9->textblob)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex>=2021.8.3 (from nltk>=3.9->textblob)\n",
      "  Downloading regex-2025.10.23-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk>=3.9->textblob)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/624.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m162.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.10.23-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.4/803.4 kB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tqdm, regex, joblib, click, nltk, textblob\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [joblib]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [joblib]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [textblob]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.0 joblib-1.5.2 nltk-3.9.2 regex-2025.10.23 textblob-0.19.0 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n"
     ]
    }
   ],
   "source": [
    "#### Make sure all required packages are installed and imported\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: Optional[str] = None, required: bool = True):\n",
    "    \"\"\"Import a package, installing it if missing.\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "        except Exception:\n",
    "            if required:\n",
    "                raise\n",
    "    mod = importlib.import_module(import_name or pkg_name)\n",
    "    globals()[import_name or pkg_name] = mod\n",
    "    return mod\n",
    "\n",
    "_ensure('pandas')\n",
    "_ensure('requests')\n",
    "_ensure('feedparser')\n",
    "_ensure('textblob')\n",
    "_ensure('pyyaml', 'yaml')\n",
    "_ensure('jupyter', required=False)\n",
    "_ensure('nbconvert', required=False)\n",
    "print('All dependencies ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00445370",
   "metadata": {},
   "source": [
    "## Update Data Sources\n",
    "\n",
    "This cell needs to find all of the /data index files, check whether that data source needs to be updated, and then do whatever updates are appropriate.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcda8f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T08:55:24.441071Z",
     "iopub.status.busy": "2025-10-23T08:55:24.440790Z",
     "iopub.status.idle": "2025-10-23T08:56:52.907134Z",
     "shell.execute_reply": "2025-10-23T08:56:52.906026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://www.washingtonpost.com/arcio/rss/category/politics/: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: news/world/nypost/news-world-nypost, news/world/toi/news-world-toi, news/world/dw/news-world-dw, news/world/wsj/news-world-wsj, news/world/wapo/news-world-wapo, news/world/nyt/news-world-nyt, news/world/chitri/news-world-chi-tribune, news/world/bbc/news-world-bbc, news/world/cbc/news-world-cbc, news/us/nypost/news-us-nypost, news/us/missionlocal/news-us-missionlocal, news/us/latimes/latimes-us, news/us/toi/news-us-toi, news/us/wsj/news-us-wsj, news/us/wapo/news-us-wapo, news/us/eltecolote/news-us-eltecolote, news/us/nyt/news-us-nyt, news/us/48hills/news-us-48hills, news/us/bbc/news-us-bbc, news/americas/nyt/news-americas-nyt, news/business/nypost/news-business-nypost, news/business/latimes/latimes-business, news/business/toi/news-business-toi, news/business/dw/news-business-dw, news/business/startribune/news-us-business-startribune, news/business/wsj/news-business-wsj, news/business/wsj/news-markets-wsj, news/business/wapo/news-business-wapo, news/business/nyt/news-business-nyt, news/business/chitri/news-business-chi-tribune, news/business/bbc/news-business-bbc, news/middle-east/toi/news-middle-east-toi, news/middle-east/nyt/news-middle-east-nyt, news/middle-east/bbc/news-middle-east-bbc, news/africa/nyt/news-africa-nyt, news/africa/bbc/news-africa-bbc, news/economy/wsj/news-economy-wsj, news/economy/nyt/news-economy-nyt, news/latin-america/bbc/news-latin-america-bbc, news/top/dw/news-top-dw, news/europe/toi/news-europe-toi, news/europe/dw/news-europe-dw, news/europe/nyt/news-europe-nyt, news/europe/bbc/news-europe-bbc, news/politics/nypost/news-us-politics-nypost, news/politics/latimes/latimes-us-politics, news/politics/wsj/news-us-politics-wsj, news/politics/wapo/news-us-politics-wapo, news/politics/nyt/news-us-politics-nyt, news/politics/chitri/news-us-politics-chi-tribune, news/politics/bbc/news-politics-bbc, news/politics/cbc/news-politics-cbc, news/asia/dw/news-asia-dw, news/asia/nyt/news-asia-nyt, news/asia/bbc/news-asia-bbc\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "import feedparser\n",
    "import textblob\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse\n",
    "import yaml\n",
    "from typing import Optional\n",
    "\n",
    "CADENCE_SECONDS = {\n",
    "    'hourly': 3600,\n",
    "    'daily': 86400,\n",
    "    'weekly': 604800,\n",
    "    'monthly': 2592000,\n",
    "    'quarterly': 7776000,\n",
    "}\n",
    "\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    \"\"\"Replace [date %Y-%m-%d] tokens with today's date.\"\"\"\n",
    "    def _replace(m):\n",
    "        return dt.date.today().strftime(m.group(1).strip())\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "def add_apikey(url: str, env_var: Optional[str]) -> str:\n",
    "    \"\"\"Append an API key from the environment if available.\"\"\"\n",
    "    if env_var:\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f\"{url}{sep}api_key={urllib.parse.quote_plus(key)}\"\n",
    "    return url\n",
    "\n",
    "def _parse_meta(path: Path):\n",
    "    \"\"\"Return metadata and body text from a markdown file.\"\"\"\n",
    "    text = path.read_text()\n",
    "    m = re.search(r'^---\\n(.*?)\\n---\\n?', text, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            meta = yaml.safe_load(m.group(1)) or {}\n",
    "        except Exception as e:\n",
    "            print(f'Error parsing metadata in {path}:', e)\n",
    "            raise\n",
    "        body = text[m.end():]\n",
    "    else:\n",
    "        meta, body = {}, text\n",
    "    return meta, body\n",
    "\n",
    "def _write_meta(path: Path, meta: dict, body: str):\n",
    "    \"\"\"Write metadata and body back to a markdown file.\"\"\"\n",
    "    header = yaml.safe_dump(meta, sort_keys=False).strip()\n",
    "    path.write_text(f\"---\\n{header}\\n---\\n{body}\")\n",
    "\n",
    "def _read_index(idx_file: Path):\n",
    "    meta, body = _parse_meta(idx_file)\n",
    "    url = str(meta.get('url', '')).strip()\n",
    "    if not url or url.lower() in ('n/a', 'na', 'none'):\n",
    "        return None\n",
    "    filetype = str(meta.get('filetype', '')).strip().lstrip('.')\n",
    "    cadence = str(meta.get('cadence', 'monthly')).lower()\n",
    "    api_key = meta.get('api_key')\n",
    "    last_fetch = (\n",
    "        pd.to_datetime(meta.get('last_fetched'))\n",
    "        if meta.get('last_fetched') else None\n",
    "    )\n",
    "    return meta, body, url, filetype, cadence, api_key, last_fetch\n",
    "\n",
    "def _build_paths(idx_file: Path, filetype: str, cadence: str,\n",
    "                 now: dt.datetime, today: dt.date):\n",
    "    folder = idx_file.parent\n",
    "    output_ext = 'json' if filetype in ('rss', 'xml') else filetype\n",
    "    latest_fp = folder / f'latest.{output_ext}'\n",
    "    name = f\"{now:%Y-%m-%d-%H}\" if cadence == 'hourly' else f\"{today:%Y-%m-%d}\"\n",
    "    dated_fp = folder / f'{name}.{output_ext}'\n",
    "    return folder, latest_fp, dated_fp\n",
    "\n",
    "def _should_skip(latest_fp: Path, dated_fp: Path,\n",
    "                 last_fetch, now: dt.datetime, min_age: int) -> bool:\n",
    "    if latest_fp.exists() and last_fetch and (now - last_fetch).total_seconds() < min_age:\n",
    "        if dated_fp.exists() and latest_fp.read_bytes() != dated_fp.read_bytes():\n",
    "            shutil.copyfile(dated_fp, latest_fp)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _fetch_content(req_url: str, filetype: str) -> bytes:\n",
    "    \"\"\"Return bytes from a URL or an empty result on error.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(\n",
    "            req_url,\n",
    "            timeout=30,\n",
    "            headers={'User-Agent': 'Mozilla/5.0'},\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException as exc:\n",
    "        print(f\"Request failed for {req_url}: {exc}\")\n",
    "        return b''\n",
    "    if filetype in ('rss', 'xml'):\n",
    "        feed = feedparser.parse(resp.content)\n",
    "        entries = []\n",
    "        for entry in feed.entries:\n",
    "            text = ' '.join(\n",
    "                filter(None, [entry.get('title'), entry.get('summary')])\n",
    "            )\n",
    "            pol = textblob.TextBlob(text).sentiment.polarity\n",
    "            entries.append(\n",
    "                {\n",
    "                    'title': entry.get('title'),\n",
    "                    'link': entry.get('link'),\n",
    "                    'published': entry.get('published'),\n",
    "                    'sentiment': pol,\n",
    "                }\n",
    "            )\n",
    "        json_bytes = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "        return json_bytes\n",
    "    return resp.content\n",
    "\n",
    "def _write_files(content: bytes, dated_fp: Path, latest_fp: Path):\n",
    "    dated_fp.write_bytes(content)\n",
    "    shutil.copyfile(dated_fp, latest_fp)\n",
    "\n",
    "def _process_index(idx_file: Path, base: Path,\n",
    "                   now: dt.datetime, today: dt.date) -> str | None:\n",
    "    result = _read_index(idx_file)\n",
    "    if not result:\n",
    "        return None\n",
    "    meta, body, url, filetype, cadence, api_key, last_fetch = result\n",
    "    min_age = CADENCE_SECONDS.get(cadence, 2592000)\n",
    "    folder, latest_fp, dated_fp = _build_paths(idx_file, filetype, cadence, now, today)\n",
    "    if _should_skip(latest_fp, dated_fp, last_fetch, now, min_age):\n",
    "        return None\n",
    "    req_url = add_apikey(substitute_date_tokens(url), api_key)\n",
    "    content = _fetch_content(req_url, filetype)\n",
    "    _write_files(content, dated_fp, latest_fp)\n",
    "    meta['last_fetched'] = now.isoformat(timespec='minutes')\n",
    "    _write_meta(idx_file, meta, body)\n",
    "    return str(folder.relative_to(base))\n",
    "\n",
    "def updateData(path: str):\n",
    "    \"\"\"Refresh any outdated data sources.\"\"\"\n",
    "    base = Path(path)\n",
    "    now = dt.datetime.now()\n",
    "    today = now.date()\n",
    "    updated = []\n",
    "    for idx_file in base.rglob('index.md'):\n",
    "        changed = _process_index(idx_file, base, now, today)\n",
    "        if changed:\n",
    "            updated.append(changed)\n",
    "    if updated:\n",
    "        print('Updated:', ', '.join(updated))\n",
    "    else:\n",
    "        print('Everything up to date.')\n",
    "\n",
    "updateData('./data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc361023",
   "metadata": {},
   "source": [
    "## Update Analyses\n",
    "\n",
    "This cell needs to do the same type of scan across all the analyses in /analysis. It needs to iterate across all the analyses and check the time last modified for all the dependencies. If any dependency was modified more recently than the analysis, then the analysis needs to be run again. The time last modified of the analysis is the most recent file modification time in the analysis directory, because the analysis directory will contain some arbitrary number of output files.  \n",
    "\n",
    "Because some analyses will list other analyses as dependencies, this loop of checking across all of the analyses needs to keep running until none of them have anything to do, up to some reasonable limit of times to prevent arbitrary recursion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b77f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T08:56:52.908924Z",
     "iopub.status.busy": "2025-10-23T08:56:52.908745Z",
     "iopub.status.idle": "2025-10-23T08:57:10.521431Z",
     "shell.execute_reply": "2025-10-23T08:57:10.520484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 17676 bytes to /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 10579 bytes to /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 24392 bytes to /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything up to date.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import yaml\n",
    "from typing import List\n",
    "\n",
    "def _parse_meta(path: Path):\n",
    "    \"\"\"Return metadata and body from a markdown file.\"\"\"\n",
    "    text = path.read_text()\n",
    "    m = re.search(r'^---\\n(.*?)\\n---\\n?', text, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            meta = yaml.safe_load(m.group(1)) or {}\n",
    "        except Exception as e:\n",
    "            print(f'Error parsing metadata in {path}:', e)\n",
    "            raise\n",
    "        body = text[m.end():]\n",
    "    else:\n",
    "        meta, body = {}, text\n",
    "    return meta, body\n",
    "\n",
    "def _latest_mtime(p: Path) -> float:\n",
    "    \"\"\"Return the most recent modification time in a path.\"\"\"\n",
    "    if p.is_file():\n",
    "        return p.stat().st_mtime\n",
    "    mt = [f.stat().st_mtime for f in p.rglob('*') if f.is_file()]\n",
    "    return max(mt) if mt else p.stat().st_mtime\n",
    "\n",
    "def build_dep_map(analysis_dir: Path, repo_dir: Path):\n",
    "    \"\"\"Map notebooks to their dependency paths.\"\"\"\n",
    "    dep_map = {}\n",
    "    for meta in analysis_dir.rglob('index.md'):\n",
    "        info, _ = _parse_meta(meta)\n",
    "        deps = [(repo_dir / d).resolve() for d in info.get('dependencies', [])]\n",
    "        for nb in meta.parent.glob('*.ipynb'):\n",
    "            dep_map[nb] = deps\n",
    "    return dep_map\n",
    "\n",
    "def outdated(nb: Path, deps: List[Path]) -> bool:\n",
    "    \"\"\"Return True if any dependency is newer than the notebook.\"\"\"\n",
    "    nb_m = _latest_mtime(nb)\n",
    "    return any(_latest_mtime(d) > nb_m for d in deps)\n",
    "\n",
    "def execute(nb: Path):\n",
    "    \"\"\"Run a notebook in-place if jupyter is available.\"\"\"\n",
    "    import shutil\n",
    "    if not shutil.which('jupyter'):\n",
    "        print('jupyter not available - skipping', nb)\n",
    "        return\n",
    "    cmd = [\n",
    "        sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "        '--to', 'notebook', '--inplace', '--execute',\n",
    "        '--ExecutePreprocessor.timeout=600', str(nb),\n",
    "    ]\n",
    "    subprocess.run(cmd, check=False)\n",
    "\n",
    "def updateAnalyses(path: str):\n",
    "    \"\"\"Execute notebooks whose dependencies have changed.\"\"\"\n",
    "    repo_dir = Path('.').resolve()\n",
    "    analysis_dir = repo_dir / path\n",
    "    for _ in range(10):\n",
    "        dep_map = build_dep_map(analysis_dir, repo_dir)\n",
    "        outdated_nbs = [\n",
    "            nb for nb, deps in dep_map.items() if deps and outdated(nb, deps)\n",
    "        ]\n",
    "        json.dump(\n",
    "            {'outdated_notebooks': [str(nb) for nb in outdated_nbs]},\n",
    "            open('dependencies.json', 'w'),\n",
    "            indent=2,\n",
    "        )\n",
    "        if not outdated_nbs:\n",
    "            print('Everything up to date.')\n",
    "            break\n",
    "        for nb in outdated_nbs:\n",
    "            execute(nb)\n",
    "\n",
    "updateAnalyses('./analysis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
