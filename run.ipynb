{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00052047",
   "metadata": {},
   "source": [
    "# Run Analysis\n",
    "\n",
    "Data sources are stored as arbitrarily deep directories in /data. A data source is a directory with an index file which contains front matter such as filetype, cadence, etc related to how to access the data and when/whether to update the data automatically.  \n",
    "\n",
    "Some data sources have a static cadence, meaning they wont be automatically updated by this notebook.  \n",
    "\n",
    "Others specify an update cadence, a time last updated, and a method of updating which may include things like an api key, etc.  \n",
    "\n",
    "An analysis, likewise will be an arbitrarily deep sirectory wihtin /analysis which contains an index file with front matter like title and dependencies.  \n",
    "\n",
    "Dependencies are paths to data sources or other analyses. Whenever an analysis was last modified before any one of its dependencies, the analysis is stale and needs to be run again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c687cb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T19:51:38.249571Z",
     "iopub.status.busy": "2025-07-23T19:51:38.249364Z",
     "iopub.status.idle": "2025-07-23T19:51:53.979918Z",
     "shell.execute_reply": "2025-07-23T19:51:53.979350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m10.5/12.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/16.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [tzdata]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.3.1 pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=5d30f17f1c3cbccefb47f76f63ce0c6795c1d1ed10234efd64f6d37a866b398e\n",
      "  Stored in directory: /home/runner/.cache/pip/wheels/3d/4d/ef/37cdccc18d6fd7e0dd7817dcdf9146d4d6789c32a227a28134\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [feedparser]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk>=3.9->textblob)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk>=3.9->textblob)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex>=2021.8.3 (from nltk>=3.9->textblob)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk>=3.9->textblob)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/624.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/796.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tqdm, regex, joblib, click, nltk, textblob\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [joblib]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [joblib]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [textblob]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 textblob-0.19.0 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n"
     ]
    }
   ],
   "source": [
    "#### Make sure all required packages are installed and imported\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: Optional[str] = None, required: bool = True):\n",
    "    \"\"\"Import a package, installing it if missing.\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "        except Exception:\n",
    "            if required:\n",
    "                raise\n",
    "    mod = importlib.import_module(import_name or pkg_name)\n",
    "    globals()[import_name or pkg_name] = mod\n",
    "    return mod\n",
    "\n",
    "_ensure('pandas')\n",
    "_ensure('requests')\n",
    "_ensure('feedparser')\n",
    "_ensure('textblob')\n",
    "_ensure('pyyaml', 'yaml')\n",
    "_ensure('jupyter', required=False)\n",
    "_ensure('nbconvert', required=False)\n",
    "print('All dependencies ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00445370",
   "metadata": {},
   "source": [
    "## Update Data Sources\n",
    "\n",
    "This cell needs to find all of the /data index files, check whether that data source needs to be updated, and then do whatever updates are appropriate.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcda8f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T19:51:53.981968Z",
     "iopub.status.busy": "2025-07-23T19:51:53.981725Z",
     "iopub.status.idle": "2025-07-23T19:51:54.097072Z",
     "shell.execute_reply": "2025-07-23T19:51:54.096547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything up to date.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "import feedparser\n",
    "import textblob\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse\n",
    "import yaml\n",
    "from typing import Optional\n",
    "\n",
    "CADENCE_SECONDS = {\n",
    "    'hourly': 3600,\n",
    "    'daily': 86400,\n",
    "    'weekly': 604800,\n",
    "    'monthly': 2592000,\n",
    "    'quarterly': 7776000,\n",
    "}\n",
    "\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    \"\"\"Replace [date %Y-%m-%d] tokens with today's date.\"\"\"\n",
    "    def _replace(m):\n",
    "        return dt.date.today().strftime(m.group(1).strip())\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "def add_apikey(url: str, env_var: Optional[str]) -> str:\n",
    "    \"\"\"Append an API key from the environment if available.\"\"\"\n",
    "    if env_var:\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f\"{url}{sep}api_key={urllib.parse.quote_plus(key)}\"\n",
    "    return url\n",
    "\n",
    "def _parse_meta(path: Path):\n",
    "    \"\"\"Return metadata and body text from a markdown file.\"\"\"\n",
    "    text = path.read_text()\n",
    "    m = re.search(r'^---\\n(.*?)\\n---\\n?', text, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            meta = yaml.safe_load(m.group(1)) or {}\n",
    "        except Exception as e:\n",
    "            print(f'Error parsing metadata in {path}:', e)\n",
    "            raise\n",
    "        body = text[m.end():]\n",
    "    else:\n",
    "        meta, body = {}, text\n",
    "    return meta, body\n",
    "\n",
    "def _write_meta(path: Path, meta: dict, body: str):\n",
    "    \"\"\"Write metadata and body back to a markdown file.\"\"\"\n",
    "    header = yaml.safe_dump(meta, sort_keys=False).strip()\n",
    "    path.write_text(f\"---\\n{header}\\n---\\n{body}\")\n",
    "\n",
    "def _read_index(idx_file: Path):\n",
    "    meta, body = _parse_meta(idx_file)\n",
    "    url = str(meta.get('url', '')).strip()\n",
    "    if not url or url.lower() in ('n/a', 'na', 'none'):\n",
    "        return None\n",
    "    filetype = str(meta.get('filetype', '')).strip().lstrip('.')\n",
    "    cadence = str(meta.get('cadence', 'monthly')).lower()\n",
    "    api_key = meta.get('api_key')\n",
    "    last_fetch = (\n",
    "        pd.to_datetime(meta.get('last_fetched'))\n",
    "        if meta.get('last_fetched') else None\n",
    "    )\n",
    "    return meta, body, url, filetype, cadence, api_key, last_fetch\n",
    "\n",
    "def _build_paths(idx_file: Path, filetype: str, cadence: str,\n",
    "                 now: dt.datetime, today: dt.date):\n",
    "    folder = idx_file.parent\n",
    "    output_ext = 'json' if filetype in ('rss', 'xml') else filetype\n",
    "    latest_fp = folder / f'latest.{output_ext}'\n",
    "    name = f\"{now:%Y-%m-%d-%H}\" if cadence == 'hourly' else f\"{today:%Y-%m-%d}\"\n",
    "    dated_fp = folder / f'{name}.{output_ext}'\n",
    "    return folder, latest_fp, dated_fp\n",
    "\n",
    "def _should_skip(latest_fp: Path, dated_fp: Path,\n",
    "                 last_fetch, now: dt.datetime, min_age: int) -> bool:\n",
    "    if latest_fp.exists() and last_fetch and (now - last_fetch).total_seconds() < min_age:\n",
    "        if dated_fp.exists() and latest_fp.read_bytes() != dated_fp.read_bytes():\n",
    "            shutil.copyfile(dated_fp, latest_fp)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _fetch_content(req_url: str, filetype: str) -> bytes:\n",
    "    \"\"\"Return bytes from a URL or an empty result on error.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(\n",
    "            req_url,\n",
    "            timeout=30,\n",
    "            headers={'User-Agent': 'Mozilla/5.0'},\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException as exc:\n",
    "        print(f\"Request failed for {req_url}: {exc}\")\n",
    "        return b''\n",
    "    if filetype in ('rss', 'xml'):\n",
    "        feed = feedparser.parse(resp.content)\n",
    "        entries = []\n",
    "        for entry in feed.entries:\n",
    "            text = ' '.join(\n",
    "                filter(None, [entry.get('title'), entry.get('summary')])\n",
    "            )\n",
    "            pol = textblob.TextBlob(text).sentiment.polarity\n",
    "            entries.append(\n",
    "                {\n",
    "                    'title': entry.get('title'),\n",
    "                    'link': entry.get('link'),\n",
    "                    'published': entry.get('published'),\n",
    "                    'sentiment': pol,\n",
    "                }\n",
    "            )\n",
    "        json_bytes = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "        return json_bytes\n",
    "    return resp.content\n",
    "\n",
    "def _write_files(content: bytes, dated_fp: Path, latest_fp: Path):\n",
    "    dated_fp.write_bytes(content)\n",
    "    shutil.copyfile(dated_fp, latest_fp)\n",
    "\n",
    "def _process_index(idx_file: Path, base: Path,\n",
    "                   now: dt.datetime, today: dt.date) -> str | None:\n",
    "    result = _read_index(idx_file)\n",
    "    if not result:\n",
    "        return None\n",
    "    meta, body, url, filetype, cadence, api_key, last_fetch = result\n",
    "    min_age = CADENCE_SECONDS.get(cadence, 2592000)\n",
    "    folder, latest_fp, dated_fp = _build_paths(idx_file, filetype, cadence, now, today)\n",
    "    if _should_skip(latest_fp, dated_fp, last_fetch, now, min_age):\n",
    "        return None\n",
    "    req_url = add_apikey(substitute_date_tokens(url), api_key)\n",
    "    content = _fetch_content(req_url, filetype)\n",
    "    _write_files(content, dated_fp, latest_fp)\n",
    "    meta['last_fetched'] = now.isoformat(timespec='minutes')\n",
    "    _write_meta(idx_file, meta, body)\n",
    "    return str(folder.relative_to(base))\n",
    "\n",
    "def updateData(path: str):\n",
    "    \"\"\"Refresh any outdated data sources.\"\"\"\n",
    "    base = Path(path)\n",
    "    now = dt.datetime.now()\n",
    "    today = now.date()\n",
    "    updated = []\n",
    "    for idx_file in base.rglob('index.md'):\n",
    "        changed = _process_index(idx_file, base, now, today)\n",
    "        if changed:\n",
    "            updated.append(changed)\n",
    "    if updated:\n",
    "        print('Updated:', ', '.join(updated))\n",
    "    else:\n",
    "        print('Everything up to date.')\n",
    "\n",
    "updateData('./data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc361023",
   "metadata": {},
   "source": [
    "## Update Analyses\n",
    "\n",
    "This cell needs to do the same type of scan across all the analyses in /analysis. It needs to iterate across all the analyses and check the time last modified for all the dependencies. If any dependency was modified more recently than the analysis, then the analysis needs to be run again. The time last modified of the analysis is the most recent file modification time in the analysis directory, because the analysis directory will contain some arbitrary number of output files.  \n",
    "\n",
    "Because some analyses will list other analyses as dependencies, this loop of checking across all of the analyses needs to keep running until none of them have anything to do, up to some reasonable limit of times to prevent arbitrary recursion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b77f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T19:51:54.098823Z",
     "iopub.status.busy": "2025-07-23T19:51:54.098650Z",
     "iopub.status.idle": "2025-07-23T19:53:06.009606Z",
     "shell.execute_reply": "2025-07-23T19:53:06.008945Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "def main() -> None:\n",
      "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
      "    build_summary(headlines, PROJECT_DIR)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n",
      "\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[32m     15\u001b[39m         headlines.append({\n",
      "\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m     20\u001b[39m         })\n",
      "\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     21\u001b[39m items = []\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n",
      "\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n",
      "\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n",
      "\u001b[32m    343\u001b[39m \n",
      "\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n",
      "\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n",
      "\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n",
      "\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "from pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\n",
      "------------------\n",
      "\n",
      "\n",
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mfrom pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\u001b[39m\n",
      "                                   ^\n",
      "\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "def main() -> None:\n",
      "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
      "    build_summary(headlines, PROJECT_DIR)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n",
      "\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[32m     15\u001b[39m         headlines.append({\n",
      "\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m     20\u001b[39m         })\n",
      "\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     21\u001b[39m items = []\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n",
      "\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n",
      "\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n",
      "\u001b[32m    343\u001b[39m \n",
      "\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n",
      "\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n",
      "\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n",
      "\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "from pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\n",
      "------------------\n",
      "\n",
      "\n",
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mfrom pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\u001b[39m\n",
      "                                   ^\n",
      "\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "def main() -> None:\n",
      "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
      "    build_summary(headlines, PROJECT_DIR)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n",
      "\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[32m     15\u001b[39m         headlines.append({\n",
      "\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m     20\u001b[39m         })\n",
      "\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     21\u001b[39m items = []\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n",
      "\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n",
      "\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n",
      "\u001b[32m    343\u001b[39m \n",
      "\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n",
      "\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n",
      "\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n",
      "\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "from pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\n",
      "------------------\n",
      "\n",
      "\n",
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mfrom pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\u001b[39m\n",
      "                                   ^\n",
      "\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "def main() -> None:\n",
      "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
      "    build_summary(headlines, PROJECT_DIR)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n",
      "\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[32m     15\u001b[39m         headlines.append({\n",
      "\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m     20\u001b[39m         })\n",
      "\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     21\u001b[39m items = []\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n",
      "\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n",
      "\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n",
      "\u001b[32m    343\u001b[39m \n",
      "\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n",
      "\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n",
      "\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n",
      "\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "from pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\n",
      "------------------\n",
      "\n",
      "\n",
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mfrom pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\u001b[39m\n",
      "                                   ^\n",
      "\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "def main() -> None:\n",
      "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
      "    build_summary(headlines, PROJECT_DIR)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n",
      "\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[32m     15\u001b[39m         headlines.append({\n",
      "\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m     20\u001b[39m         })\n",
      "\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     21\u001b[39m items = []\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n",
      "\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n",
      "\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n",
      "\u001b[32m    343\u001b[39m \n",
      "\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n",
      "\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n",
      "\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n",
      "\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "from pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\n",
      "------------------\n",
      "\n",
      "\n",
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mfrom pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\u001b[39m\n",
      "                                   ^\n",
      "\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "def main() -> None:\n",
      "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
      "    build_summary(headlines, PROJECT_DIR)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n",
      "\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[32m     15\u001b[39m         headlines.append({\n",
      "\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m     20\u001b[39m         })\n",
      "\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     21\u001b[39m items = []\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n",
      "\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n",
      "\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n",
      "\u001b[32m    343\u001b[39m \n",
      "\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n",
      "\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n",
      "\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n",
      "\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "from pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\n",
      "------------------\n",
      "\n",
      "\n",
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mfrom pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\u001b[39m\n",
      "                                   ^\n",
      "\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "def main() -> None:\n",
      "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
      "    build_summary(headlines, PROJECT_DIR)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n",
      "\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[32m     15\u001b[39m         headlines.append({\n",
      "\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m     20\u001b[39m         })\n",
      "\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     21\u001b[39m items = []\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n",
      "\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n",
      "\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n",
      "\u001b[32m    343\u001b[39m \n",
      "\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n",
      "\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n",
      "\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n",
      "\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "from pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\n",
      "------------------\n",
      "\n",
      "\n",
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mfrom pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\u001b[39m\n",
      "                                   ^\n",
      "\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "def main() -> None:\n",
      "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
      "    build_summary(headlines, PROJECT_DIR)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n",
      "\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[32m     15\u001b[39m         headlines.append({\n",
      "\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m     20\u001b[39m         })\n",
      "\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     21\u001b[39m items = []\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n",
      "\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n",
      "\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n",
      "\u001b[32m    343\u001b[39m \n",
      "\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n",
      "\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n",
      "\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n",
      "\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "from pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\n",
      "------------------\n",
      "\n",
      "\n",
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mfrom pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\u001b[39m\n",
      "                                   ^\n",
      "\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "def main() -> None:\n",
      "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
      "    build_summary(headlines, PROJECT_DIR)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n",
      "\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[32m     15\u001b[39m         headlines.append({\n",
      "\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m     20\u001b[39m         })\n",
      "\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     21\u001b[39m items = []\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n",
      "\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n",
      "\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n",
      "\u001b[32m    343\u001b[39m \n",
      "\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n",
      "\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n",
      "\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n",
      "\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "from pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\n",
      "------------------\n",
      "\n",
      "\n",
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mfrom pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\u001b[39m\n",
      "                                   ^\n",
      "\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headline_analysis/headline_analysis.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "def main() -> None:\n",
      "    sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "    headlines = collect_headlines(sources, PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=1), '1h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(hours=24), '24h', PROJECT_DIR)\n",
      "    filter_headlines(headlines, timedelta(days=7), '7d', PROJECT_DIR)\n",
      "    build_summary(headlines, PROJECT_DIR)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      7\u001b[39m     build_summary(headlines, PROJECT_DIR)\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m      2\u001b[39m     sources = collect_sources(NEWS_DIR, PROJECT_DIR)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     headlines = \u001b[43mcollect_headlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      4\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m1h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\u001b[32m      5\u001b[39m     filter_headlines(headlines, timedelta(hours=\u001b[32m24\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m24h\u001b[39m\u001b[33m'\u001b[39m, PROJECT_DIR)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcollect_headlines\u001b[39m\u001b[34m(sources, out_dir)\u001b[39m\n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest:\n",
      "\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[32m     15\u001b[39m         headlines.append({\n",
      "\u001b[32m     16\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     17\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     18\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: src[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m],\n",
      "\u001b[32m     19\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mpubdate\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m     20\u001b[39m         })\n",
      "\u001b[32m     21\u001b[39m min_time = datetime.min.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mparse_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_feed\u001b[39m(path: Path) -> \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a feed file of any supported type.\"\"\"\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path.suffix == \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m parse_xml_feed(path)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mparse_json_feed\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a JSON feed file.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     21\u001b[39m items = []\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[32m     24\u001b[39m     title = item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n",
      "\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n",
      "\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n",
      "\u001b[32m    343\u001b[39m \n",
      "\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n",
      "\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n",
      "\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n",
      "\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    \u001b[31masyncio.get_running_loop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mno running event loop\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/bin/jupyter-nbconvert\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/application.py\"\u001b[0m, line \u001b[35m284\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31msuper().launch_instance\u001b[0m\u001b[1;31m(argv=argv, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/traitlets/config/application.py\"\u001b[0m, line \u001b[35m1075\u001b[0m, in \u001b[35mlaunch_instance\u001b[0m\n",
      "    \u001b[31mapp.start\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m420\u001b[0m, in \u001b[35mstart\u001b[0m\n",
      "    \u001b[31mself.convert_notebooks\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m597\u001b[0m, in \u001b[35mconvert_notebooks\u001b[0m\n",
      "    \u001b[31mself.convert_single_notebook\u001b[0m\u001b[1;31m(notebook_filename)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m563\u001b[0m, in \u001b[35mconvert_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.export_single_notebook\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources, input_buffer=input_buffer\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/nbconvertapp.py\"\u001b[0m, line \u001b[35m487\u001b[0m, in \u001b[35mexport_single_notebook\u001b[0m\n",
      "    output, resources = \u001b[31mself.exporter.from_filename\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnotebook_filename, resources=resources\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m201\u001b[0m, in \u001b[35mfrom_filename\u001b[0m\n",
      "    return \u001b[31mself.from_file\u001b[0m\u001b[1;31m(f, resources=resources, **kw)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m220\u001b[0m, in \u001b[35mfrom_file\u001b[0m\n",
      "    return \u001b[31mself.from_notebook_node\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mnbformat.read(file_stream, as_version=4), resources=resources, **kw\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/notebook.py\"\u001b[0m, line \u001b[35m36\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31msuper().from_notebook_node\u001b[0m\u001b[1;31m(nb, resources, **kw)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35mfrom_notebook_node\u001b[0m\n",
      "    nb_copy, resources = \u001b[31mself._preprocess\u001b[0m\u001b[1;31m(nb_copy, resources)\u001b[0m\n",
      "                         \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/exporters/exporter.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35m_preprocess\u001b[0m\n",
      "    nbc, resc = \u001b[31mpreprocessor\u001b[0m\u001b[1;31m(nbc, resc)\u001b[0m\n",
      "                \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/base.py\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.preprocess\u001b[0m\u001b[1;31m(nb, resources)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m103\u001b[0m, in \u001b[35mpreprocess\u001b[0m\n",
      "    \u001b[31mself.preprocess_cell\u001b[0m\u001b[1;31m(cell, resources, index)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mpreprocess_cell\u001b[0m\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/jupyter_core/utils/__init__.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return \u001b[31mloop.run_until_complete\u001b[0m\u001b[1;31m(inner)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    return \u001b[31mfuture.result\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m1062\u001b[0m, in \u001b[35masync_execute_cell\u001b[0m\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \u001b[35m\"/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/nbclient/client.py\"\u001b[0m, line \u001b[35m918\u001b[0m, in \u001b[35m_check_raise_for_error\u001b[0m\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;35mnbclient.exceptions.CellExecutionError\u001b[0m: \u001b[35mAn error occurred while executing the following cell:\n",
      "------------------\n",
      "from pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\n",
      "------------------\n",
      "\n",
      "\n",
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mfrom pathlib import Pathimport csvimport jsonimport xml.etree.ElementTree as ETfrom datetime import datetime, timezone, timedeltafrom email.utils import parsedate_to_datetimeimport shutilBASE_DIR = Path.cwd()REPO_DIR = BASE_DIRwhile not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):    if REPO_DIR.parent == REPO_DIR:        raise FileNotFoundError('Repository root not found')    REPO_DIR = REPO_DIR.parentDATA_DIR = REPO_DIR / 'data'HEADLINES_DIR = REPO_DIR / 'analysis/headlines'HEADLINES_DIR.mkdir(parents=True, exist_ok=True)ARCHIVE_DIR = HEADLINES_DIR / 'archive'ARCHIVE_DIR.mkdir(exist_ok=True)def parse_pubdate(date_str):    \"\"\"Return a timezone-aware datetime parsed from a feed string.\"\"\"    try:        dt = parsedate_to_datetime(date_str) if date_str else None        if dt is None:            return None        if dt.tzinfo is None:            dt = dt.replace(tzinfo=timezone.utc)        return dt.astimezone(timezone.utc)    except Exception:        return Nonedef format_pubdate(dt):    \"\"\"Format a publication datetime for CSV output.\"\"\"    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''def parse_feed(path: Path):    \"\"\"Parse an RSS or JSON feed file into a list of entries.\"\"\"    entries = []    if path.suffix == '.json':        with open(path, 'r', encoding='utf-8') as f:            data = json.load(f)        for item in data.get('entries', []):            title = item.get('title')            link = item.get('link')            pub = parse_pubdate(item.get('published'))            if title and link:                entries.append((pub, title.strip(), link.strip()))    else:        try:            tree = ET.parse(path)            root = tree.getroot()        except ET.ParseError:            return entries        for item in root.iter():            if item.tag.lower().endswith(('item', 'entry')):                title = None                link = None                pub = None                for child in item:                    tag = child.tag.lower()                    if tag.endswith('title'):                        title = (child.text or '').strip()                    if tag.endswith('link'):                        link = (child.text or '').strip() or child.attrib.get('href')                    if tag.endswith(('pubdate', 'published', 'updated')):                        pub = parse_pubdate((child.text or '').strip())                if title and link:                    entries.append((pub, title, link))    return entriesdef collect_headlines():    \"\"\"Gather headlines from all news source files.\"\"\"    all_entries = []    feed_info = {}    for source in DATA_DIR.iterdir():        if not source.is_dir() or not source.name.startswith('news'):            continue        candidates = [            p for p in source.rglob('latest.*')            if p.suffix in {'.json', '.rss', '.xml'}        ]        if not candidates:            candidates = [                p for p in source.rglob('*')                if p.suffix in {'.json', '.rss', '.xml'}            ]        if not candidates:            continue        latest_file = max(candidates, key=lambda p: p.stat().st_mtime)        source_name = latest_file.relative_to(DATA_DIR).parts[1]        feed_entries = parse_feed(latest_file)        if feed_entries:            recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)            feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}        for pub, title, link in feed_entries:            all_entries.append((pub, title, link, source_name))    return all_entries, feed_infodef _date_key(date_str):    try:        return parsedate_to_datetime(date_str) if date_str else datetime.min    except Exception:        return datetime.mindef deduplicate_entries(entries):    \"\"\"Remove duplicate headlines by title or link.\"\"\"    deduped = []    seen_titles = set()    seen_links = set()    for pub, title, link, src in entries:        t_key = title.lower()        l_key = link.lower()        if t_key in seen_titles or l_key in seen_links:            continue        deduped.append((pub, src, title, link))        seen_titles.add(t_key)        seen_links.add(l_key)    cutoff = datetime.now(timezone.utc) - timedelta(days=1)    return [r for r in deduped if r[0] and r[0] >= cutoff]def write_csv(rows, path):    \"\"\"Write headline rows to a CSV file.\"\"\"    with open(path, 'w', newline='', encoding='utf-8') as f:        writer = csv.writer(f)        writer.writerow(['pubdate', 'source', 'title', 'link'])        for pub, src, title, link in rows:            writer.writerow([format_pubdate(pub), src, title, link])def print_summary(feed_info):    \"\"\"Display a summary of feed counts.\"\"\"    print('Feed summary:')    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")    for src, info in sorted(feed_info.items()):        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")def update_headlines():    \"\"\"Update headline archive from the most recent feeds.\"\"\"    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')    hourly_file = ARCHIVE_DIR / f\"{timestamp}.csv\"    if hourly_file.exists():        print(f\"{hourly_file.name} already exists. Skipping update.\")        return    entries, feed_info = collect_headlines()    entries.sort(        key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc),        reverse=True,    )    deduped = deduplicate_entries(entries)    write_csv(deduped, hourly_file)    shutil.copy(hourly_file, HEADLINES_DIR / 'latest.csv')    print(f\"Wrote {hourly_file} and updated latest.csv\")    print()    print_summary(feed_info)update_headlines()\u001b[39m\n",
      "                                   ^\n",
      "\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n",
      "\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import yaml\n",
    "from typing import List\n",
    "\n",
    "def _parse_meta(path: Path):\n",
    "    \"\"\"Return metadata and body from a markdown file.\"\"\"\n",
    "    text = path.read_text()\n",
    "    m = re.search(r'^---\\n(.*?)\\n---\\n?', text, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            meta = yaml.safe_load(m.group(1)) or {}\n",
    "        except Exception as e:\n",
    "            print(f'Error parsing metadata in {path}:', e)\n",
    "            raise\n",
    "        body = text[m.end():]\n",
    "    else:\n",
    "        meta, body = {}, text\n",
    "    return meta, body\n",
    "\n",
    "def _latest_mtime(p: Path) -> float:\n",
    "    \"\"\"Return the most recent modification time in a path.\"\"\"\n",
    "    if p.is_file():\n",
    "        return p.stat().st_mtime\n",
    "    mt = [f.stat().st_mtime for f in p.rglob('*') if f.is_file()]\n",
    "    return max(mt) if mt else p.stat().st_mtime\n",
    "\n",
    "def build_dep_map(analysis_dir: Path, repo_dir: Path):\n",
    "    \"\"\"Map notebooks to their dependency paths.\"\"\"\n",
    "    dep_map = {}\n",
    "    for meta in analysis_dir.rglob('index.md'):\n",
    "        info, _ = _parse_meta(meta)\n",
    "        deps = [(repo_dir / d).resolve() for d in info.get('dependencies', [])]\n",
    "        for nb in meta.parent.glob('*.ipynb'):\n",
    "            dep_map[nb] = deps\n",
    "    return dep_map\n",
    "\n",
    "def outdated(nb: Path, deps: List[Path]) -> bool:\n",
    "    \"\"\"Return True if any dependency is newer than the notebook.\"\"\"\n",
    "    nb_m = _latest_mtime(nb)\n",
    "    return any(_latest_mtime(d) > nb_m for d in deps)\n",
    "\n",
    "def execute(nb: Path):\n",
    "    \"\"\"Run a notebook in-place if jupyter is available.\"\"\"\n",
    "    import shutil\n",
    "    if not shutil.which('jupyter'):\n",
    "        print('jupyter not available - skipping', nb)\n",
    "        return\n",
    "    cmd = [\n",
    "        sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "        '--to', 'notebook', '--inplace', '--execute',\n",
    "        '--ExecutePreprocessor.timeout=600', str(nb),\n",
    "    ]\n",
    "    subprocess.run(cmd, check=False)\n",
    "\n",
    "def updateAnalyses(path: str):\n",
    "    \"\"\"Execute notebooks whose dependencies have changed.\"\"\"\n",
    "    repo_dir = Path('.').resolve()\n",
    "    analysis_dir = repo_dir / path\n",
    "    for _ in range(10):\n",
    "        dep_map = build_dep_map(analysis_dir, repo_dir)\n",
    "        outdated_nbs = [\n",
    "            nb for nb, deps in dep_map.items() if deps and outdated(nb, deps)\n",
    "        ]\n",
    "        json.dump(\n",
    "            {'outdated_notebooks': [str(nb) for nb in outdated_nbs]},\n",
    "            open('dependencies.json', 'w'),\n",
    "            indent=2,\n",
    "        )\n",
    "        if not outdated_nbs:\n",
    "            print('Everything up to date.')\n",
    "            break\n",
    "        for nb in outdated_nbs:\n",
    "            execute(nb)\n",
    "\n",
    "updateAnalyses('./analysis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
