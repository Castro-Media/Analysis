{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00052047",
   "metadata": {},
   "source": [
    "# Run Analysis\n",
    "\n",
    "Data sources are stored as arbitrarily deep directories in /data. A data source is a directory with an index file which contains front matter such as filetype, cadence, etc related to how to access the data and when/whether to update the data automatically.  \n",
    "\n",
    "Some data sources have a static cadence, meaning they wont be automatically updated by this notebook.  \n",
    "\n",
    "Others specify an update cadence, a time last updated, and a method of updating which may include things like an api key, etc.  \n",
    "\n",
    "An analysis, likewise will be an arbitrarily deep sirectory wihtin /analysis which contains an index file with front matter like title and dependencies.  \n",
    "\n",
    "Dependencies are paths to data sources or other analyses. Whenever an analysis was last modified before any one of its dependencies, the analysis is stale and needs to be run again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c687cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Make sure all required packages are installed and imported\n",
    "\n",
    "import importlib, subprocess, sys\n",
    "from typing import Optional\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: Optional[str] = None, required: bool = True):\n",
    "    try:\n",
    "        return importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "        except Exception:\n",
    "            if required:\n",
    "                raise\n",
    "    mod = importlib.import_module(import_name or pkg_name)\n",
    "    globals()[import_name or pkg_name] = mod\n",
    "    return mod\n",
    "\n",
    "_ensure('pandas')\n",
    "_ensure('requests')\n",
    "_ensure('feedparser')\n",
    "_ensure('textblob')\n",
    "_ensure('pyyaml', 'yaml')\n",
    "_ensure('jupyter', required=False)\n",
    "_ensure('nbconvert', required=False)\n",
    "print('All dependencies ready.\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00445370",
   "metadata": {},
   "source": [
    "## Update Data Sources\n",
    "\n",
    "This cell needs to find all of the /data index files, check whether that data source needs to be updated, and then do whatever updates are appropriate.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcda8f53",
   "metadata": {},
   "outputs": [
    {
     "ename": "ScannerError",
     "evalue": "mapping values are not allowed here\n  in \"<unicode string>\", line 2, column 30:\n    title: fred - US Federal Debt: Total US Public Debt\n                                 ^",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mScannerError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     88\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEverything up to date.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mupdateData\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mupdateData\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     45\u001b[39m updated = []\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx_file \u001b[38;5;129;01min\u001b[39;00m base.rglob(\u001b[33m'\u001b[39m\u001b[33mindex.md\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     meta, body = \u001b[43m_parse_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     url = \u001b[38;5;28mstr\u001b[39m(meta.get(\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)).strip()\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url \u001b[38;5;129;01mor\u001b[39;00m url.lower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mn/a\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mna\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36m_parse_meta\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     30\u001b[39m m = re.search(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m^---\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn(.*?)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn---\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn?\u001b[39m\u001b[33m'\u001b[39m, text, re.S)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     meta = \u001b[43myaml\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m     33\u001b[39m     body = text[m.end():]\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\__init__.py:125\u001b[39m, in \u001b[36msafe_load\u001b[39m\u001b[34m(stream)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_load\u001b[39m(stream):\n\u001b[32m    118\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \u001b[33;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\__init__.py:81\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(stream, Loader)\u001b[39m\n\u001b[32m     79\u001b[39m loader = Loader(stream)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     loader.dispose()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\constructor.py:49\u001b[39m, in \u001b[36mBaseConstructor.get_single_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_single_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_single_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.construct_document(node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\composer.py:36\u001b[39m, in \u001b[36mComposer.get_single_node\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     34\u001b[39m document = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_event(StreamEndEvent):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     document = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Ensure that the stream contains no more documents.\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_event(StreamEndEvent):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\composer.py:55\u001b[39m, in \u001b[36mComposer.compose_document\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mself\u001b[39m.get_event()\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Compose the root node.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Drop the DOCUMENT-END event.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.get_event()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\composer.py:84\u001b[39m, in \u001b[36mComposer.compose_node\u001b[39m\u001b[34m(self, parent, index)\u001b[39m\n\u001b[32m     82\u001b[39m     node = \u001b[38;5;28mself\u001b[39m.compose_sequence_node(anchor)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_event(MappingStartEvent):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m.ascend_resolver()\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\composer.py:127\u001b[39m, in \u001b[36mComposer.compose_mapping_node\u001b[39m\u001b[34m(self, anchor)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m anchor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m.anchors[anchor] = node\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMappingEndEvent\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m#key_event = self.peek_event()\u001b[39;00m\n\u001b[32m    129\u001b[39m     item_key = \u001b[38;5;28mself\u001b[39m.compose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m#if item_key in node.value:\u001b[39;00m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\parser.py:98\u001b[39m, in \u001b[36mParser.check_event\u001b[39m\u001b[34m(self, *choices)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m         \u001b[38;5;28mself\u001b[39m.current_event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\parser.py:428\u001b[39m, in \u001b[36mParser.parse_block_mapping_key\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_block_mapping_key\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKeyToken\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    429\u001b[39m         token = \u001b[38;5;28mself\u001b[39m.get_token()\n\u001b[32m    430\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_token(KeyToken, ValueToken, BlockEndToken):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\scanner.py:116\u001b[39m, in \u001b[36mScanner.check_token\u001b[39m\u001b[34m(self, *choices)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, *choices):\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# Check if the next token is one of the given types.\u001b[39;00m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.need_more_tokens():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_more_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokens:\n\u001b[32m    118\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\scanner.py:223\u001b[39m, in \u001b[36mScanner.fetch_more_tokens\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Is it the value indicator?\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ch == \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_value():\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Is it an alias?\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ch == \u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\scanner.py:577\u001b[39m, in \u001b[36mScanner.fetch_value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.flow_level:\n\u001b[32m    573\u001b[39m \n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# We are allowed to start a complex value if and only if\u001b[39;00m\n\u001b[32m    575\u001b[39m     \u001b[38;5;66;03m# we can start a simple key.\u001b[39;00m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.allow_simple_key:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ScannerError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    578\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmapping values are not allowed here\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    579\u001b[39m                 \u001b[38;5;28mself\u001b[39m.get_mark())\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# If this value starts a new block mapping, we need to add\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# BLOCK-MAPPING-START.  It will be detected as an error later by\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;66;03m# the parser.\u001b[39;00m\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.flow_level:\n",
      "\u001b[31mScannerError\u001b[39m: mapping values are not allowed here\n  in \"<unicode string>\", line 2, column 30:\n    title: fred - US Federal Debt: Total US Public Debt\n                                 ^"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json, feedparser, textblob\n",
    "import pandas as pd, requests, urllib.parse, yaml\n",
    "from typing import Optional\n",
    "\n",
    "CADENCE_SECONDS = {\n",
    "    'hourly': 3600,\n",
    "    'daily': 86400,\n",
    "    'weekly': 604800,\n",
    "    'monthly': 2592000,\n",
    "    'quarterly': 7776000,\n",
    "}\n",
    "\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        return dt.date.today().strftime(m.group(1).strip())\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "def add_apikey(url: str, env_var: Optional[str]) -> str:\n",
    "    if env_var:\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f\"{url}{sep}api_key={urllib.parse.quote_plus(key)}\"\n",
    "    return url\n",
    "\n",
    "def _parse_meta(path: Path):\n",
    "    text = path.read_text()\n",
    "    m = re.search(r'^---\\n(.*?)\\n---\\n?', text, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            meta = yaml.safe_load(m.group(1)) or {}\n",
    "        except Exception as e:\n",
    "            print(f'Error parsing metadata in {path}:', e)\n",
    "            raise\n",
    "        body = text[m.end():]\n",
    "    else:\n",
    "        meta, body = {}, text\n",
    "    return meta, body\n",
    "\n",
    "def _write_meta(path: Path, meta: dict, body: str):\n",
    "    path.write_text('---\\n' + yaml.safe_dump(meta, sort_keys=False).strip() + '\\n---\\n' + body)\n",
    "\n",
    "def updateData(path: str):\n",
    "    base = Path(path)\n",
    "    now = dt.datetime.now()\n",
    "    today = now.date()\n",
    "    updated = []\n",
    "    for idx_file in base.rglob('index.md'):\n",
    "        meta, body = _parse_meta(idx_file)\n",
    "        url = str(meta.get('url', '')).strip()\n",
    "        if not url or url.lower() in ('n/a', 'na', 'none'):\n",
    "            continue\n",
    "        filetype = str(meta.get('filetype', '')).strip().lstrip('.')\n",
    "        cadence = str(meta.get('cadence', 'monthly')).lower()\n",
    "        api_key = meta.get('api_key')\n",
    "        last_fetch = pd.to_datetime(meta.get('last_fetched')) if meta.get('last_fetched') else None\n",
    "        min_age = CADENCE_SECONDS.get(cadence, 2592000)\n",
    "        folder = idx_file.parent\n",
    "        output_ext = 'json' if filetype in ('rss', 'xml') else filetype\n",
    "        latest_fp = folder / f'latest.{output_ext}'\n",
    "        dated_fp = folder / (f\"{now:%Y-%m-%d-%H}.{output_ext}\" if cadence == 'hourly' else f\"{today:%Y-%m-%d}.{output_ext}\")\n",
    "        if latest_fp.exists() and last_fetch and (now - last_fetch).total_seconds() < min_age:\n",
    "            if dated_fp.exists() and latest_fp.read_bytes() != dated_fp.read_bytes():\n",
    "                shutil.copyfile(dated_fp, latest_fp)\n",
    "            continue\n",
    "        req_url = add_apikey(substitute_date_tokens(url), api_key)\n",
    "        try:\n",
    "            r = requests.get(req_url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            r.raise_for_status()\n",
    "            if filetype in ('rss', 'xml'):\n",
    "                feed = feedparser.parse(r.content)\n",
    "                entries = []\n",
    "                for e in feed.entries:\n",
    "                    txt = ' '.join(filter(None, [e.get('title'), e.get('summary')]))\n",
    "                    pol = textblob.TextBlob(txt).sentiment.polarity\n",
    "                    entries.append({'title': e.get('title'), 'link': e.get('link'), 'published': e.get('published'), 'sentiment': pol})\n",
    "                content = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "            else:\n",
    "                content = r.content\n",
    "            dated_fp.write_bytes(content)\n",
    "            shutil.copyfile(dated_fp, latest_fp)\n",
    "            meta['last_fetched'] = now.isoformat(timespec='minutes')\n",
    "            _write_meta(idx_file, meta, body)\n",
    "            updated.append(str(folder.relative_to(base)))\n",
    "        except Exception as e:\n",
    "            print('Failed to fetch', folder, e)\n",
    "    if updated:\n",
    "        print('Updated:', ', '.join(updated))\n",
    "    else:\n",
    "        print('Everything up to date.')\n",
    "\n",
    "updateData('./data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc361023",
   "metadata": {},
   "source": [
    "## Update Analyses\n",
    "\n",
    "This cell needs to do the same type of scan across all the analyses in /analysis. It needs to iterate across all the analyses and check the time last modified for all the dependencies. If any dependency was modified more recently than the analysis, then the analysis needs to be run again. The time last modified of the analysis is the most recent file modification time in the analysis directory, because the analysis directory will contain some arbitrary number of output files.  \n",
    "\n",
    "Because some analyses will list other analyses as dependencies, this loop of checking across all of the analyses needs to keep running until none of them have anything to do, up to some reasonable limit of times to prevent arbitrary recursion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b77f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, re, subprocess, sys, yaml\n",
    "from typing import List\n",
    "\n",
    "def _parse_meta(path: Path):\n",
    "    text = path.read_text()\n",
    "    m = re.search(r'^---\\n(.*?)\\n---\\n?', text, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            meta = yaml.safe_load(m.group(1)) or {}\n",
    "        except Exception as e:\n",
    "            print(f'Error parsing metadata in {path}:', e)\n",
    "            raise\n",
    "        body = text[m.end():]\n",
    "    else:\n",
    "        meta, body = {}, text\n",
    "    return meta, body\n",
    "\n",
    "def _latest_mtime(p: Path) -> float:\n",
    "    if p.is_file():\n",
    "        return p.stat().st_mtime\n",
    "    mt=[f.stat().st_mtime for f in p.rglob('*') if f.is_file()]\n",
    "    return max(mt) if mt else p.stat().st_mtime\n",
    "\n",
    "def updateAnalyses(path: str):\n",
    "    repo_dir = Path('.').resolve()\n",
    "    analysis_dir = repo_dir / path\n",
    "    def build_dep_map():\n",
    "        dep_map={}\n",
    "        for meta in analysis_dir.rglob('index.md'):\n",
    "            info,_ = _parse_meta(meta)\n",
    "            deps=[(repo_dir/d).resolve() for d in info.get('dependencies',[])]\n",
    "            for nb in meta.parent.glob('*.ipynb'):\n",
    "                dep_map[nb]=deps\n",
    "        return dep_map\n",
    "    def outdated(nb,deps):\n",
    "        nb_m=_latest_mtime(nb)\n",
    "        return any(_latest_mtime(d)>nb_m for d in deps)\n",
    "    def execute(nb: Path):\n",
    "        import shutil\n",
    "        if not shutil.which('jupyter'):\n",
    "            print('jupyter not available - skipping', nb)\n",
    "            return\n",
    "        cmd=[sys.executable,'-m','jupyter','nbconvert','--to','notebook','--inplace','--execute','--ExecutePreprocessor.timeout=600',str(nb)]\n",
    "        subprocess.run(cmd, check=False)\n",
    "    for _ in range(10):\n",
    "        dep_map=build_dep_map()\n",
    "        outdated_nbs=[nb for nb,deps in dep_map.items() if deps and outdated(nb,deps)]\n",
    "        json.dump({'outdated_notebooks':[str(nb) for nb in outdated_nbs]}, open('dependencies.json','w'), indent=2)\n",
    "        if not outdated_nbs:\n",
    "            print('Everything up to date.')\n",
    "            break\n",
    "        for nb in outdated_nbs:\n",
    "            execute(nb)\n",
    "updateAnalyses('./analysis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}