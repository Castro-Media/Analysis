{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00052047",
   "metadata": {},
   "source": [
    "# Run Analysis\n",
    "\n",
    "Data sources are stored as arbitrarily deep directories in /data. A data source is a directory with an index file which contains front matter such as filetype, cadence, etc related to how to access the data and when/whether to update the data automatically.  \n",
    "\n",
    "Some data sources have a static cadence, meaning they wont be automatically updated by this notebook.  \n",
    "\n",
    "Others specify an update cadence, a time last updated, and a method of updating which may include things like an api key, etc.  \n",
    "\n",
    "An analysis, likewise will be an arbitrarily deep sirectory wihtin /analysis which contains an index file with front matter like title and dependencies.  \n",
    "\n",
    "Dependencies are paths to data sources or other analyses. Whenever an analysis was last modified before any one of its dependencies, the analysis is stale and needs to be run again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c687cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Make sure all required packages are installed and imported\n",
    "\n",
    "import importlib, subprocess, sys\n",
    "from typing import Optional\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: Optional[str] = None, required: bool = True):\n",
    "    try:\n",
    "        return importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "        except Exception:\n",
    "            if required:\n",
    "                raise\n",
    "    mod = importlib.import_module(import_name or pkg_name)\n",
    "    globals()[import_name or pkg_name] = mod\n",
    "    return mod\n",
    "\n",
    "_ensure('pandas')\n",
    "_ensure('requests')\n",
    "_ensure('feedparser')\n",
    "_ensure('textblob')\n",
    "_ensure('pyyaml', 'yaml')\n",
    "_ensure('jupyter', required=False)\n",
    "_ensure('nbconvert', required=False)\n",
    "print('All dependencies ready.\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00445370",
   "metadata": {},
   "source": [
    "## Update Data Sources\n",
    "\n",
    "This cell needs to find all of the /data index files, check whether that data source needs to be updated, and then do whatever updates are appropriate.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json, feedparser, textblob\n",
    "import pandas as pd, requests, urllib.parse, yaml\n",
    "from typing import Optional\n",
    "\n",
    "CADENCE_SECONDS = {\n",
    "    'hourly': 3600,\n",
    "    'daily': 86400,\n",
    "    'weekly': 604800,\n",
    "    'monthly': 2592000,\n",
    "    'quarterly': 7776000,\n",
    "}\n",
    "\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        return dt.date.today().strftime(m.group(1).strip())\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "def add_apikey(url: str, env_var: Optional[str]) -> str:\n",
    "    if env_var and str(env_var).lower() != 'nan':\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f\"{url}{sep}api_key={urllib.parse.quote_plus(key)}\"\n",
    "    return url\n",
    "\n",
    "def _parse_meta(path: Path) -> dict:\n",
    "    text = path.read_text()\n",
    "    m = re.search(r'^---\\n(.*?)\\n---', text, re.S)\n",
    "    return yaml.safe_load(m.group(1)) if m else {}\n",
    "\n",
    "def updateData(path: str):\n",
    "    base = Path(path)\n",
    "    rows = []\n",
    "    for meta_fp in base.rglob('metadata.md'):\n",
    "        meta = _parse_meta(meta_fp)\n",
    "        if 'folder' in meta:\n",
    "            meta['metadata_file'] = str(meta_fp)\n",
    "            rows.append(meta)\n",
    "    cat = pd.DataFrame(rows)\n",
    "    cat['filetype'] = cat['filetype'].astype(str).str.strip().str.lstrip('.')\n",
    "    now = dt.datetime.now(); today = now.date()\n",
    "    updated = []\n",
    "    for idx,row in cat.iterrows():\n",
    "        folder = base / str(row['category']) / str(row['source']) / str(row['folder'])\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "        filetype = str(row['filetype']).strip().lstrip('.')\n",
    "        output_ext = 'json' if filetype.lower() in ('rss','xml') else filetype\n",
    "        latest_fp = folder / f'latest.{output_ext}'\n",
    "        url = str(row.get('url','')).strip()\n",
    "        if not url or url.lower() in ('n/a','na','none'):\n",
    "            continue\n",
    "        dated_fp = folder / f\"{now:%Y-%m-%d-%H}.{output_ext}\" if row['cadence']=='hourly' else folder / f\"{today:%Y-%m-%d}.{output_ext}\"\n",
    "        last_fetched = pd.to_datetime(row.get('last_fetched')) if pd.notna(row.get('last_fetched')) else None\n",
    "        min_age = CADENCE_SECONDS.get(str(row['cadence']).lower().strip(), 2592000)\n",
    "        if latest_fp.exists() and last_fetched and (now - last_fetched).total_seconds() < min_age:\n",
    "            if dated_fp.exists() and latest_fp.read_bytes()!=dated_fp.read_bytes():\n",
    "                shutil.copyfile(dated_fp, latest_fp)\n",
    "            continue\n",
    "        req_url = add_apikey(substitute_date_tokens(url), str(row.get('api_key') or '').strip() or None)\n",
    "        try:\n",
    "            r = requests.get(req_url, timeout=30, headers={'User-Agent':'Mozilla/5.0'})\n",
    "            r.raise_for_status()\n",
    "            if filetype.lower() in ('rss','xml'):\n",
    "                feed = feedparser.parse(r.content)\n",
    "                entries=[]\n",
    "                for e in feed.entries:\n",
    "                    text=' '.join(filter(None,[e.get('title'), e.get('summary')]))\n",
    "                    pol=textblob.TextBlob(text).sentiment.polarity\n",
    "                    entries.append({'title': e.get('title'), 'link': e.get('link'), 'published': e.get('published'), 'sentiment': pol})\n",
    "                content = json.dumps({'entries':entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "            else:\n",
    "                content=r.content\n",
    "            dated_fp.write_bytes(content)\n",
    "            shutil.copyfile(dated_fp, latest_fp)\n",
    "            cat.at[idx,'last_fetched']=now.isoformat(timespec='minutes')\n",
    "            meta=_parse_meta(Path(row['metadata_file']))\n",
    "            meta['last_fetched']=now.isoformat(timespec='minutes')\n",
    "            Path(row['metadata_file']).write_text('---\\n'+yaml.safe_dump(meta, sort_keys=False).strip()+'\\n---\\n')\n",
    "            updated.append(row['folder'])\n",
    "        except Exception as e:\n",
    "            print('Failed to fetch', row['folder'], e)\n",
    "    if updated:\n",
    "        (base/'catalog.csv').write_text(cat.to_csv(index=False))\n",
    "        print('Updated:', ', '.join(updated))\n",
    "    else:\n",
    "        print('Everything up to date.')\n",
    "\n",
    "updateData('./data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc361023",
   "metadata": {},
   "source": [
    "## Update Analyses\n",
    "\n",
    "This cell needs to do the same type of scan across all the analyses in /analysis. It needs to iterate across all the analyses and check the time last modified for all the dependencies. If any dependency was modified more recently than the analysis, then the analysis needs to be run again. The time last modified of the analysis is the most recent file modification time in the analysis directory, because the analysis directory will contain some arbitrary number of output files.  \n",
    "\n",
    "Because some analyses will list other analyses as dependencies, this loop of checking across all of the analyses needs to keep running until none of them have anything to do, up to some reasonable limit of times to prevent arbitrary recursion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b77f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import json, re, time, subprocess, sys, yaml\n",
    "from typing import List\n",
    "\n",
    "def _parse_meta(path: Path) -> dict:\n",
    "    text = path.read_text()\n",
    "    m = re.search(r'^---\\n(.*?)\\n---', text, re.S)\n",
    "    return yaml.safe_load(m.group(1)) if m else {}\n",
    "\n",
    "def _latest_mtime(p: Path) -> float:\n",
    "    if p.is_file():\n",
    "        return p.stat().st_mtime\n",
    "    mt=[f.stat().st_mtime for f in p.rglob('*') if f.is_file()]\n",
    "    return max(mt) if mt else p.stat().st_mtime\n",
    "\n",
    "def updateAnalyses(path: str):\n",
    "    repo_dir = Path('.').resolve()\n",
    "    analysis_dir = repo_dir / path\n",
    "    def build_dep_map():\n",
    "        dep_map={}\n",
    "        for meta in analysis_dir.rglob('index.md'):\n",
    "            info=_parse_meta(meta)\n",
    "            deps=[(repo_dir/d).resolve() for d in info.get('dependencies',[])]\n",
    "            for nb in meta.parent.glob('*.ipynb'):\n",
    "                dep_map[nb]=deps\n",
    "        return dep_map\n",
    "    def outdated(nb,deps):\n",
    "        nb_m=_latest_mtime(nb)\n",
    "        return any(_latest_mtime(d)>nb_m for d in deps)\n",
    "    def execute(nb: Path):\n",
    "        import shutil\n",
    "        if not shutil.which('jupyter'):\n",
    "            print('jupyter not available - skipping', nb)\n",
    "            return\n",
    "        cmd=[sys.executable,'-m','jupyter','nbconvert','--to','notebook','--inplace','--execute','--ExecutePreprocessor.timeout=600',str(nb)]\n",
    "        subprocess.run(cmd, check=False)\n",
    "    for _ in range(10):\n",
    "        dep_map=build_dep_map()\n",
    "        outdated_nbs=[nb for nb,deps in dep_map.items() if deps and outdated(nb,deps)]\n",
    "        json.dump({'outdated_notebooks':[str(nb) for nb in outdated_nbs]}, open('dependencies.json','w'), indent=2)\n",
    "        if not outdated_nbs:\n",
    "            print('Everything up to date.')\n",
    "            break\n",
    "        for nb in outdated_nbs:\n",
    "            execute(nb)\n",
    "updateAnalyses('./analysis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}