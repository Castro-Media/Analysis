{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00052047",
   "metadata": {},
   "source": [
    "# Run Analysis\n",
    "\n",
    "Data sources are stored as arbitrarily deep directories in /data. A data source is a directory with an index file which contains front matter such as filetype, cadence, etc related to how to access the data and when/whether to update the data automatically.  \n",
    "\n",
    "Some data sources have a static cadence, meaning they wont be automatically updated by this notebook.  \n",
    "\n",
    "Others specify an update cadence, a time last updated, and a method of updating which may include things like an api key, etc.  \n",
    "\n",
    "An analysis, likewise will be an arbitrarily deep sirectory wihtin /analysis which contains an index file with front matter like title and dependencies.  \n",
    "\n",
    "Dependencies are paths to data sources or other analyses. Whenever an analysis was last modified before any one of its dependencies, the analysis is stale and needs to be run again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c687cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Make sure all required packages are installed and imported\n",
    "\n",
    "import importlib, subprocess, sys\n",
    "from typing import Optional\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: Optional[str] = None, required: bool = True):\n",
    "    try:\n",
    "        return importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "        except Exception:\n",
    "            if required:\n",
    "                raise\n",
    "    mod = importlib.import_module(import_name or pkg_name)\n",
    "    globals()[import_name or pkg_name] = mod\n",
    "    return mod\n",
    "\n",
    "_ensure('pandas')\n",
    "_ensure('requests')\n",
    "_ensure('feedparser')\n",
    "_ensure('textblob')\n",
    "_ensure('pyyaml', 'yaml')\n",
    "_ensure('jupyter', required=False)\n",
    "_ensure('nbconvert', required=False)\n",
    "print('All dependencies ready.\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00445370",
   "metadata": {},
   "source": [
    "## Update Data Sources\n",
    "\n",
    "This cell needs to find all of the /data index files, check whether that data source needs to be updated, and then do whatever updates are appropriate.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcda8f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data\\news\\business\\wapo\\news-business-wapo 502 Server Error: Bad Gateway for url: https://feeds.washingtonpost.com/rss/business\n",
      "Failed to fetch data\\news\\politics\\startribune\\news-us-politics-startribune 404 Client Error: Not Found for url: https://www.startribune.com/politics/index.rss2\n",
      "Failed to fetch data\\news\\politics\\wapo\\news-us-politics-wapo HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=30)\n",
      "Failed to fetch data\\news\\us\\wapo\\news-us-wapo 502 Server Error: Bad Gateway for url: https://feeds.washingtonpost.com/rss/national\n",
      "Updated: news\\africa\\bbc\\news-africa-bbc, news\\africa\\nyt\\news-africa-nyt, news\\americas\\nyt\\news-americas-nyt, news\\asia\\bbc\\news-asia-bbc, news\\asia\\dw\\news-asia-dw, news\\asia\\nyt\\news-asia-nyt, news\\business\\bbc\\news-business-bbc, news\\business\\chitri\\news-business-chi-tribune, news\\business\\dw\\news-business-dw, news\\business\\latimes\\latimes-business, news\\business\\nypost\\news-business-nypost, news\\business\\nyt\\news-business-nyt, news\\business\\startribune\\news-us-business-startribune, news\\business\\toi\\news-business-toi, news\\business\\wsj\\news-business-wsj, news\\business\\wsj\\news-markets-wsj, news\\economy\\nyt\\news-economy-nyt, news\\economy\\wsj\\news-economy-wsj, news\\europe\\bbc\\news-europe-bbc, news\\europe\\dw\\news-europe-dw, news\\europe\\nyt\\news-europe-nyt, news\\europe\\toi\\news-europe-toi, news\\latin-america\\bbc\\news-latin-america-bbc, news\\middle-east\\bbc\\news-middle-east-bbc, news\\middle-east\\nyt\\news-middle-east-nyt, news\\middle-east\\toi\\news-middle-east-toi, news\\politics\\bbc\\news-politics-bbc, news\\politics\\cbc\\news-politics-cbc, news\\politics\\chitri\\news-us-politics-chi-tribune, news\\politics\\latimes\\latimes-us-politics, news\\politics\\nypost\\news-us-politics-nypost, news\\politics\\nyt\\news-us-politics-nyt, news\\politics\\wsj\\news-us-politics-wsj, news\\top\\dw\\news-top-dw, news\\us\\bbc\\news-us-bbc, news\\us\\latimes\\latimes-us, news\\us\\nypost\\news-us-nypost, news\\us\\nyt\\news-us-nyt, news\\us\\toi\\news-us-toi, news\\us\\wsj\\news-us-wsj, news\\world\\bbc\\news-world-bbc, news\\world\\cbc\\news-world-cbc, news\\world\\chitri\\news-world-chi-tribune, news\\world\\dw\\news-world-dw, news\\world\\nypost\\news-world-nypost, news\\world\\nyt\\news-world-nyt, news\\world\\toi\\news-world-toi, news\\world\\wapo\\news-world-wapo, news\\world\\wsj\\news-world-wsj\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json, feedparser, textblob\n",
    "import pandas as pd, requests, urllib.parse, yaml\n",
    "from typing import Optional\n",
    "\n",
    "CADENCE_SECONDS = {\n",
    "    'hourly': 3600,\n",
    "    'daily': 86400,\n",
    "    'weekly': 604800,\n",
    "    'monthly': 2592000,\n",
    "    'quarterly': 7776000,\n",
    "}\n",
    "\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        return dt.date.today().strftime(m.group(1).strip())\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "def add_apikey(url: str, env_var: Optional[str]) -> str:\n",
    "    if env_var:\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f\"{url}{sep}api_key={urllib.parse.quote_plus(key)}\"\n",
    "    return url\n",
    "\n",
    "def _parse_meta(path: Path):\n",
    "    text = path.read_text()\n",
    "    m = re.search(r'^---\\n(.*?)\\n---\\n?', text, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            meta = yaml.safe_load(m.group(1)) or {}\n",
    "        except Exception as e:\n",
    "            print(f'Error parsing metadata in {path}:', e)\n",
    "            raise\n",
    "        body = text[m.end():]\n",
    "    else:\n",
    "        meta, body = {}, text\n",
    "    return meta, body\n",
    "\n",
    "def _write_meta(path: Path, meta: dict, body: str):\n",
    "    path.write_text('---\\n' + yaml.safe_dump(meta, sort_keys=False).strip() + '\\n---\\n' + body)\n",
    "\n",
    "def updateData(path: str):\n",
    "    base = Path(path)\n",
    "    now = dt.datetime.now()\n",
    "    today = now.date()\n",
    "    updated = []\n",
    "    for idx_file in base.rglob('index.md'):\n",
    "        meta, body = _parse_meta(idx_file)\n",
    "        url = str(meta.get('url', '')).strip()\n",
    "        if not url or url.lower() in ('n/a', 'na', 'none'):\n",
    "            continue\n",
    "        filetype = str(meta.get('filetype', '')).strip().lstrip('.')\n",
    "        cadence = str(meta.get('cadence', 'monthly')).lower()\n",
    "        api_key = meta.get('api_key')\n",
    "        last_fetch = pd.to_datetime(meta.get('last_fetched')) if meta.get('last_fetched') else None\n",
    "        min_age = CADENCE_SECONDS.get(cadence, 2592000)\n",
    "        folder = idx_file.parent\n",
    "        output_ext = 'json' if filetype in ('rss', 'xml') else filetype\n",
    "        latest_fp = folder / f'latest.{output_ext}'\n",
    "        dated_fp = folder / (f\"{now:%Y-%m-%d-%H}.{output_ext}\" if cadence == 'hourly' else f\"{today:%Y-%m-%d}.{output_ext}\")\n",
    "        if latest_fp.exists() and last_fetch and (now - last_fetch).total_seconds() < min_age:\n",
    "            if dated_fp.exists() and latest_fp.read_bytes() != dated_fp.read_bytes():\n",
    "                shutil.copyfile(dated_fp, latest_fp)\n",
    "            continue\n",
    "        req_url = add_apikey(substitute_date_tokens(url), api_key)\n",
    "        try:\n",
    "            r = requests.get(req_url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            r.raise_for_status()\n",
    "            if filetype in ('rss', 'xml'):\n",
    "                feed = feedparser.parse(r.content)\n",
    "                entries = []\n",
    "                for e in feed.entries:\n",
    "                    txt = ' '.join(filter(None, [e.get('title'), e.get('summary')]))\n",
    "                    pol = textblob.TextBlob(txt).sentiment.polarity\n",
    "                    entries.append({'title': e.get('title'), 'link': e.get('link'), 'published': e.get('published'), 'sentiment': pol})\n",
    "                content = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "            else:\n",
    "                content = r.content\n",
    "            dated_fp.write_bytes(content)\n",
    "            shutil.copyfile(dated_fp, latest_fp)\n",
    "            meta['last_fetched'] = now.isoformat(timespec='minutes')\n",
    "            _write_meta(idx_file, meta, body)\n",
    "            updated.append(str(folder.relative_to(base)))\n",
    "        except Exception as e:\n",
    "            print('Failed to fetch', folder, e)\n",
    "    if updated:\n",
    "        print('Updated:', ', '.join(updated))\n",
    "    else:\n",
    "        print('Everything up to date.')\n",
    "\n",
    "updateData('./data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc361023",
   "metadata": {},
   "source": [
    "## Update Analyses\n",
    "\n",
    "This cell needs to do the same type of scan across all the analyses in /analysis. It needs to iterate across all the analyses and check the time last modified for all the dependencies. If any dependency was modified more recently than the analysis, then the analysis needs to be run again. The time last modified of the analysis is the most recent file modification time in the analysis directory, because the analysis directory will contain some arbitrary number of output files.  \n",
    "\n",
    "Because some analyses will list other analyses as dependencies, this loop of checking across all of the analyses needs to keep running until none of them have anything to do, up to some reasonable limit of times to prevent arbitrary recursion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b77f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything up to date.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, re, subprocess, sys, yaml\n",
    "from typing import List\n",
    "\n",
    "def _parse_meta(path: Path):\n",
    "    text = path.read_text()\n",
    "    m = re.search(r'^---\\n(.*?)\\n---\\n?', text, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            meta = yaml.safe_load(m.group(1)) or {}\n",
    "        except Exception as e:\n",
    "            print(f'Error parsing metadata in {path}:', e)\n",
    "            raise\n",
    "        body = text[m.end():]\n",
    "    else:\n",
    "        meta, body = {}, text\n",
    "    return meta, body\n",
    "\n",
    "def _latest_mtime(p: Path) -> float:\n",
    "    if p.is_file():\n",
    "        return p.stat().st_mtime\n",
    "    mt=[f.stat().st_mtime for f in p.rglob('*') if f.is_file()]\n",
    "    return max(mt) if mt else p.stat().st_mtime\n",
    "\n",
    "def updateAnalyses(path: str):\n",
    "    repo_dir = Path('.').resolve()\n",
    "    analysis_dir = repo_dir / path\n",
    "    def build_dep_map():\n",
    "        dep_map={}\n",
    "        for meta in analysis_dir.rglob('index.md'):\n",
    "            info,_ = _parse_meta(meta)\n",
    "            deps=[(repo_dir/d).resolve() for d in info.get('dependencies',[])]\n",
    "            for nb in meta.parent.glob('*.ipynb'):\n",
    "                dep_map[nb]=deps\n",
    "        return dep_map\n",
    "    def outdated(nb,deps):\n",
    "        nb_m=_latest_mtime(nb)\n",
    "        return any(_latest_mtime(d)>nb_m for d in deps)\n",
    "    def execute(nb: Path):\n",
    "        import shutil\n",
    "        if not shutil.which('jupyter'):\n",
    "            print('jupyter not available - skipping', nb)\n",
    "            return\n",
    "        cmd=[sys.executable,'-m','jupyter','nbconvert','--to','notebook','--inplace','--execute','--ExecutePreprocessor.timeout=600',str(nb)]\n",
    "        subprocess.run(cmd, check=False)\n",
    "    for _ in range(10):\n",
    "        dep_map=build_dep_map()\n",
    "        outdated_nbs=[nb for nb,deps in dep_map.items() if deps and outdated(nb,deps)]\n",
    "        json.dump({'outdated_notebooks':[str(nb) for nb in outdated_nbs]}, open('dependencies.json','w'), indent=2)\n",
    "        if not outdated_nbs:\n",
    "            print('Everything up to date.')\n",
    "            break\n",
    "        for nb in outdated_nbs:\n",
    "            execute(nb)\n",
    "updateAnalyses('./analysis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
